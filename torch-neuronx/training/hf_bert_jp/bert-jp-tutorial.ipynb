{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d73d16-fce1-4340-8857-9a7815f0773f",
   "metadata": {},
   "source": [
    "# 日本語 BERT Base Model Fine-tuning & Deployment on Inferentia2\n",
    "本 Notebook の元ネタのブログはこちらから\n",
    "+ https://aws.amazon.com/jp/blogs/news/aws-trainium-amazon-ec2-trn1-ml-training-part1/\n",
    "+ https://aws.amazon.com/jp/blogs/news/aws-trainium-amazon-ec2-trn1-ml-training-part2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0641e6b-67e8-4f10-86ef-1ce00c20162b",
   "metadata": {},
   "source": [
    "## 事前準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf706fb-f236-4b87-870f-4e7f81806dcc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%env TOKENIZERS_PARALLELISM=True #Supresses tokenizer warnings making errors easier to detect\n",
    "!pip install -U pip\n",
    "!pip install -U transformers[ja]==4.52.3 datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa82ee-39d2-4e6c-9294-b920a1284890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip list | grep \"neuron\\|torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2168c77-6ac7-4f71-95c7-06e6df4386fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sudo rmmod neuron; sudo modprobe neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87668961-d047-4603-a3b1-a05544713952",
   "metadata": {},
   "source": [
    "## データセットの準備\n",
    "本テストでは、Huggingface Hub で利用可能な以下のセンチメント（感情）データセットのうち、日本語のサブセットを使用します。\n",
    "https://huggingface.co/datasets/tyqiangz/multilingual-sentiments\n",
    "\n",
    "本テストではテキストデータをPositiveかNegativeに分類する 2 クラスの分類問題として扱うことにします。元々のデータセットは positive(LABEL_0)、neutral(LABEL_1)、negative(LABEL_2)としてラベル付けされていますが、neutralのデータは使用しないこととし、ラベルをpositive(LABEL_0)、negative(LABEL_1)として再定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53189385-ef49-45a3-b87a-9955c91395ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertJapaneseTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Prepare dataset\n",
    "dataset = load_dataset(\"tyqiangz/multilingual-sentiments\", \"japanese\")\n",
    "print(dataset)\n",
    "\n",
    "print(dataset[\"train\"].features)\n",
    "\n",
    "dataset = dataset.remove_columns([\"source\"])\n",
    "dataset = dataset.filter(lambda dataset: dataset[\"label\"] != 1)\n",
    "dataset = dataset.map(lambda dataset: {\"labels\": int(dataset[\"label\"] == 2)}, remove_columns=[\"label\"])\n",
    "\n",
    "print(dataset[\"train\"][20000])\n",
    "print(dataset[\"train\"][50000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0481f80-52cf-4255-a76e-c49a948247ec",
   "metadata": {},
   "source": [
    "次に、文章テキストのままだとモデルのトレーニングはできないため、テキストを意味のある単位で分割（トークナイズ）した上で数値に変換します。トークナイザーには MeCab ベースの BertJapaneseTokenizer を利用しました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f7c32e-c0c0-4a1e-92c2-4b6490f68917",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", max_length=128, truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle().select(range(4000))\n",
    "eval_dataset = tokenized_datasets[\"test\"].shuffle().select(range(256))\n",
    "\n",
    "# Save dataset\n",
    "train_dataset.save_to_disk(\"./train/\")\n",
    "eval_dataset.save_to_disk(\"./test/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4005724-1c52-4426-b7cb-16996462d29d",
   "metadata": {},
   "source": [
    "実際にどのように変換されているのか、以下のスクリプトを実行し確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229a9403-e9b7-407e-a6ce-71ecd69697d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 50000\n",
    "print(dataset[\"train\"][index])\n",
    "print('Tokenize:', tokenizer.tokenize(dataset[\"train\"]['text'][index]))\n",
    "print('Encode:', tokenizer.encode(dataset[\"train\"]['text'][index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b00108-8105-4190-b6ad-27effb7092d5",
   "metadata": {},
   "source": [
    "## Trainer API を使用した トレーニング（ファインチューニング）実行\n",
    "Transformers には Trainer という便利なクラスがあり、Torch Neuron からも利用可能です。 ここでは Trainer API を利用してトレーニングを実行していきます。\n",
    "\n",
    "With transformers==4.44.0, running one worker fine-tuning without torchrun would result in a hang. To workaround and run one worker fine-tuning, use `torchrun --nproc_per_node=1 <script>`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36508a19-4f9d-483c-b118-bbf66b2e85ef",
   "metadata": {},
   "source": [
    "### neuron_parallel_compile による事前コンパイル\n",
    "トレーニングの各ステップでは、グラフがトレースされ、トレースされたグラフが以前のものと異なる場合は、再度計算グラフのコンパイルが発生します。大規模なモデルの場合、各グラフのコンパイル時間が長くなることがあり、トレーニング時間の中で占めるコンパイル時間がボトルネックとなってしまう場合もあり得ます。このコンパイル時間を短縮するため、PyTorch Neuron では neuron_parallel_compile ユーティリティが提供されています。neuron_parallel_compile は、スクリプトの試行からグラフを抽出し並列事前コンパイルを実施、コンパイル結果（NEFF : Neuron Executable File Format）をキャッシュとしてディスク上に保持します。\n",
    "\n",
    "では実際に事前コンパイルを実行してみましょう。以下の内容でbert-jp-precompile.pyというファイル名の Python スクリプトを作成し実行します。スクリプトは基本的にこの後実行するトレーニングスクリプトと同じ内容ですが、neuron_parallel_compileはグラフの高速コンパイルのみを目的とし実際の演算は実行されず、出力結果は無効となります。トレーニング実行中も必要に応じてグラフはコンパイルされるため、この事前コンパイルのプロセスはスキップし、次のトレーニング処理に直接進んでも問題はありません。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6acfe7-0dc2-4341-af96-f837bc627583",
   "metadata": {},
   "source": [
    "コンパイル時間を短縮するためデータセット、epoch 数を制限している点にご注意ください。コンパイル結果は `/var/tmp/neuron-compile-cache/` 以下に保存されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8f769-91a2-4966-8b55-2e1df787d716",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bert-jp-precompile.py\n",
    "\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_from_disk\n",
    "import torch, torch_xla.core.xla_model as xm\n",
    "import os\n",
    "\n",
    "os.environ[\"NEURON_CC_FLAGS\"] = \"--model-type=transformer\"\n",
    "\n",
    "device = xm.xla_device()\n",
    "\n",
    "MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
    "\n",
    "train_dataset = load_from_disk(\"./train/\").with_format(\"torch\")\n",
    "train_dataset = train_dataset.select(range(64))\n",
    "\n",
    "eval_dataset = load_from_disk(\"./test/\").with_format(\"torch\")\n",
    "eval_dataset = eval_dataset.select(range(64))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs = 2,\n",
    "    learning_rate = 5e-5,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    output_dir = \"./results\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db73bea-dcee-4d21-badc-ea7844418195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!time XLA_USE_BF16=1 neuron_parallel_compile torchrun --nproc_per_node=1 bert-jp-precompile.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049a9a17-1e83-4628-a6f4-29fb5dff2d04",
   "metadata": {},
   "source": [
    "### シングルワーカーでのトレーニング実行\n",
    "次に実際にトレーニングを実行してみます。事前コンパイルを実行した場合でも、追加のコンパイルが発生することがあります。一通りのコンパイルが終了した後、2度目以降の実行では、Neuron コアの恩恵を受けた高速トレーニングを体験できます。以下の内容で bert-jp-single.py というファイル名の Python スクリプトを作成し実行してみましょう。\n",
    "\n",
    "先程の事前コンパイルとは異なり、今回は実際にトレーニングを実行するため、用意したデータセット全てに対して epoch = 10 で実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78747ec6-3e55-4b1a-b57f-a8f656b069a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bert-jp-single.py\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertJapaneseTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_from_disk\n",
    "import torch, torch_xla.core.xla_model as xm\n",
    "import os\n",
    "\n",
    "os.environ[\"NEURON_CC_FLAGS\"] = \"--model-type=transformer\"\n",
    "\n",
    "device = xm.xla_device()\n",
    "\n",
    "MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
    "\n",
    "train_dataset = load_from_disk(\"./train/\").with_format(\"torch\")\n",
    "eval_dataset = load_from_disk(\"./test/\").with_format(\"torch\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs = 10,\n",
    "    learning_rate = 5e-5,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    output_dir = \"./results\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    tokenizer = tokenizer,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(train_result)\n",
    "\n",
    "eval_result = trainer.evaluate()\n",
    "print(eval_result)\n",
    "\n",
    "trainer.save_model(\"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e1319d-30c2-4b9d-be36-07ae7a64fa57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!time XLA_USE_BF16=1 torchrun --nproc_per_node=1 bert-jp-single.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aae6d30-f843-45fd-b6ef-65a65f61763e",
   "metadata": {},
   "source": [
    "ステップ数 5000 のトレーニングが 9~10分程で完了しました。\n",
    "\n",
    "トレーニング実行中に、AWS Neuron で提供される `neuron-top` ツールを利用すると、Neuron コア及び vCPU の利用率、アクセラレータメモリ、ホストメモリの利用状況等を確認することができます。inf2.xlarge には、一つの Inferentia2 チップ、チップ内に二つの Neuron コアが搭載されています。結果から、二つある Neuron コア（NC0 及び NC1）のうち一つの Neuron コアのみが利用されていることが分かります。まだ最適化の余地はありそうです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4bb724-139b-4a2b-9e2c-c66023c65768",
   "metadata": {},
   "source": [
    "生成されたモデルから期待通りの出力が得られるか確認しておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5ea0d5-d506-42b5-9f24-0438bc5e742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model = \"./results/\")\n",
    "\n",
    "print(classifier(\"大変すばらしい商品でした。感激です。\"))\n",
    "print(classifier(\"期待していた商品とは異なりました。残念です。\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfdbb2d-d477-42e3-9967-f64fba883b0d",
   "metadata": {},
   "source": [
    "期待通りの出力を得られることが確認できたようです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28270199-5a61-4ec1-a045-833abc6c16e7",
   "metadata": {},
   "source": [
    "## torchrun を用いたマルチワーカーでのトレーニング実行\n",
    "それでは、先程のトレーニングスクリプトに変更を加え、二つある Neuron コアを有効活用してみましょう。複数の Neuron コアを利用したマルチワーカーで実行するためには `torchrun` コマンドを利用します。`torchrun` コマンドに対して、オプション `--nproc_per_node` で利用する Neuron コアの数（並列実行するワーカー数）を指定します。trn1.2xlarge (inf2.xlarge) では 2 を、trn1.32xlargeの場合は 2, 8, 32 が指定可能です。\n",
    "\n",
    "`torchrun` を利用したデータパラレルトレーニングを実行するにあたって、先程のスクリプトに一部変更を加えた `bert-jp-dual.py` というファイル名のスクリプトを作成し実行します。\n",
    "\n",
    "それでは変更後のスクリプトを利用して　inf2.xlarge 上の二つ Neuron コアを利用したトレーニングを実行してみましょう。シングルワーカーでのトレーニング結果と比較し `Total train batch size` の値が倍の 16 に、`Total optimization steps` が半分の 2500 となっている点を確認できると思います。\n",
    "\n",
    "シングルワーカー時の手順同様、まずは事前コンパイルを実行し、その後に実際のトレーニングを実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2837ccc-aac9-47d7-803e-3c75afccc426",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bert-jp-dual-precompile.py\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertJapaneseTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_from_disk\n",
    "import torch, torch_xla.distributed.xla_backend\n",
    "import os\n",
    "\n",
    "os.environ[\"NEURON_CC_FLAGS\"] = \"--model-type=transformer\"\n",
    "\n",
    "MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "train_dataset = load_from_disk(\"./train/\").with_format(\"torch\")\n",
    "train_dataset = train_dataset.select(range(64))\n",
    "\n",
    "eval_dataset = load_from_disk(\"./test/\").with_format(\"torch\")\n",
    "eval_dataset = eval_dataset.select(range(64))\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs = 2,\n",
    "    learning_rate = 5e-5,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    output_dir = \"./results\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(train_result)\n",
    "\n",
    "eval_result = trainer.evaluate()\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d164bb-deb5-4c34-bafa-8c66b2291ff5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!time XLA_USE_BF16=1 neuron_parallel_compile torchrun --nproc_per_node=2 bert-jp-dual-precompile.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bddfc6-2cb6-4383-b095-e6cb58a9e68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bert-jp-dual.py\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertJapaneseTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_from_disk\n",
    "import torch, torch_xla.distributed.xla_backend\n",
    "import os\n",
    "\n",
    "os.environ[\"NEURON_CC_FLAGS\"] = \"--model-type=transformer\"\n",
    "\n",
    "MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "train_dataset = load_from_disk(\"./train/\").with_format(\"torch\")\n",
    "eval_dataset = load_from_disk(\"./test/\").with_format(\"torch\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs = 10,\n",
    "    learning_rate = 5e-5,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    output_dir = \"./results\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    tokenizer = tokenizer,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(train_result)\n",
    "\n",
    "eval_result = trainer.evaluate()\n",
    "print(eval_result)\n",
    "\n",
    "trainer.save_model(\"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73d8ec0-162b-4d6a-bebf-a0b991b08213",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!time XLA_USE_BF16=1 NEURONCORE_NUM_DEVICES=2 torchrun --nproc_per_node=2 bert-jp-dual.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582fc974-a3fa-45ad-ba0b-1d3ac99d8f33",
   "metadata": {},
   "source": [
    "トレーニング実行中の neuron-top の出力も確認してみましょう。今度は二つの Neuron コアが利用されている事が確認できると思います。トレーニングに要する実行時間も 5~6 分に削減されました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c4c41-9152-4b02-91fa-d5897079427f",
   "metadata": {},
   "source": [
    "## 推論実行\n",
    "先ほどは生成されたモデルから期待通りの出力が得られるかどうかCPU上で推論実行し、結果を確認しました。ここでは生成されたモデルをinf2.xlarge上で推論実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b881d0f-28b5-49e5-849f-3f0266daa97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_neuronx\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import transformers\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertJapaneseTokenizer\n",
    "\n",
    "def encode(tokenizer, *inputs, max_length=128, batch_size=1):\n",
    "    tokens = tokenizer.encode_plus(\n",
    "        *inputs,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return (\n",
    "        torch.repeat_interleave(tokens['input_ids'], batch_size, 0),\n",
    "        torch.repeat_interleave(tokens['attention_mask'], batch_size, 0),\n",
    "        torch.repeat_interleave(tokens['token_type_ids'], batch_size, 0),\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./results/\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./results/\", torchscript=True)\n",
    "\n",
    "\n",
    "sequence = \"大変すばらしい商品でした。感激です。\"\n",
    "paraphrase = encode(tokenizer, sequence)\n",
    "cpu_paraphrase_logits = model(*paraphrase)[0]\n",
    "print('CPU paraphrase logits:', cpu_paraphrase_logits.detach().numpy())\n",
    "\n",
    "sequence = \"期待していた商品とは異なりました。残念です。\"\n",
    "paraphrase = encode(tokenizer, sequence)\n",
    "cpu_paraphrase_logits = model(*paraphrase)[0]\n",
    "print('CPU paraphrase logits:', cpu_paraphrase_logits.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e608f7d-16cb-4dc0-b7ac-253129e64df2",
   "metadata": {},
   "source": [
    "### Compile the model for Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bc7d66-7fdb-42d8-91f3-98708982fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee754ea0-97ae-4cd4-8184-4cd3b7fa6fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_neuron = torch_neuronx.trace(model, paraphrase)\n",
    "\n",
    "# Save the TorchScript for inference deployment\n",
    "torch.jit.save(model_neuron, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f805991-1929-4899-aa6d-46682b7e3acb",
   "metadata": {},
   "source": [
    "### Run inference and compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81e935f-0eab-4128-85db-047e5022e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_neuron = torch.jit.load(filename)\n",
    "\n",
    "sequence = \"大変すばらしい商品でした。感激です。\"\n",
    "paraphrase = encode(tokenizer, sequence)\n",
    "neuron_paraphrase_logits = model_neuron(*paraphrase)[0]\n",
    "print('Neuron paraphrase logits:', neuron_paraphrase_logits.detach().numpy())\n",
    "\n",
    "sequence = \"期待していた商品とは異なりました。残念です。\"\n",
    "paraphrase = encode(tokenizer, sequence)\n",
    "neuron_paraphrase_logits = model_neuron(*paraphrase)[0]\n",
    "print('Neuron paraphrase logits:', neuron_paraphrase_logits.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e3299b-f8ec-4083-8813-54b693270cb0",
   "metadata": {},
   "source": [
    "CPUで推論実行した結果と同様の結果が得られている事が確認できました。推論性能を評価する方法は以下のサンプルをご参照下さい。\n",
    "+ https://github.com/aws-neuron/aws-neuron-sdk/blob/master/src/examples/pytorch/torch-neuronx/bert-base-cased-finetuned-mrpc-inference-on-trn1-tutorial.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
