{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# AWS Trainium Single Neuron - \"bert-base-cased\" for Sentiment Analysis\n",
    "This notebook shows how to fine-tune a \"bert base cased\" PyTorch model with AWS Trainium (trn1 instances) using NeuronSDK. The original implementation is provided by HuggingFace.\n",
    "\n",
    "Our goal is building a Machine Learning model that will predict whether the tweet is offensive, neutral, or positive (<b>Sentiment Analysis</b>).\n",
    "\n",
    "The target variable is the **Sentiment**, which can be:\n",
    "* Neutral\n",
    "* Positive\n",
    "* Negative\n",
    "\n",
    "In this exercise you will do:\n",
    " - Run a PyTorch training by using a Single Neuron Core\n",
    "\n",
    "The example code referenced for this example is [trainium-single-core](./code/01-trainium-single-core/train.py)\n",
    "\n",
    "It has been tested and run on a **trn1.32xlarge**\n",
    "\n",
    "**Reference:** https://huggingface.co/bert-base-cased"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1 - Install dependencies\n",
    "\n",
    "Let's install some required dependencies for our environment."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! pip install datasets numpy<=1.20.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2 - Fine-Tune the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's take a look to our train.py code\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mcsv\u001B[39;49;00m\r\n",
      "\u001B[34mfrom\u001B[39;49;00m \u001B[04m\u001B[36mdatasets\u001B[39;49;00m \u001B[34mimport\u001B[39;49;00m Dataset, DatasetDict\r\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mlogging\u001B[39;49;00m\r\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mos\u001B[39;49;00m\r\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mpandas\u001B[39;49;00m \u001B[34mas\u001B[39;49;00m \u001B[04m\u001B[36mpd\u001B[39;49;00m\r\n",
      "\u001B[34mfrom\u001B[39;49;00m \u001B[04m\u001B[36mtime\u001B[39;49;00m \u001B[34mimport\u001B[39;49;00m gmtime, strftime\r\n",
      "\u001B[34mfrom\u001B[39;49;00m \u001B[04m\u001B[36mtqdm\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36mauto\u001B[39;49;00m \u001B[34mimport\u001B[39;49;00m tqdm\r\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mtorch\u001B[39;49;00m\r\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mtorch_xla\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36mcore\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36mxla_model\u001B[39;49;00m \u001B[34mas\u001B[39;49;00m \u001B[04m\u001B[36mxm\u001B[39;49;00m\r\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mtorch_xla\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36mdistributed\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36mparallel_loader\u001B[39;49;00m \u001B[34mas\u001B[39;49;00m \u001B[04m\u001B[36mpl\u001B[39;49;00m\r\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mtorch_xla\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36mdistributed\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36mxla_backend\u001B[39;49;00m\r\n",
      "\u001B[34mfrom\u001B[39;49;00m \u001B[04m\u001B[36mtorch\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36moptim\u001B[39;49;00m \u001B[34mimport\u001B[39;49;00m AdamW\r\n",
      "\u001B[34mfrom\u001B[39;49;00m \u001B[04m\u001B[36mtorch\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36mutils\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36mdata\u001B[39;49;00m \u001B[34mimport\u001B[39;49;00m DataLoader\r\n",
      "\u001B[34mfrom\u001B[39;49;00m \u001B[04m\u001B[36mtorch\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36mutils\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36mdata\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36mdistributed\u001B[39;49;00m \u001B[34mimport\u001B[39;49;00m DistributedSampler\r\n",
      "\u001B[34mfrom\u001B[39;49;00m \u001B[04m\u001B[36mtransformers\u001B[39;49;00m \u001B[34mimport\u001B[39;49;00m AutoTokenizer, AutoModelForSequenceClassification\r\n",
      "\r\n",
      "logging.basicConfig(level=logging.INFO)\r\n",
      "logger = logging.getLogger(\u001B[31m__name__\u001B[39;49;00m)\r\n",
      "\r\n",
      "torch.manual_seed(\u001B[34m0\u001B[39;49;00m)\r\n",
      "\r\n",
      "model_name = \u001B[33m\"\u001B[39;49;00m\u001B[33mbert-base-cased\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\r\n",
      "\u001B[37m## define xla as device for using AWS Trainium Neuron Cores\u001B[39;49;00m\r\n",
      "device = \u001B[33m\"\u001B[39;49;00m\u001B[33mxla\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\r\n",
      "\r\n",
      "torch.distributed.init_process_group(device)\r\n",
      "\r\n",
      "\u001B[37m# Get the global number of workes.\u001B[39;49;00m\r\n",
      "world_size = xm.xrt_world_size()\r\n",
      "logger.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mWorkers: \u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(world_size))\r\n",
      "\r\n",
      "batch_size = \u001B[34m8\u001B[39;49;00m\r\n",
      "num_epochs = \u001B[34m6\u001B[39;49;00m\r\n",
      "\r\n",
      "logger.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mDevice: \u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(device))\r\n",
      "\r\n",
      "\u001B[37m## tokenize_and_encode\u001B[39;49;00m\r\n",
      "\u001B[37m# params:\u001B[39;49;00m\r\n",
      "\u001B[37m#   data: DatasetDict\u001B[39;49;00m\r\n",
      "\u001B[37m# This method returns a dictionary of input_ids, token_type_ids, attention_mask\u001B[39;49;00m\r\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32mtokenize_and_encode\u001B[39;49;00m(data):\r\n",
      "    results = tokenizer(data[\u001B[33m\"\u001B[39;49;00m\u001B[33mtext\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m], padding=\u001B[33m\"\u001B[39;49;00m\u001B[33mmax_length\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, truncation=\u001B[34mTrue\u001B[39;49;00m)\r\n",
      "    \u001B[34mreturn\u001B[39;49;00m results\r\n",
      "\r\n",
      "\u001B[34mif\u001B[39;49;00m \u001B[31m__name__\u001B[39;49;00m == \u001B[33m'\u001B[39;49;00m\u001B[33m__main__\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m:\r\n",
      "\r\n",
      "    train = pd.read_csv(\r\n",
      "        \u001B[33m\"\u001B[39;49;00m\u001B[33m./../../data/train.csv\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\r\n",
      "        sep=\u001B[33m'\u001B[39;49;00m\u001B[33m,\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m,\r\n",
      "        quotechar=\u001B[33m'\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m,\r\n",
      "        quoting=csv.QUOTE_ALL,\r\n",
      "        escapechar=\u001B[33m'\u001B[39;49;00m\u001B[33m\\\\\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m,\r\n",
      "        encoding=\u001B[33m'\u001B[39;49;00m\u001B[33mutf-8\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m,\r\n",
      "        error_bad_lines=\u001B[34mFalse\u001B[39;49;00m\r\n",
      "    )\r\n",
      "\r\n",
      "    train_dataset = Dataset.from_dict(train)\r\n",
      "\r\n",
      "    hg_dataset = DatasetDict({\u001B[33m\"\u001B[39;49;00m\u001B[33mtrain\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m: train_dataset})\r\n",
      "\r\n",
      "    \u001B[37m## Loading Hugging Face AutoTokenizer for the defined model\u001B[39;49;00m\r\n",
      "    tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n",
      "\r\n",
      "    ds_encoded = hg_dataset.map(tokenize_and_encode, batched=\u001B[34mTrue\u001B[39;49;00m, remove_columns=[\u001B[33m\"\u001B[39;49;00m\u001B[33mtext\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m])\r\n",
      "\r\n",
      "    ds_encoded.set_format(\u001B[33m\"\u001B[39;49;00m\u001B[33mtorch\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\r\n",
      "\r\n",
      "    \u001B[37m## Create a subsed of data sampler, for parallelizing the training across multiple cores\u001B[39;49;00m\r\n",
      "    \u001B[34mif\u001B[39;49;00m world_size > \u001B[34m1\u001B[39;49;00m:\r\n",
      "        train_sampler = DistributedSampler(\r\n",
      "            ds_encoded[\u001B[33m\"\u001B[39;49;00m\u001B[33mtrain\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m],\r\n",
      "            num_replicas=world_size,\r\n",
      "            rank=xm.get_ordinal(),\r\n",
      "            shuffle=\u001B[34mTrue\u001B[39;49;00m,\r\n",
      "        )\r\n",
      "\r\n",
      "    \u001B[37m## Creating a DataLoader object for iterating over it during the training epochs\u001B[39;49;00m\r\n",
      "    train_dl = DataLoader(\r\n",
      "        ds_encoded[\u001B[33m\"\u001B[39;49;00m\u001B[33mtrain\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m],\r\n",
      "        batch_size=batch_size,\r\n",
      "        sampler=train_sampler,\r\n",
      "        shuffle=\u001B[34mFalse\u001B[39;49;00m \u001B[34mif\u001B[39;49;00m train_sampler \u001B[34melse\u001B[39;49;00m \u001B[34mTrue\u001B[39;49;00m)\r\n",
      "\r\n",
      "    \u001B[37m## Loading a subset of the data in the different Neuron Cores provided as input\u001B[39;49;00m\r\n",
      "    train_device_loader = pl.MpDeviceLoader(train_dl, device)\r\n",
      "\r\n",
      "    \u001B[37m## Loading Hugging Face pre-trained model for sequence classification for the defined model\u001B[39;49;00m\r\n",
      "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=\u001B[34m3\u001B[39;49;00m).to(device)\r\n",
      "\r\n",
      "    current_timestamp = strftime(\u001B[33m\"\u001B[39;49;00m\u001B[33m%\u001B[39;49;00m\u001B[33mY-\u001B[39;49;00m\u001B[33m%\u001B[39;49;00m\u001B[33mm-\u001B[39;49;00m\u001B[33m%d\u001B[39;49;00m\u001B[33m-\u001B[39;49;00m\u001B[33m%\u001B[39;49;00m\u001B[33mH-\u001B[39;49;00m\u001B[33m%\u001B[39;49;00m\u001B[33mM\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, gmtime())\r\n",
      "\r\n",
      "    optimizer = AdamW(model.parameters(), lr=\u001B[34m1.45e-4\u001B[39;49;00m * world_size)\r\n",
      "\r\n",
      "    num_training_steps = num_epochs * \u001B[36mlen\u001B[39;49;00m(train_dl)\r\n",
      "    progress_bar = tqdm(\u001B[36mrange\u001B[39;49;00m(num_training_steps))\r\n",
      "\r\n",
      "    logger.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mStart training: \u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(strftime(\u001B[33m\"\u001B[39;49;00m\u001B[33m%\u001B[39;49;00m\u001B[33mY-\u001B[39;49;00m\u001B[33m%\u001B[39;49;00m\u001B[33mm-\u001B[39;49;00m\u001B[33m%d\u001B[39;49;00m\u001B[33m \u001B[39;49;00m\u001B[33m%\u001B[39;49;00m\u001B[33mH:\u001B[39;49;00m\u001B[33m%\u001B[39;49;00m\u001B[33mM:\u001B[39;49;00m\u001B[33m%\u001B[39;49;00m\u001B[33mS\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, gmtime())))\r\n",
      "\r\n",
      "    \u001B[37m## Start model training and defining the training loop\u001B[39;49;00m\r\n",
      "    model.train()\r\n",
      "    \u001B[34mfor\u001B[39;49;00m epoch \u001B[35min\u001B[39;49;00m \u001B[36mrange\u001B[39;49;00m(num_epochs):\r\n",
      "        \u001B[34mfor\u001B[39;49;00m batch \u001B[35min\u001B[39;49;00m train_device_loader:\r\n",
      "            batch = {k: v.to(device) \u001B[34mfor\u001B[39;49;00m k, v \u001B[35min\u001B[39;49;00m batch.items()}\r\n",
      "            outputs = model(**batch)\r\n",
      "            optimizer.zero_grad()\r\n",
      "            loss = outputs.loss\r\n",
      "            loss.backward()\r\n",
      "            \u001B[37m## xm.optimizer_step is performing the sum of all the gradients updates done in the different Cores\u001B[39;49;00m\r\n",
      "            xm.optimizer_step(optimizer)\r\n",
      "            progress_bar.update(\u001B[34m1\u001B[39;49;00m)\r\n",
      "\r\n",
      "        logger.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mEpoch \u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m, rank \u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m, Loss \u001B[39;49;00m\u001B[33m{:0.4f}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(epoch, xm.get_ordinal(), loss.detach().to(\u001B[33m\"\u001B[39;49;00m\u001B[33mcpu\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)))\r\n",
      "\r\n",
      "    logger.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mEnd training: \u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(strftime(\u001B[33m\"\u001B[39;49;00m\u001B[33m%\u001B[39;49;00m\u001B[33mY-\u001B[39;49;00m\u001B[33m%\u001B[39;49;00m\u001B[33mm-\u001B[39;49;00m\u001B[33m%d\u001B[39;49;00m\u001B[33m \u001B[39;49;00m\u001B[33m%\u001B[39;49;00m\u001B[33mH:\u001B[39;49;00m\u001B[33m%\u001B[39;49;00m\u001B[33mM:\u001B[39;49;00m\u001B[33m%\u001B[39;49;00m\u001B[33mS\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, gmtime())))\r\n",
      "\r\n",
      "    \u001B[37m## Using XLA for saving model after training for being sure only one copy of the model is saved\u001B[39;49;00m\r\n",
      "    os.makedirs(\u001B[33m\"\u001B[39;49;00m\u001B[33m./../../models/checkpoints/\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(current_timestamp), exist_ok=\u001B[34mTrue\u001B[39;49;00m)\r\n",
      "    checkpoint = {\u001B[33m\"\u001B[39;49;00m\u001B[33mstate_dict\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m: model.state_dict()}\r\n",
      "    xm.save(checkpoint, \u001B[33m\"\u001B[39;49;00m\u001B[33m./../../models/checkpoints/\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m/checkpoint.pt\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(current_timestamp))\r\n"
     ]
    }
   ],
   "source": [
    "! pygmentize ./code/02-trainium-distributed-training/train.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the training script on a single Neuron Core"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Train model\")\n",
    "RUN_CMD = f\"\"\"python3 ./code/01-trainium-single-core/train.py\"\"\"\n",
    "\n",
    "print(f'Running command: \\n{RUN_CMD}')\n",
    "! {RUN_CMD}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}