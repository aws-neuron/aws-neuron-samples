{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb8d1e07",
   "metadata": {},
   "source": [
    "# BertBaseCased - Pytorch\n",
    "This notebook shows how to fine-tune a \"bert base cased\" PyTorch model with AWS Trainium (trn1 instances) using NeuronSDK. The original implementation is provided by HuggingFace.\n",
    "\n",
    "The example has 2 stages:\n",
    "1. First compile the model using the utility `neuron_parallel_compile` to compile the model to run on the AWS Trainium device.\n",
    "1. Run the fine-tuning script to train the model based on the associated task (e.g. mrpc). The training job will use 2 workers with data parallel to speed up the training. If you have a larger instance (trn1.32xlarge) you can increase the worker count to 8 or 32.\n",
    "\n",
    "It has been tested and run on a trn1.2xlarge\n",
    "\n",
    "**Reference:** https://huggingface.co/bert-base-cased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec7fb64",
   "metadata": {},
   "source": [
    "## 1) Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c66c6d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/ec2-user/.config/pip/pip.conf\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Set Pip repository  to point to the Neuron repository\n",
    "%pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com\n",
    "# now restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b410a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Neuron Compiler and Neuron/XLA packages\n",
    "%pip install -U torch-neuronx==\"1.11.0.1.*\" \"numpy<=1.20.0\" \"protobuf<4\" \"transformers==4.16.2\" datasets sklearn\n",
    "# use --force-reinstall if you're facing some issues while loading the modules\n",
    "# now restart the kernel again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02a757f",
   "metadata": {},
   "source": [
    "## 2) Set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6cbca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-cased\"\n",
    "env_var_options = \"XLA_USE_BF16=1 NEURON_CC_FLAGS=\\\"--model-type=transformer\\\"\"\n",
    "num_workers = 2\n",
    "task_name = \"mrpc\"\n",
    "batch_size = 8\n",
    "max_seq_length = 128\n",
    "learning_rate = 2e-05\n",
    "num_train_epochs = 5\n",
    "model_base_name = model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e067b78",
   "metadata": {},
   "source": [
    "## 3) Compile the model with neuron_parallel_compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "036583ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile model\n",
      "Running command: \n",
      "XLA_USE_BF16=1 NEURON_CC_FLAGS=\"--model-type=transformer\" neuron_parallel_compile python3 ./run_glue.py --model_name_or_path bert-base-cased --task_name mrpc --do_train --max_seq_length 128 --per_device_train_batch_size 8 --learning_rate 2e-05 --max_train_samples 128 --overwrite_output_dir --output_dir bert-base-cased-mrpc-8bs |& tee log_compile_bert-base-cased-mrpc-8bs\n",
      "2022-10-19 21:11:57.000896: INFO ||PARALLEL_COMPILE||: Removing existing workdir /tmp/parallel_compile_workdir\n",
      "2022-10-19 21:11:57.000898: INFO ||PARALLEL_COMPILE||: Running trial run (add option to terminate trial run early; also ignore trial run's generated outputs, i.e. loss, checkpoints)\n",
      "__main__: Process rank: -1, device: xla:1, n_gpu: 0distributed training: False, 16-bits training: False\n",
      "__main__: Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=bert-base-cased-mrpc-8bs/runs/Oct19_21-12-02_ip-172-31-51-63.us-west-2.compute.internal,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=bert-base-cased-mrpc-8bs,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=bert-base-cased-mrpc-8bs,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "datasets.info: Loading Dataset Infos from /home/ec2-user/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "datasets.builder: Overwrite dataset info from restored data version.\n",
      "datasets.info: Loading Dataset info from /home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "datasets.builder: Reusing dataset glue (/home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "datasets.info: Loading Dataset info from /home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "100%|██████████| 3/3 [00:00<00:00, 164.47it/s]\n",
      "[INFO|file_utils.py:2140] 2022-10-19 21:12:04,003 >> https://huggingface.co/bert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /home/ec2-user/.cache/huggingface/transformers/tmpsf6__nlg\n",
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 1.25MB/s]\n",
      "[INFO|file_utils.py:2144] 2022-10-19 21:12:04,263 >> storing https://huggingface.co/bert-base-cased/resolve/main/config.json in cache at /home/ec2-user/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "[INFO|file_utils.py:2152] 2022-10-19 21:12:04,263 >> creating metadata file for /home/ec2-user/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "[INFO|configuration_utils.py:644] 2022-10-19 21:12:04,263 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "[INFO|configuration_utils.py:680] 2022-10-19 21:12:04,264 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"mrpc\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|file_utils.py:2140] 2022-10-19 21:12:04,520 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /home/ec2-user/.cache/huggingface/transformers/tmp9qny3mws\n",
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 50.6kB/s]\n",
      "[INFO|file_utils.py:2144] 2022-10-19 21:12:04,790 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json in cache at /home/ec2-user/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "[INFO|file_utils.py:2152] 2022-10-19 21:12:04,790 >> creating metadata file for /home/ec2-user/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "[INFO|configuration_utils.py:644] 2022-10-19 21:12:05,041 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "[INFO|configuration_utils.py:680] 2022-10-19 21:12:05,041 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|file_utils.py:2140] 2022-10-19 21:12:05,580 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /home/ec2-user/.cache/huggingface/transformers/tmpp9b7ncob\n",
      "Downloading: 100%|██████████| 208k/208k [00:00<00:00, 674kB/s] \n",
      "[INFO|file_utils.py:2144] 2022-10-19 21:12:06,169 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /home/ec2-user/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "[INFO|file_utils.py:2152] 2022-10-19 21:12:06,169 >> creating metadata file for /home/ec2-user/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|file_utils.py:2140] 2022-10-19 21:12:06,424 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /home/ec2-user/.cache/huggingface/transformers/tmpsutvy1n4\n",
      "Downloading: 100%|██████████| 426k/426k [00:00<00:00, 1.22MB/s]\n",
      "[INFO|file_utils.py:2144] 2022-10-19 21:12:07,059 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /home/ec2-user/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
      "[INFO|file_utils.py:2152] 2022-10-19 21:12:07,059 >> creating metadata file for /home/ec2-user/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-19 21:12:07,846 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /home/ec2-user/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-19 21:12:07,846 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /home/ec2-user/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-19 21:12:07,846 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-19 21:12:07,846 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-19 21:12:07,846 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /home/ec2-user/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "[INFO|configuration_utils.py:644] 2022-10-19 21:12:08,105 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "[INFO|configuration_utils.py:680] 2022-10-19 21:12:08,105 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|file_utils.py:2140] 2022-10-19 21:12:08,480 >> https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /home/ec2-user/.cache/huggingface/transformers/tmp2_l8kzzw\n",
      "Downloading: 100%|██████████| 416M/416M [00:04<00:00, 88.4MB/s] \n",
      "[INFO|file_utils.py:2144] 2022-10-19 21:12:13,467 >> storing https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin in cache at /home/ec2-user/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
      "[INFO|file_utils.py:2152] 2022-10-19 21:12:13,467 >> creating metadata file for /home/ec2-user/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
      "[INFO|modeling_utils.py:1427] 2022-10-19 21:12:13,467 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /home/ec2-user/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
      "[WARNING|modeling_utils.py:1686] 2022-10-19 21:12:14,367 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1697] 2022-10-19 21:12:14,367 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset:   0%|          | 0/4 [00:00<?, ?ba/s]Ignored unknown kwarg option direction\n",
      "datasets.arrow_dataset: Caching processed dataset at /home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-933fca1f571e4614.arrow\n",
      "Ignored unknown kwarg option direction\n",
      "Running tokenizer on dataset:  50%|█████     | 2/4 [00:00<00:00, 13.22ba/s]Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Running tokenizer on dataset: 100%|██████████| 4/4 [00:00<00:00, 16.44ba/s]\n",
      "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]Ignored unknown kwarg option direction\n",
      "datasets.arrow_dataset: Caching processed dataset at /home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-a409d170fec07ae5.arrow\n",
      "Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00, 42.73ba/s]\n",
      "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]Ignored unknown kwarg option direction\n",
      "datasets.arrow_dataset: Caching processed dataset at /home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-711e17b78b98f8d8.arrow\n",
      "Ignored unknown kwarg option direction\n",
      "Running tokenizer on dataset: 100%|██████████| 2/2 [00:00<00:00, 21.35ba/s]\n",
      "__main__: Sample 28 of the training set: {'sentence1': \"Singapore is already the United States ' 12th-largest trading partner , with two-way trade totaling more than $ 34 billion .\", 'sentence2': 'Although a small city-state , Singapore is the 12th-largest trading partner of the United States , with trade volume of $ 33.4 billion last year .', 'label': 1, 'idx': 31, 'input_ids': [101, 4478, 1110, 1640, 1103, 1244, 1311, 112, 5247, 118, 2026, 6157, 3547, 117, 1114, 1160, 118, 1236, 2597, 24224, 1167, 1190, 109, 3236, 3775, 119, 102, 1966, 170, 1353, 1331, 118, 1352, 117, 4478, 1110, 1103, 5247, 118, 2026, 6157, 3547, 1104, 1103, 1244, 1311, 117, 1114, 2597, 3884, 1104, 109, 3081, 119, 125, 3775, 1314, 1214, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "__main__: Sample 6 of the training set: {'sentence1': 'The Nasdaq had a weekly gain of 17.27 , or 1.2 percent , closing at 1,520.15 on Friday .', 'sentence2': 'The tech-laced Nasdaq Composite .IXIC rallied 30.46 points , or 2.04 percent , to 1,520.15 .', 'label': 0, 'idx': 6, 'input_ids': [101, 1109, 11896, 1116, 1810, 4426, 1125, 170, 5392, 4361, 1104, 1542, 119, 1765, 117, 1137, 122, 119, 123, 3029, 117, 5134, 1120, 122, 117, 21338, 119, 1405, 1113, 5286, 119, 102, 1109, 13395, 118, 19498, 11896, 1116, 1810, 4426, 3291, 24729, 13068, 119, 12607, 9741, 27429, 1476, 119, 3993, 1827, 117, 1137, 123, 119, 5129, 3029, 117, 1106, 122, 117, 21338, 119, 1405, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "__main__: Sample 70 of the training set: {'sentence1': 'Kelly will begin meetings with Russian Deputy Foreign Minister Alexander Losyukov in Washington on Monday .', 'sentence2': 'Russian Deputy Foreign Minister Alexander Losyukov said in Moscow Tuesday a firm date would be fixed by this months end .', 'label': 0, 'idx': 81, 'input_ids': [101, 4368, 1209, 3295, 5845, 1114, 1938, 4831, 4201, 2110, 2792, 2238, 9379, 7498, 1107, 1994, 1113, 6356, 119, 102, 1938, 4831, 4201, 2110, 2792, 2238, 9379, 7498, 1163, 1107, 4116, 9667, 170, 3016, 2236, 1156, 1129, 4275, 1118, 1142, 1808, 1322, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:554] 2022-10-19 21:12:15,618 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx.\n",
      "/home/ec2-user/aws_neuron_venv_pytorch_p37/lib64/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1244] 2022-10-19 21:12:15,626 >> ***** Running training *****\n",
      "[INFO|trainer.py:1245] 2022-10-19 21:12:15,626 >>   Num examples = 128\n",
      "[INFO|trainer.py:1246] 2022-10-19 21:12:15,626 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1247] 2022-10-19 21:12:15,626 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1248] 2022-10-19 21:12:15,626 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1249] 2022-10-19 21:12:15,626 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1250] 2022-10-19 21:12:15,626 >>   Total optimization steps = 48\n",
      "  0%|          | 0/48 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2022-10-19 21:12:18.000030: DEBUG ||NCC_WRAPPER||: Compiling HLO: /tmp/MODULE_SyncTensorsGraph.206_9263903885196581569.hlo.pb \n",
      "2022-10-19 21:12:18.000031: INFO ||NCC_WRAPPER||: No candidate found under /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_9263903885196581569.\n",
      "2022-10-19 21:12:18.000031: INFO ||NCC_WRAPPER||: Cache dir for the neff: /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_9263903885196581569/MODULE_SyncTensorsGraph.206_9263903885196581569/39745b28-8a81-46fb-afb1-31e8817a73c1\n",
      "2022-10-19 21:12:18.000041: INFO ||NCC_WRAPPER||: Extracting graphs for ahead-of-time parallel compilation. Nocompilation was done.\n",
      "  2%|▏         | 1/48 [00:00<00:21,  2.19it/s]2022-10-19 21:12:18.000981: DEBUG ||NCC_WRAPPER||: Compiling HLO: /tmp/MODULE_SyncTensorsGraph.21351_2134296482958621629.hlo.pb \n",
      "2022-10-19 21:12:18.000982: INFO ||NCC_WRAPPER||: No candidate found under /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_2134296482958621629.\n",
      "2022-10-19 21:12:18.000983: INFO ||NCC_WRAPPER||: Cache dir for the neff: /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_2134296482958621629/MODULE_SyncTensorsGraph.21351_2134296482958621629/52a2dd39-1d74-4a24-bacb-0352edc31232\n",
      "2022-10-19 21:12:18.000988: INFO ||NCC_WRAPPER||: Extracting graphs for ahead-of-time parallel compilation. Nocompilation was done.\n",
      "  4%|▍         | 2/48 [00:02<00:56,  1.22s/it]2022-10-19 21:12:20.000784: DEBUG ||NCC_WRAPPER||: Compiling HLO: /tmp/MODULE_SyncTensorsGraph.21351_12152727777414806681.hlo.pb \n",
      "2022-10-19 21:12:20.000784: INFO ||NCC_WRAPPER||: No candidate found under /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_12152727777414806681.\n",
      "2022-10-19 21:12:20.000785: INFO ||NCC_WRAPPER||: Cache dir for the neff: /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_12152727777414806681/MODULE_SyncTensorsGraph.21351_12152727777414806681/563f7a25-7310-494e-9c59-255e2bfc54df\n",
      "2022-10-19 21:12:20.000791: INFO ||NCC_WRAPPER||: Extracting graphs for ahead-of-time parallel compilation. Nocompilation was done.\n",
      " 98%|█████████▊| 47/48 [00:06<00:00, 18.59it/s][INFO|trainer.py:1473] 2022-10-19 21:12:24,352 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "2022-10-19 21:12:24.000592: DEBUG ||NCC_WRAPPER||: Compiling HLO: /tmp/MODULE_SyncTensorsGraph.5983_13446926691378773034.hlo.pb \n",
      "2022-10-19 21:12:24.000593: INFO ||NCC_WRAPPER||: No candidate found under /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_13446926691378773034.\n",
      "2022-10-19 21:12:24.000594: INFO ||NCC_WRAPPER||: Cache dir for the neff: /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_13446926691378773034/MODULE_SyncTensorsGraph.5983_13446926691378773034/cf80056e-9937-4c46-b03b-e5eb2975368f\n",
      "2022-10-19 21:12:24.000599: INFO ||NCC_WRAPPER||: Extracting graphs for ahead-of-time parallel compilation. Nocompilation was done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:07<00:00,  6.82it/s]\n",
      "[INFO|trainer.py:2060] 2022-10-19 21:12:24,759 >> Saving model checkpoint to bert-base-cased-mrpc-8bs\n",
      "[INFO|configuration_utils.py:430] 2022-10-19 21:12:24,761 >> Configuration saved in bert-base-cased-mrpc-8bs/config.json\n",
      "2022-10-19 21:12:25.000533: DEBUG ||NCC_WRAPPER||: Compiling HLO: /tmp/MODULE_SyncTensorsGraph.20342_16825510754301468670.hlo.pb \n",
      "2022-10-19 21:12:25.000533: INFO ||NCC_WRAPPER||: No candidate found under /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_16825510754301468670.\n",
      "2022-10-19 21:12:25.000534: INFO ||NCC_WRAPPER||: Cache dir for the neff: /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_16825510754301468670/MODULE_SyncTensorsGraph.20342_16825510754301468670/186f72f8-bee8-41ba-9f7c-e1e64d8fa0f7\n",
      "2022-10-19 21:12:25.000540: INFO ||NCC_WRAPPER||: Extracting graphs for ahead-of-time parallel compilation. Nocompilation was done.\n",
      "[INFO|modeling_utils.py:1074] 2022-10-19 21:12:27,466 >> Model weights saved in bert-base-cased-mrpc-8bs/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2074] 2022-10-19 21:12:27,467 >> tokenizer config file saved in bert-base-cased-mrpc-8bs/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2080] 2022-10-19 21:12:27,467 >> Special tokens file saved in bert-base-cased-mrpc-8bs/special_tokens_map.json\n",
      "[INFO|modelcard.py:460] 2022-10-19 21:12:27,758 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'dataset': {'name': 'GLUE MRPC', 'type': 'glue', 'args': 'mrpc'}}\n",
      "{'train_runtime': 9.1311, 'train_samples_per_second': 42.054, 'train_steps_per_second': 5.257, 'train_loss': 0.0, 'epoch': 3.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  train_loss               =        0.0\n",
      "  train_runtime            = 0:00:09.13\n",
      "  train_samples            =        128\n",
      "  train_samples_per_second =     42.054\n",
      "  train_steps_per_second   =      5.257\n",
      "2022-10-19 21:12:29.000495: INFO ||PARALLEL_COMPILE||: Starting parallel compilations of the extracted graphs\n",
      "2022-10-19 21:12:29.000497: INFO ||PARALLEL_COMPILE||: Compiling /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.206_9263903885196581569.hlo.pb using following command: neuronx-cc compile --target=trn1 --framework XLA /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.206_9263903885196581569.hlo.pb --model-type=transformer --verbose=35 --output /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.206_9263903885196581569.neff\n",
      "2022-10-19 21:12:29.000498: INFO ||PARALLEL_COMPILE||: Compiling /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.21351_2134296482958621629.hlo.pb using following command: neuronx-cc compile --target=trn1 --framework XLA /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.21351_2134296482958621629.hlo.pb --model-type=transformer --verbose=35 --output /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.21351_2134296482958621629.neff\n",
      "..\n",
      "Compiler status PASS\n",
      "2022-10-19 21:12:34.000605: INFO ||PARALLEL_COMPILE||: Compiling /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.21351_12152727777414806681.hlo.pb using following command: neuronx-cc compile --target=trn1 --framework XLA /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.21351_12152727777414806681.hlo.pb --model-type=transformer --verbose=35 --output /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.21351_12152727777414806681.neff\n",
      "........................\n",
      "Compiler status PASS\n",
      "2022-10-19 21:16:33.000198: INFO ||PARALLEL_COMPILE||: Compiling /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.5983_13446926691378773034.hlo.pb using following command: neuronx-cc compile --target=trn1 --framework XLA /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.5983_13446926691378773034.hlo.pb --model-type=transformer --verbose=35 --output /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.5983_13446926691378773034.neff\n",
      "....\n",
      "Compiler status PASS\n",
      "2022-10-19 21:17:11.000818: INFO ||PARALLEL_COMPILE||: Compiling /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.20342_16825510754301468670.hlo.pb using following command: neuronx-cc compile --target=trn1 --framework XLA /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.20342_16825510754301468670.hlo.pb --model-type=transformer --verbose=35 --output /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.20342_16825510754301468670.neff\n",
      "...\n",
      "Compiler status PASS\n",
      "............\n",
      "Compiler status PASS\n",
      "2022-10-19 21:21:38.000961: INFO ||PARALLEL_COMPILE||: Compilation summary:\n",
      "2022-10-19 21:21:38.000962: INFO ||PARALLEL_COMPILE||: Compilation of /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.206_9263903885196581569.hlo.pb was successful. Plugged neff in the cache dir: /var/tmp\n",
      "2022-10-19 21:21:38.000962: INFO ||PARALLEL_COMPILE||: Compilation of /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.21351_2134296482958621629.hlo.pb was successful. Plugged neff in the cache dir: /var/tmp\n",
      "2022-10-19 21:21:38.000962: INFO ||PARALLEL_COMPILE||: Compilation of /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.5983_13446926691378773034.hlo.pb was successful. Plugged neff in the cache dir: /var/tmp\n",
      "2022-10-19 21:21:38.000962: INFO ||PARALLEL_COMPILE||: Compilation of /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.21351_12152727777414806681.hlo.pb was successful. Plugged neff in the cache dir: /var/tmp\n",
      "2022-10-19 21:21:38.000962: INFO ||PARALLEL_COMPILE||: Compilation of /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.20342_16825510754301468670.hlo.pb was successful. Plugged neff in the cache dir: /var/tmp\n",
      "2022-10-19 21:21:38.000962: INFO ||PARALLEL_COMPILE||: Total graphs: 5\n",
      "2022-10-19 21:21:38.000962: INFO ||PARALLEL_COMPILE||: Total successful compilations: 5\n",
      "2022-10-19 21:21:38.000962: INFO ||PARALLEL_COMPILE||: Total failed compilations: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Compile model\")\n",
    "COMPILE_CMD = f\"\"\"{env_var_options} neuron_parallel_compile python3 ./run_glue.py \\\n",
    "--model_name_or_path {model_name} \\\n",
    "--task_name {task_name} \\\n",
    "--do_train \\\n",
    "--max_seq_length {max_seq_length} \\\n",
    "--per_device_train_batch_size {batch_size} \\\n",
    "--learning_rate {learning_rate} \\\n",
    "--max_train_samples 128 \\\n",
    "--overwrite_output_dir \\\n",
    "--output_dir {model_base_name}-{task_name}-{batch_size}bs |& tee log_compile_{model_base_name}-{task_name}-{batch_size}bs\"\"\"\n",
    "\n",
    "print(f'Running command: \\n{COMPILE_CMD}')\n",
    "! {COMPILE_CMD}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80e2249",
   "metadata": {},
   "source": [
    "## 4) Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4ac998d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model\n",
      "Running command: \n",
      "XLA_USE_BF16=1 NEURON_CC_FLAGS=\"--model-type=transformer\" torchrun --nproc_per_node=2 ./run_glue.py --model_name_or_path bert-base-cased --task_name mrpc --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 8 --learning_rate 2e-05 --num_train_epochs 5 --overwrite_output_dir --output_dir bert-base-cased-mrpc-2w-8bs |& tee log_train_bert-base-cased-mrpc-2w-8bs\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "__main__: Process rank: 1, device: xla:0, n_gpu: 0distributed training: True, 16-bits training: False\n",
      "__main__: Process rank: 0, device: xla:1, n_gpu: 0distributed training: True, 16-bits training: False\n",
      "__main__: Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=bert-base-cased-mrpc-2w-8bs/runs/Oct19_21-21-49_ip-172-31-51-63.us-west-2.compute.internal,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=bert-base-cased-mrpc-2w-8bs,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=bert-base-cased-mrpc-2w-8bs,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "datasets.info: Loading Dataset Infos from /home/ec2-user/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "datasets.builder: Overwrite dataset info from restored data version.\n",
      "datasets.info: Loading Dataset info from /home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "datasets.builder: Reusing dataset glue (/home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "datasets.info: Loading Dataset info from /home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "100%|██████████| 3/3 [00:00<00:00, 1112.25it/s]\n",
      "datasets.builder: Reusing dataset glue (/home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1128.82it/s]\n",
      "[INFO|configuration_utils.py:644] 2022-10-19 21:21:50,293 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "[INFO|configuration_utils.py:680] 2022-10-19 21:21:50,293 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"mrpc\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:644] 2022-10-19 21:21:50,823 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "[INFO|configuration_utils.py:680] 2022-10-19 21:21:50,823 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-19 21:21:52,397 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /home/ec2-user/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-19 21:21:52,397 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /home/ec2-user/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-19 21:21:52,397 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-19 21:21:52,397 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-19 21:21:52,397 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /home/ec2-user/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "[INFO|configuration_utils.py:644] 2022-10-19 21:21:52,663 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "[INFO|configuration_utils.py:680] 2022-10-19 21:21:52,664 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1427] 2022-10-19 21:21:53,019 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /home/ec2-user/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
      "[WARNING|modeling_utils.py:1686] 2022-10-19 21:21:53,923 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1697] 2022-10-19 21:21:53,923 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[WARNING|modeling_utils.py:1686] 2022-10-19 21:21:53,926 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1697] 2022-10-19 21:21:53,926 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-933fca1f571e4614.arrow\n",
      "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]Ignored unknown kwarg option direction\n",
      "datasets.arrow_dataset: Caching processed dataset at /home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-939825a6cb95f0d3.arrow\n",
      "Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00, 32.48ba/s]\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-711e17b78b98f8d8.arrow\n",
      "__main__: Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [101, 1109, 10830, 1127, 1678, 1146, 1114, 24987, 1149, 13260, 1147, 1692, 1222, 7277, 2180, 5303, 117, 3455, 3081, 5097, 1104, 4961, 1149, 13260, 9966, 1222, 1140, 119, 102, 20661, 1127, 1678, 1146, 1114, 24987, 1149, 13260, 1147, 1692, 1222, 7277, 2180, 5303, 117, 3455, 170, 3081, 118, 3674, 21100, 2998, 1106, 1103, 2175, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "__main__: Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [101, 20394, 11252, 1424, 3878, 1684, 1111, 1103, 4116, 118, 5534, 1433, 1132, 170, 6539, 4010, 1111, 9283, 1105, 6646, 1110, 1919, 1344, 3075, 1104, 1397, 3625, 112, 188, 5200, 1728, 1107, 1594, 118, 7820, 20394, 11252, 15449, 119, 102, 9018, 1116, 1107, 20394, 11252, 15449, 112, 188, 4116, 118, 5534, 1433, 1132, 170, 6539, 4010, 1111, 9283, 117, 1105, 6646, 1110, 1919, 1344, 3075, 1104, 3625, 112, 188, 5200, 1728, 1107, 1103, 1594, 118, 187, 15677, 3660, 1805, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "__main__: Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [101, 6433, 111, 11767, 112, 188, 2260, 4482, 7448, 2174, 1116, 5799, 125, 119, 1969, 1827, 1106, 5103, 1495, 119, 1851, 117, 1229, 11896, 1116, 1810, 4426, 2174, 1116, 2204, 127, 119, 126, 1827, 1106, 122, 117, 20278, 119, 1851, 119, 102, 1109, 6433, 111, 11767, 112, 188, 2260, 10146, 1108, 1146, 122, 119, 3453, 1827, 117, 1137, 121, 119, 1407, 3029, 117, 1106, 5311, 1559, 119, 5599, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-933fca1f571e4614.arrow\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-939825a6cb95f0d3.arrow\n",
      "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Running tokenizer on dataset: 100%|██████████| 2/2 [00:00<00:00, 18.83ba/s]\n",
      "[INFO|trainer.py:554] 2022-10-19 21:21:54,731 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2.\n",
      "/home/ec2-user/aws_neuron_venv_pytorch_p37/lib64/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1244] 2022-10-19 21:21:54,739 >> ***** Running training *****\n",
      "[INFO|trainer.py:1245] 2022-10-19 21:21:54,739 >>   Num examples = 3668\n",
      "[INFO|trainer.py:1246] 2022-10-19 21:21:54,739 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:1247] 2022-10-19 21:21:54,739 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1248] 2022-10-19 21:21:54,739 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:1249] 2022-10-19 21:21:54,739 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1250] 2022-10-19 21:21:54,739 >>   Total optimization steps = 1150\n",
      "/home/ec2-user/aws_neuron_venv_pytorch_p37/lib64/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2022-10-19 21:21:55.000474: INFO ||NCC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_9263903885196581569/MODULE_SyncTensorsGraph.206_9263903885196581569/dd203394-023a-4962-9227-24e3602700f5/MODULE_SyncTensorsGraph.206_9263903885196581569.neff. Exiting with a successfully compiled graph\n",
      "  0%|          | 0/1150 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  0%|          | 1/1150 [00:00<06:19,  3.03it/s]2022-10-19 21:21:56.000581: INFO ||NCC_WRAPPER||: No candidate found under /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_17172443945148493437.\n",
      "2022-10-19 21:21:56.000582: INFO ||NCC_WRAPPER||: Cache dir for the neff: /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_17172443945148493437/MODULE_1_SyncTensorsGraph.22421_17172443945148493437_ip-172-31-51-63.us-west-2.compute.internal-9e985ef0-13942-5eb69cc607127/806b17fd-f437-426e-bdbb-dfd3f1bb2eac\n",
      "..............\n",
      "Compiler status PASS\n",
      "2022-10-19 21:26:24.000957: INFO ||NCC_WRAPPER||: Exiting with a successfully compiled graph\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  0%|          | 2/1150 [04:30<50:44:06, 159.10s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "ip-172-31-51-63:13942:14016 [1] init.cc:101 NCCL WARN OFI plugin initNet() failed is EFA enabled?\n",
      "2022-10-19 21:26:29.000501: INFO ||NCC_WRAPPER||: No candidate found under /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_12730472474735722736.\n",
      "2022-10-19 21:26:29.000502: INFO ||NCC_WRAPPER||: Cache dir for the neff: /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_12730472474735722736/MODULE_2_SyncTensorsGraph.22421_12730472474735722736_ip-172-31-51-63.us-west-2.compute.internal-76edf9f5-13942-5eb69dca4a142/61a36814-26e9-4307-b80b-ed0b2ddabc69\n",
      "................\n",
      "Compiler status PASS\n",
      "2022-10-19 21:31:47.000521: INFO ||NCC_WRAPPER||: Exiting with a successfully compiled graph\n",
      " 43%|████▎     | 499/1150 [10:38<00:55, 11.72it/s]  2022-10-19 21:32:34.000166: INFO ||NCC_WRAPPER||: No candidate found under /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_11145057064457806766.\n",
      "2022-10-19 21:32:34.000166: INFO ||NCC_WRAPPER||: Cache dir for the neff: /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_11145057064457806766/MODULE_3_SyncTensorsGraph.4_11145057064457806766_ip-172-31-51-63.us-west-2.compute.internal-bb026f56-13942-5eb69f260d5a3/e9a94680-22cd-4531-8153-c8b62526ffad\n",
      ".\n",
      "Compiler status PASS\n",
      "2022-10-19 21:32:36.000536: INFO ||NCC_WRAPPER||: Exiting with a successfully compiled graph\n",
      "{'loss': 0.407, 'learning_rate': 1.1304347826086957e-05, 'epoch': 2.17}\n",
      " 43%|████▎     | 500/1150 [10:41<00:55, 11.72it/s][INFO|trainer.py:2060] 2022-10-19 21:32:36,560 >> Saving model checkpoint to bert-base-cased-mrpc-2w-8bs/checkpoint-500\n",
      "[INFO|configuration_utils.py:430] 2022-10-19 21:32:36,561 >> Configuration saved in bert-base-cased-mrpc-2w-8bs/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1074] 2022-10-19 21:32:37,345 >> Model weights saved in bert-base-cased-mrpc-2w-8bs/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2074] 2022-10-19 21:32:37,346 >> tokenizer config file saved in bert-base-cased-mrpc-2w-8bs/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2080] 2022-10-19 21:32:37,346 >> Special tokens file saved in bert-base-cased-mrpc-2w-8bs/checkpoint-500/special_tokens_map.json\n",
      "2022-10-19 21:32:38.000959: INFO ||NCC_WRAPPER||: No candidate found under /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_4327322914518251210.\n",
      "2022-10-19 21:32:38.000960: INFO ||NCC_WRAPPER||: Cache dir for the neff: /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_4327322914518251210/MODULE_4_SyncTensorsGraph.6_4327322914518251210_ip-172-31-51-63.us-west-2.compute.internal-dcaa08ab-13942-5eb69f2a9dffe/5d3ab8c9-eeb1-4405-9c81-2fb3db91335d\n",
      ".\n",
      "Compiler status PASS\n",
      "2022-10-19 21:32:41.000357: INFO ||NCC_WRAPPER||: Exiting with a successfully compiled graph\n",
      "{'loss': 0.094, 'learning_rate': 2.6086956521739132e-06, 'epoch': 4.35}\n",
      " 87%|████████▋ | 1000/1150 [11:29<00:12, 11.81it/s][INFO|trainer.py:2060] 2022-10-19 21:33:24,605 >> Saving model checkpoint to bert-base-cased-mrpc-2w-8bs/checkpoint-1000\n",
      "[INFO|configuration_utils.py:430] 2022-10-19 21:33:24,607 >> Configuration saved in bert-base-cased-mrpc-2w-8bs/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1074] 2022-10-19 21:33:25,362 >> Model weights saved in bert-base-cased-mrpc-2w-8bs/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2074] 2022-10-19 21:33:25,363 >> tokenizer config file saved in bert-base-cased-mrpc-2w-8bs/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2080] 2022-10-19 21:33:25,363 >> Special tokens file saved in bert-base-cased-mrpc-2w-8bs/checkpoint-1000/special_tokens_map.json\n",
      "100%|█████████▉| 1149/1150 [11:44<00:00, 11.88it/s][INFO|trainer.py:1473] 2022-10-19 21:33:39,650 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "2022-10-19 21:33:39.000947: INFO ||NCC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_13446926691378773034/MODULE_SyncTensorsGraph.5983_13446926691378773034/1f621976-c785-4499-86cb-8f44424c107e/MODULE_SyncTensorsGraph.5983_13446926691378773034.neff. Exiting with a successfully compiled graph\n",
      "{'train_runtime': 705.5189, 'train_samples_per_second': 25.995, 'train_steps_per_second': 1.63, 'train_loss': 0.22230978260869566, 'epoch': 5.0}\n",
      "100%|██████████| 1150/1150 [11:44<00:00,  1.63it/s]\n",
      "[INFO|trainer.py:2060] 2022-10-19 21:33:40,259 >> Saving model checkpoint to bert-base-cased-mrpc-2w-8bs\n",
      "[INFO|configuration_utils.py:430] 2022-10-19 21:33:40,261 >> Configuration saved in bert-base-cased-mrpc-2w-8bs/config.json\n",
      "2022-10-19 21:33:41.000156: INFO ||NCC_WRAPPER||: No candidate found under /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_8408630903939688154.\n",
      "2022-10-19 21:33:41.000156: INFO ||NCC_WRAPPER||: Cache dir for the neff: /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_8408630903939688154/MODULE_6_SyncTensorsGraph.21412_8408630903939688154_ip-172-31-51-63.us-west-2.compute.internal-7a733dd3-13942-5eb69f65ec986/c03b27bf-5980-4414-9ef4-b497093c3863\n",
      "...............\n",
      "Compiler status PASS\n",
      "2022-10-19 21:38:42.000271: INFO ||NCC_WRAPPER||: Exiting with a successfully compiled graph\n",
      "[INFO|modeling_utils.py:1074] 2022-10-19 21:38:46,315 >> Model weights saved in bert-base-cased-mrpc-2w-8bs/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2074] 2022-10-19 21:38:46,316 >> tokenizer config file saved in bert-base-cased-mrpc-2w-8bs/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2080] 2022-10-19 21:38:46,316 >> Special tokens file saved in bert-base-cased-mrpc-2w-8bs/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  train_loss               =     0.2223\n",
      "  train_runtime            = 0:11:45.51\n",
      "  train_samples            =       3668\n",
      "  train_samples_per_second =     25.995\n",
      "  train_steps_per_second   =       1.63\n",
      "__main__: *** Evaluate ***\n",
      "[INFO|trainer.py:554] 2022-10-19 21:38:46,347 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2.\n",
      "[INFO|trainer.py:2340] 2022-10-19 21:38:46,348 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2342] 2022-10-19 21:38:46,348 >>   Num examples = 408\n",
      "[INFO|trainer.py:2345] 2022-10-19 21:38:46,349 >>   Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-19 21:38:47.000210: INFO ||NCC_WRAPPER||: No candidate found under /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_8461474067289264054.\n",
      "2022-10-19 21:38:47.000212: INFO ||NCC_WRAPPER||: Cache dir for the neff: /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_8461474067289264054/MODULE_7_SyncTensorsGraph.20606_8461474067289264054_ip-172-31-51-63.us-west-2.compute.internal-93b4b990-13942-5eb6a089c86ee/61ff5a58-4ed4-4c65-ae16-1fc43e0cd7c9\n",
      "..............\n",
      "Compiler status PASS\n",
      "2022-10-19 21:43:22.000345: INFO ||NCC_WRAPPER||: Exiting with a successfully compiled graph\n",
      "2022-10-19 21:43:22.000346: INFO ||NCC_WRAPPER||: No candidate found under /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_16960748577934655409.\n",
      "2022-10-19 21:43:22.000347: INFO ||NCC_WRAPPER||: Cache dir for the neff: /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_16960748577934655409/MODULE_8_SyncTensorsGraph.20610_16960748577934655409_ip-172-31-51-63.us-west-2.compute.internal-40519307-13942-5eb6a089dcf7a/721f8f49-6ee7-428d-98ff-c4aa342287f2\n",
      ".......\n",
      "ip-172-31-51-63:13942:14016 [1] include/socket.h:505 NCCL WARN Timeout waiting for RX (waited 120 sec) - retrying\n",
      "......\n",
      "ip-172-31-51-63:13942:14016 [1] include/socket.h:505 NCCL WARN Timeout waiting for RX (waited 120 sec) - retrying\n",
      ".\n",
      "Compiler status PASS\n",
      "2022-10-19 21:47:57.000570: INFO ||NCC_WRAPPER||: Exiting with a successfully compiled graph\n",
      "2022-10-19 21:47:57.000571: INFO ||NCC_WRAPPER||: No candidate found under /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_741325380437399185.\n",
      "2022-10-19 21:47:57.000571: INFO ||NCC_WRAPPER||: Cache dir for the neff: /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_741325380437399185/MODULE_9_SyncTensorsGraph.4548_741325380437399185_ip-172-31-51-63.us-west-2.compute.internal-93b4b990-13942-5eb6a1917428d/90775ab2-f5c6-4415-b291-a5b390aec861\n",
      "..\n",
      "Compiler status PASS\n",
      "2022-10-19 21:48:31.000689: INFO ||NCC_WRAPPER||: Exiting with a successfully compiled graph\n",
      "2022-10-19 21:48:32.000075: INFO ||NCC_WRAPPER||: No candidate found under /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_6638039383976247867.\n",
      "2022-10-19 21:48:32.000075: INFO ||NCC_WRAPPER||: Cache dir for the neff: /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_6638039383976247867/MODULE_10_SyncTensorsGraph.4_6638039383976247867_ip-172-31-51-63.us-west-2.compute.internal-ff0b9ba2-13942-5eb6a2b78c2cb/3ab43046-b73e-4515-b4e9-f6bb09433354\n",
      ".\n",
      "Compiler status PASS\n",
      "2022-10-19 21:48:34.000597: INFO ||NCC_WRAPPER||: Exiting with a successfully compiled graph\n",
      "2022-10-19 21:48:34.000747: INFO ||NCC_WRAPPER||: No candidate found under /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_16354810424380851617.\n",
      "2022-10-19 21:48:34.000748: INFO ||NCC_WRAPPER||: Cache dir for the neff: /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_16354810424380851617/MODULE_11_SyncTensorsGraph.4_16354810424380851617_ip-172-31-51-63.us-west-2.compute.internal-b2d67156-13942-5eb6a2ba18cda/fc5ea99f-b57f-4ca7-ab5f-cfc70bca8eed\n",
      ".\n",
      "Compiler status PASS\n",
      "2022-10-19 21:48:37.000241: INFO ||NCC_WRAPPER||: Exiting with a successfully compiled graph\n",
      " 92%|█████████▏| 24/26 [00:00<00:00, 31.23it/s]datasets.metric: Removing /home/ec2-user/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow\n",
      "100%|██████████| 26/26 [00:00<00:00, 28.94it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        5.0\n",
      "  eval_accuracy           =     0.8137\n",
      "  eval_combined_score     =     0.8378\n",
      "  eval_f1                 =     0.8618\n",
      "  eval_loss               =      0.755\n",
      "  eval_runtime            = 0:09:51.81\n",
      "  eval_samples            =        408\n",
      "  eval_samples_per_second =      0.689\n",
      "  eval_steps_per_second   =      0.044\n"
     ]
    }
   ],
   "source": [
    "print(\"Train model\")\n",
    "RUN_CMD = f\"\"\"{env_var_options} torchrun --nproc_per_node={num_workers} ./run_glue.py \\\n",
    "--model_name_or_path {model_name} \\\n",
    "--task_name {task_name} \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--max_seq_length {max_seq_length} \\\n",
    "--per_device_train_batch_size {batch_size} \\\n",
    "--learning_rate {learning_rate} \\\n",
    "--num_train_epochs {num_train_epochs} \\\n",
    "--overwrite_output_dir \\\n",
    "--output_dir {model_base_name}-{task_name}-{num_workers}w-{batch_size}bs |& tee log_train_{model_base_name}-{task_name}-{num_workers}w-{batch_size}bs\"\"\"\n",
    "\n",
    "print(f'Running command: \\n{RUN_CMD}')\n",
    "! {RUN_CMD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10792d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
