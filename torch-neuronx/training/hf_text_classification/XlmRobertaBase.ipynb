{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "fb8d1e07",
            "metadata": {},
            "source": [
                "# XlmRobertaBase- Pytorch\n",
                "This notebook shows how to fine-tune a \"xlm roberta base\" PyTorch model with AWS Trainium (trn1 instances) using NeuronSDK. The original implementation is provided by HuggingFace.\n",
                "\n",
                "The example has 2 stages:\n",
                "1. First compile the model using the utility `neuron_parallel_compile` to compile the model to run on the AWS Trainium device.\n",
                "1. Run the fine-tuning script to train the model based on the associated task (e.g. mrpc). The training job will use 2 workers with data parallel to speed up the training. If you have a larger instance (trn1.32xlarge) you can increase the worker count to 8 or 32.\n",
                "\n",
                "It has been tested and run on a trn1.32xlarge\n",
                "\n",
                "**Reference:** https://huggingface.co/xlm-roberta-base"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bec7fb64",
            "metadata": {},
            "source": [
                "## 1) Install dependencies"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Verify that this Jupyter notebook is running the Python kernel environment that was set up according to the [PyTorch Installation Guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/torch-neuronx.html#setup-torch-neuronx). You can select the kernel from the 'Kernel -> Change Kernel' option on the top of this Jupyter notebook page."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c4b410a6",
            "metadata": {},
            "outputs": [],
            "source": [
                "%env TOKENIZERS_PARALLELISM=True #Supresses tokenizer warnings making errors easier to detect\n",
                "#Install Neuron Compiler and Neuron/XLA packages\n",
                "%pip install -U \"protobuf<4\" \"transformers==4.27.3\" datasets scikit-learn evaluate\n",
                "# use --force-reinstall if you're facing some issues while loading the modules\n",
                "# now restart the kernel again"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "997b3d78",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone transformers from Gighub\n",
                "!git clone https://github.com/huggingface/transformers --branch v4.27.3"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c02a757f",
            "metadata": {},
            "source": [
                "## 2) Set the parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d6cbca3c",
            "metadata": {},
            "outputs": [],
            "source": [
                "model_name = \"xlm-roberta-base\"\n",
                "env_var_options = \"XLA_USE_BF16=1 NEURON_CC_FLAGS=\\'--model-type=transformer --verbose=info\\'\"\n",
                "num_workers = 32\n",
                "task_name = \"mrpc\"\n",
                "batch_size = 16\n",
                "max_seq_length = 512\n",
                "learning_rate = 2e-05\n",
                "num_train_epochs = 100\n",
                "model_base_name = model_name"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0e067b78",
            "metadata": {},
            "source": [
                "## 3) Compile the model with neuron_parallel_compile"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "036583ee",
            "metadata": {},
            "outputs": [],
            "source": [
                "import subprocess\n",
                "print(\"Compile model\")\n",
                "COMPILE_CMD = f\"\"\"{env_var_options} neuron_parallel_compile \\\n",
                "torchrun --nproc_per_node={num_workers} \\\n",
                "transformers/examples/pytorch/text-classification/run_glue.py \\\n",
                "--model_name_or_path {model_name} \\\n",
                "--task_name {task_name} \\\n",
                "--do_train \\\n",
                "--max_seq_length {max_seq_length} \\\n",
                "--per_device_train_batch_size {batch_size} \\\n",
                "--learning_rate {learning_rate} \\\n",
                "--max_train_samples 128 \\\n",
                "--overwrite_output_dir \\\n",
                "--output_dir {model_base_name}-{task_name}-{batch_size}bs\"\"\"\n",
                "\n",
                "print(f'Running command: \\n{COMPILE_CMD}')\n",
                "if subprocess.check_call(COMPILE_CMD,shell=True):\n",
                "   print(\"There was an error with the compilation command\")\n",
                "else:\n",
                "   print(\"Compilation Success!!!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c80e2249",
            "metadata": {},
            "source": [
                "## 4) Fine-tune the model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e4ac998d",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Train model\")\n",
                "RUN_CMD = f\"\"\"{env_var_options} torchrun --nproc_per_node={num_workers} \\\n",
                "transformers/examples/pytorch/text-classification/run_glue.py \\\n",
                "--model_name_or_path {model_name} \\\n",
                "--task_name {task_name} \\\n",
                "--do_train \\\n",
                "--do_eval \\\n",
                "--max_seq_length {max_seq_length} \\\n",
                "--per_device_train_batch_size {batch_size} \\\n",
                "--learning_rate {learning_rate} \\\n",
                "--num_train_epochs {num_train_epochs} \\\n",
                "--overwrite_output_dir \\\n",
                "--output_dir {model_base_name}-{task_name}-{num_workers}w-{batch_size}bs\"\"\"\n",
                "\n",
                "print(f'Running command: \\n{RUN_CMD}')\n",
                "if subprocess.check_call(RUN_CMD,shell=True):\n",
                "   print(\"There was an error with the fine-tune command\")\n",
                "else:\n",
                "   print(\"Fine-tune Successful!!!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "10792d8e",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        },
        "vscode": {
            "interpreter": {
                "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
