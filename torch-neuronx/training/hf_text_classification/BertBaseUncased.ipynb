{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb8d1e07",
   "metadata": {},
   "source": [
    "# BertBaseUncased - Pytorch\n",
    "This notebook shows how to fine-tune a \"bert base uncased\" PyTorch model with AWS Trainium (trn1 instances) using NeuronSDK. The original implementation is provided by HuggingFace.\n",
    "\n",
    "The example has 2 stages:\n",
    "1. First compile the model using the utility `neuron_parallel_compile` to compile the model to run on the AWS Trainium device.\n",
    "1. Run the fine-tuning script to train the model based on the associated task (e.g. mrpc). The training job will use 2 workers with data parallel to speed up the training. If you have a larger instance (trn1.32xlarge) you can increase the worker count to 8 or 32.\n",
    "\n",
    "It has been tested and run on a trn1.2xlarge\n",
    "\n",
    "**Reference:** https://huggingface.co/bert-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec7fb64",
   "metadata": {},
   "source": [
    "## 1) Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66c6d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pip repository  to point to the Neuron repository\n",
    "%pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com\n",
    "# now restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b410a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Neuron Compiler and Neuron/XLA packages\n",
    "%pip install -U torch-neuronx==\"1.11.0.1.*\" \"numpy<=1.20.0\" \"protobuf<4\" \"transformers==4.16.2\" datasets sklearn\n",
    "# use --force-reinstall if you're facing some issues while loading the modules\n",
    "# now restart the kernel again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02a757f",
   "metadata": {},
   "source": [
    "## 2) Set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6cbca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "env_var_options = \"XLA_USE_BF16=1 NEURON_CC_FLAGS=\\\"--model-type=transformer\\\"\"\n",
    "num_workers = 2\n",
    "task_name = \"mrpc\"\n",
    "batch_size = 8\n",
    "max_seq_length = 128\n",
    "learning_rate = 2e-05\n",
    "num_train_epochs = 5\n",
    "model_base_name = model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e067b78",
   "metadata": {},
   "source": [
    "## 3) Compile the model with neuron_parallel_compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "036583ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile model\n",
      "Running command: \n",
      "XLA_USE_BF16=1 NEURON_CC_FLAGS=\"--model-type=transformer\" neuron_parallel_compile python3 ./run_glue.py --model_name_or_path bert-base-uncased --task_name mrpc --do_train --max_seq_length 128 --per_device_train_batch_size 8 --learning_rate 2e-05 --max_train_samples 128 --overwrite_output_dir --output_dir bert-base-uncased-mrpc-8bs |& tee log_compile_bert-base-uncased-mrpc-8bs\n",
      "2022-10-19 21:50:13.000000: INFO ||PARALLEL_COMPILE||: Removing existing workdir /tmp/parallel_compile_workdir\n",
      "2022-10-19 21:50:13.000003: INFO ||PARALLEL_COMPILE||: Running trial run (add option to terminate trial run early; also ignore trial run's generated outputs, i.e. loss, checkpoints)\n",
      "__main__: Process rank: -1, device: xla:1, n_gpu: 0distributed training: False, 16-bits training: False\n",
      "__main__: Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=bert-base-uncased-mrpc-8bs/runs/Oct19_21-50-14_ip-172-31-51-63.us-west-2.compute.internal,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=bert-base-uncased-mrpc-8bs,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=bert-base-uncased-mrpc-8bs,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "datasets.info: Loading Dataset Infos from /home/ec2-user/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "datasets.builder: Overwrite dataset info from restored data version.\n",
      "datasets.info: Loading Dataset info from /home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "datasets.builder: Reusing dataset glue (/home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "datasets.info: Loading Dataset info from /home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "100%|██████████| 3/3 [00:00<00:00, 1106.97it/s]\n",
      "[INFO|file_utils.py:2140] 2022-10-19 21:50:15,485 >> https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /home/ec2-user/.cache/huggingface/transformers/tmpouqn21l5\n",
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 971kB/s]\n",
      "[INFO|file_utils.py:2144] 2022-10-19 21:50:15,749 >> storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /home/ec2-user/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|file_utils.py:2152] 2022-10-19 21:50:15,749 >> creating metadata file for /home/ec2-user/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:644] 2022-10-19 21:50:15,749 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:680] 2022-10-19 21:50:15,749 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"mrpc\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|file_utils.py:2140] 2022-10-19 21:50:16,005 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /home/ec2-user/.cache/huggingface/transformers/tmpgyj8ztid\n",
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 67.1kB/s]\n",
      "[INFO|file_utils.py:2144] 2022-10-19 21:50:16,269 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /home/ec2-user/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "[INFO|file_utils.py:2152] 2022-10-19 21:50:16,269 >> creating metadata file for /home/ec2-user/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "[INFO|configuration_utils.py:644] 2022-10-19 21:50:16,525 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:680] 2022-10-19 21:50:16,526 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|file_utils.py:2140] 2022-10-19 21:50:17,039 >> https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /home/ec2-user/.cache/huggingface/transformers/tmpt6kw93q6\n",
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 755kB/s] \n",
      "[INFO|file_utils.py:2144] 2022-10-19 21:50:17,623 >> storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /home/ec2-user/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|file_utils.py:2152] 2022-10-19 21:50:17,623 >> creating metadata file for /home/ec2-user/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|file_utils.py:2140] 2022-10-19 21:50:17,885 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /home/ec2-user/.cache/huggingface/transformers/tmpuoz9n_8y\n",
      "Downloading: 100%|██████████| 455k/455k [00:00<00:00, 1.23MB/s]\n",
      "[INFO|file_utils.py:2144] 2022-10-19 21:50:18,539 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /home/ec2-user/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|file_utils.py:2152] 2022-10-19 21:50:18,539 >> creating metadata file for /home/ec2-user/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-19 21:50:19,344 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/ec2-user/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-19 21:50:19,344 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/ec2-user/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-19 21:50:19,344 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-19 21:50:19,344 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-19 21:50:19,344 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/ec2-user/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "[INFO|configuration_utils.py:644] 2022-10-19 21:50:19,610 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:680] 2022-10-19 21:50:19,610 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|file_utils.py:2140] 2022-10-19 21:50:19,957 >> https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /home/ec2-user/.cache/huggingface/transformers/tmpn7ekppi9\n",
      "Downloading: 100%|██████████| 420M/420M [00:05<00:00, 86.9MB/s] \n",
      "[INFO|file_utils.py:2144] 2022-10-19 21:50:25,078 >> storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /home/ec2-user/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "[INFO|file_utils.py:2152] 2022-10-19 21:50:25,078 >> creating metadata file for /home/ec2-user/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "[INFO|modeling_utils.py:1427] 2022-10-19 21:50:25,079 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/ec2-user/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "[WARNING|modeling_utils.py:1686] 2022-10-19 21:50:25,979 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1697] 2022-10-19 21:50:25,979 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset:   0%|          | 0/4 [00:00<?, ?ba/s]Ignored unknown kwarg option direction\n",
      "datasets.arrow_dataset: Caching processed dataset at /home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-87d3b5a1597ccd6d.arrow\n",
      "Ignored unknown kwarg option direction\n",
      "Running tokenizer on dataset:  50%|█████     | 2/4 [00:00<00:00, 15.52ba/s]Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Running tokenizer on dataset: 100%|██████████| 4/4 [00:00<00:00, 17.40ba/s]\n",
      "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]Ignored unknown kwarg option direction\n",
      "datasets.arrow_dataset: Caching processed dataset at /home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-9aac6e9586d507d7.arrow\n",
      "Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00, 38.46ba/s]\n",
      "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]Ignored unknown kwarg option direction\n",
      "datasets.arrow_dataset: Caching processed dataset at /home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-90c7d45defef1156.arrow\n",
      "Ignored unknown kwarg option direction\n",
      "Running tokenizer on dataset: 100%|██████████| 2/2 [00:00<00:00, 19.37ba/s]\n",
      "__main__: Sample 28 of the training set: {'sentence1': \"Singapore is already the United States ' 12th-largest trading partner , with two-way trade totaling more than $ 34 billion .\", 'sentence2': 'Although a small city-state , Singapore is the 12th-largest trading partner of the United States , with trade volume of $ 33.4 billion last year .', 'label': 1, 'idx': 31, 'input_ids': [101, 5264, 2003, 2525, 1996, 2142, 2163, 1005, 5940, 1011, 2922, 6202, 4256, 1010, 2007, 2048, 1011, 2126, 3119, 21798, 2062, 2084, 1002, 4090, 4551, 1012, 102, 2348, 1037, 2235, 2103, 1011, 2110, 1010, 5264, 2003, 1996, 5940, 1011, 2922, 6202, 4256, 1997, 1996, 2142, 2163, 1010, 2007, 3119, 3872, 1997, 1002, 3943, 1012, 1018, 4551, 2197, 2095, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "__main__: Sample 6 of the training set: {'sentence1': 'The Nasdaq had a weekly gain of 17.27 , or 1.2 percent , closing at 1,520.15 on Friday .', 'sentence2': 'The tech-laced Nasdaq Composite .IXIC rallied 30.46 points , or 2.04 percent , to 1,520.15 .', 'label': 0, 'idx': 6, 'input_ids': [101, 1996, 17235, 2850, 4160, 2018, 1037, 4882, 5114, 1997, 2459, 1012, 2676, 1010, 2030, 1015, 1012, 1016, 3867, 1010, 5494, 2012, 1015, 1010, 19611, 1012, 2321, 2006, 5958, 1012, 102, 1996, 6627, 1011, 17958, 17235, 2850, 4160, 12490, 1012, 11814, 2594, 24356, 2382, 1012, 4805, 2685, 1010, 2030, 1016, 1012, 5840, 3867, 1010, 2000, 1015, 1010, 19611, 1012, 2321, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "__main__: Sample 70 of the training set: {'sentence1': 'Kelly will begin meetings with Russian Deputy Foreign Minister Alexander Losyukov in Washington on Monday .', 'sentence2': 'Russian Deputy Foreign Minister Alexander Losyukov said in Moscow Tuesday a firm date would be fixed by this months end .', 'label': 0, 'idx': 81, 'input_ids': [101, 5163, 2097, 4088, 6295, 2007, 2845, 4112, 3097, 2704, 3656, 3050, 10513, 7724, 1999, 2899, 2006, 6928, 1012, 102, 2845, 4112, 3097, 2704, 3656, 3050, 10513, 7724, 2056, 1999, 4924, 9857, 1037, 3813, 3058, 2052, 2022, 4964, 2011, 2023, 2706, 2203, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:554] 2022-10-19 21:50:27,126 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1.\n",
      "/home/ec2-user/aws_neuron_venv_pytorch_p37/lib64/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1244] 2022-10-19 21:50:27,133 >> ***** Running training *****\n",
      "[INFO|trainer.py:1245] 2022-10-19 21:50:27,133 >>   Num examples = 128\n",
      "[INFO|trainer.py:1246] 2022-10-19 21:50:27,133 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1247] 2022-10-19 21:50:27,133 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1248] 2022-10-19 21:50:27,133 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1249] 2022-10-19 21:50:27,133 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1250] 2022-10-19 21:50:27,134 >>   Total optimization steps = 48\n",
      "  0%|          | 0/48 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2022-10-19 21:50:28.000057: DEBUG ||NCC_WRAPPER||: Compiling HLO: /tmp/MODULE_SyncTensorsGraph.206_13827541762401764204.hlo.pb \n",
      "2022-10-19 21:50:28.000058: INFO ||NCC_WRAPPER||: No candidate found under /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_13827541762401764204.\n",
      "2022-10-19 21:50:28.000058: INFO ||NCC_WRAPPER||: Cache dir for the neff: /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_13827541762401764204/MODULE_SyncTensorsGraph.206_13827541762401764204/8e8848d0-45bd-4a95-b912-c525dc9052e3\n",
      "2022-10-19 21:50:28.000063: INFO ||NCC_WRAPPER||: Extracting graphs for ahead-of-time parallel compilation. Nocompilation was done.\n",
      "  2%|▏         | 1/48 [00:00<00:16,  2.77it/s]2022-10-19 21:50:28.000964: DEBUG ||NCC_WRAPPER||: Compiling HLO: /tmp/MODULE_SyncTensorsGraph.21351_18104846064719385361.hlo.pb \n",
      "2022-10-19 21:50:28.000965: INFO ||NCC_WRAPPER||: No candidate found under /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_18104846064719385361.\n",
      "2022-10-19 21:50:28.000966: INFO ||NCC_WRAPPER||: Cache dir for the neff: /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_18104846064719385361/MODULE_SyncTensorsGraph.21351_18104846064719385361/8158cba7-2020-4dbe-a4df-ee329f283d46\n",
      "2022-10-19 21:50:28.000971: INFO ||NCC_WRAPPER||: Extracting graphs for ahead-of-time parallel compilation. Nocompilation was done.\n",
      "  4%|▍         | 2/48 [00:02<00:53,  1.16s/it]2022-10-19 21:50:30.000745: DEBUG ||NCC_WRAPPER||: Compiling HLO: /tmp/MODULE_SyncTensorsGraph.21351_6463299337364174202.hlo.pb \n",
      "2022-10-19 21:50:30.000745: INFO ||NCC_WRAPPER||: No candidate found under /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_6463299337364174202.\n",
      "2022-10-19 21:50:30.000746: INFO ||NCC_WRAPPER||: Cache dir for the neff: /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_6463299337364174202/MODULE_SyncTensorsGraph.21351_6463299337364174202/6e385ea3-86c1-4602-8400-65bdfef2c63e\n",
      "2022-10-19 21:50:30.000752: INFO ||NCC_WRAPPER||: Extracting graphs for ahead-of-time parallel compilation. Nocompilation was done.\n",
      " 98%|█████████▊| 47/48 [00:06<00:00, 18.58it/s][INFO|trainer.py:1473] 2022-10-19 21:50:34,300 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "2022-10-19 21:50:34.000547: DEBUG ||NCC_WRAPPER||: Compiling HLO: /tmp/MODULE_SyncTensorsGraph.5983_6498126092765017155.hlo.pb \n",
      "2022-10-19 21:50:34.000547: INFO ||NCC_WRAPPER||: No candidate found under /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_6498126092765017155.\n",
      "2022-10-19 21:50:34.000548: INFO ||NCC_WRAPPER||: Cache dir for the neff: /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_6498126092765017155/MODULE_SyncTensorsGraph.5983_6498126092765017155/e0cc634b-6b63-455f-9a20-5f227f2cf0c6\n",
      "2022-10-19 21:50:34.000553: INFO ||NCC_WRAPPER||: Extracting graphs for ahead-of-time parallel compilation. Nocompilation was done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  6.95it/s]\n",
      "[INFO|trainer.py:2060] 2022-10-19 21:50:34,712 >> Saving model checkpoint to bert-base-uncased-mrpc-8bs\n",
      "[INFO|configuration_utils.py:430] 2022-10-19 21:50:34,714 >> Configuration saved in bert-base-uncased-mrpc-8bs/config.json\n",
      "2022-10-19 21:50:35.000488: DEBUG ||NCC_WRAPPER||: Compiling HLO: /tmp/MODULE_SyncTensorsGraph.20342_9371962420284060645.hlo.pb \n",
      "2022-10-19 21:50:35.000488: INFO ||NCC_WRAPPER||: No candidate found under /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_9371962420284060645.\n",
      "2022-10-19 21:50:35.000490: INFO ||NCC_WRAPPER||: Cache dir for the neff: /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_9371962420284060645/MODULE_SyncTensorsGraph.20342_9371962420284060645/c118aad9-35c4-4b28-a576-a6df651f7a5e\n",
      "2022-10-19 21:50:35.000495: INFO ||NCC_WRAPPER||: Extracting graphs for ahead-of-time parallel compilation. Nocompilation was done.\n",
      "[INFO|modeling_utils.py:1074] 2022-10-19 21:50:37,387 >> Model weights saved in bert-base-uncased-mrpc-8bs/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2074] 2022-10-19 21:50:37,388 >> tokenizer config file saved in bert-base-uncased-mrpc-8bs/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2080] 2022-10-19 21:50:37,388 >> Special tokens file saved in bert-base-uncased-mrpc-8bs/special_tokens_map.json\n",
      "[INFO|modelcard.py:460] 2022-10-19 21:50:37,711 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'dataset': {'name': 'GLUE MRPC', 'type': 'glue', 'args': 'mrpc'}}\n",
      "{'train_runtime': 7.5772, 'train_samples_per_second': 50.678, 'train_steps_per_second': 6.335, 'train_loss': 0.0, 'epoch': 3.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  train_loss               =        0.0\n",
      "  train_runtime            = 0:00:07.57\n",
      "  train_samples            =        128\n",
      "  train_samples_per_second =     50.678\n",
      "  train_steps_per_second   =      6.335\n",
      "2022-10-19 21:50:39.000184: INFO ||PARALLEL_COMPILE||: Starting parallel compilations of the extracted graphs\n",
      "2022-10-19 21:50:39.000185: INFO ||PARALLEL_COMPILE||: Compiling /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.206_13827541762401764204.hlo.pb using following command: neuronx-cc compile --target=trn1 --framework XLA /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.206_13827541762401764204.hlo.pb --model-type=transformer --verbose=35 --output /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.206_13827541762401764204.neff\n",
      "2022-10-19 21:50:39.000186: INFO ||PARALLEL_COMPILE||: Compiling /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.21351_18104846064719385361.hlo.pb using following command: neuronx-cc compile --target=trn1 --framework XLA /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.21351_18104846064719385361.hlo.pb --model-type=transformer --verbose=35 --output /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.21351_18104846064719385361.neff\n",
      "..\n",
      "Compiler status PASS\n",
      "2022-10-19 21:50:42.000778: INFO ||PARALLEL_COMPILE||: Compiling /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.21351_6463299337364174202.hlo.pb using following command: neuronx-cc compile --target=trn1 --framework XLA /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.21351_6463299337364174202.hlo.pb --model-type=transformer --verbose=35 --output /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.21351_6463299337364174202.neff\n",
      ".......................\n",
      "Compiler status PASS\n",
      "2022-10-19 21:54:41.000053: INFO ||PARALLEL_COMPILE||: Compiling /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.5983_6498126092765017155.hlo.pb using following command: neuronx-cc compile --target=trn1 --framework XLA /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.5983_6498126092765017155.hlo.pb --model-type=transformer --verbose=35 --output /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.5983_6498126092765017155.neff\n",
      "....\n",
      "Compiler status PASS\n",
      "2022-10-19 21:55:19.000156: INFO ||PARALLEL_COMPILE||: Compiling /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.20342_9371962420284060645.hlo.pb using following command: neuronx-cc compile --target=trn1 --framework XLA /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.20342_9371962420284060645.hlo.pb --model-type=transformer --verbose=35 --output /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.20342_9371962420284060645.neff\n",
      "....\n",
      "Compiler status PASS\n",
      "............\n",
      "Compiler status PASS\n",
      "2022-10-19 21:59:47.000710: INFO ||PARALLEL_COMPILE||: Compilation summary:\n",
      "2022-10-19 21:59:47.000710: INFO ||PARALLEL_COMPILE||: Compilation of /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.206_13827541762401764204.hlo.pb was successful. Plugged neff in the cache dir: /var/tmp\n",
      "2022-10-19 21:59:47.000711: INFO ||PARALLEL_COMPILE||: Compilation of /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.21351_18104846064719385361.hlo.pb was successful. Plugged neff in the cache dir: /var/tmp\n",
      "2022-10-19 21:59:47.000711: INFO ||PARALLEL_COMPILE||: Compilation of /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.5983_6498126092765017155.hlo.pb was successful. Plugged neff in the cache dir: /var/tmp\n",
      "2022-10-19 21:59:47.000711: INFO ||PARALLEL_COMPILE||: Compilation of /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.21351_6463299337364174202.hlo.pb was successful. Plugged neff in the cache dir: /var/tmp\n",
      "2022-10-19 21:59:47.000711: INFO ||PARALLEL_COMPILE||: Compilation of /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.20342_9371962420284060645.hlo.pb was successful. Plugged neff in the cache dir: /var/tmp\n",
      "2022-10-19 21:59:47.000711: INFO ||PARALLEL_COMPILE||: Total graphs: 5\n",
      "2022-10-19 21:59:47.000711: INFO ||PARALLEL_COMPILE||: Total successful compilations: 5\n",
      "2022-10-19 21:59:47.000711: INFO ||PARALLEL_COMPILE||: Total failed compilations: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Compile model\")\n",
    "COMPILE_CMD = f\"\"\"{env_var_options} neuron_parallel_compile python3 ./run_glue.py \\\n",
    "--model_name_or_path {model_name} \\\n",
    "--task_name {task_name} \\\n",
    "--do_train \\\n",
    "--max_seq_length {max_seq_length} \\\n",
    "--per_device_train_batch_size {batch_size} \\\n",
    "--learning_rate {learning_rate} \\\n",
    "--max_train_samples 128 \\\n",
    "--overwrite_output_dir \\\n",
    "--output_dir {model_base_name}-{task_name}-{batch_size}bs |& tee log_compile_{model_base_name}-{task_name}-{batch_size}bs\"\"\"\n",
    "\n",
    "print(f'Running command: \\n{COMPILE_CMD}')\n",
    "! {COMPILE_CMD}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80e2249",
   "metadata": {},
   "source": [
    "## 4) Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ac998d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model\n",
      "Running command: \n",
      "XLA_USE_BF16=1 NEURON_CC_FLAGS=\"--model-type=transformer\" torchrun --nproc_per_node=2 ./run_glue.py --model_name_or_path bert-base-uncased --task_name mrpc --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 8 --learning_rate 2e-05 --num_train_epochs 5 --overwrite_output_dir --output_dir bert-base-uncased-mrpc-2w-8bs |& tee log_train_bert-base-uncased-mrpc-2w-8bs\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "__main__: Process rank: 1, device: xla:0, n_gpu: 0distributed training: True, 16-bits training: False\n",
      "__main__: Process rank: 0, device: xla:1, n_gpu: 0distributed training: True, 16-bits training: False\n",
      "__main__: Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=bert-base-uncased-mrpc-2w-8bs/runs/Oct19_22-00-45_ip-172-31-51-63.us-west-2.compute.internal,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=bert-base-uncased-mrpc-2w-8bs,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=bert-base-uncased-mrpc-2w-8bs,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "datasets.info: Loading Dataset Infos from /home/ec2-user/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "datasets.builder: Overwrite dataset info from restored data version.\n",
      "datasets.info: Loading Dataset info from /home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "datasets.builder: Reusing dataset glue (/home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "datasets.info: Loading Dataset info from /home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "100%|██████████| 3/3 [00:00<00:00, 944.03it/s]\n",
      "datasets.builder: Reusing dataset glue (/home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1144.63it/s]\n",
      "[INFO|configuration_utils.py:644] 2022-10-19 22:00:46,231 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:680] 2022-10-19 22:00:46,231 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"mrpc\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:644] 2022-10-19 22:00:46,892 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:680] 2022-10-19 22:00:46,893 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-19 22:00:48,485 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/ec2-user/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-19 22:00:48,485 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/ec2-user/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-19 22:00:48,485 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-19 22:00:48,485 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-19 22:00:48,485 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/ec2-user/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "[INFO|configuration_utils.py:644] 2022-10-19 22:00:48,745 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:680] 2022-10-19 22:00:48,746 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1427] 2022-10-19 22:00:49,095 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/ec2-user/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "[WARNING|modeling_utils.py:1686] 2022-10-19 22:00:49,903 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1697] 2022-10-19 22:00:49,903 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[WARNING|modeling_utils.py:1686] 2022-10-19 22:00:50,020 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1697] 2022-10-19 22:00:50,020 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-87d3b5a1597ccd6d.arrow\n",
      "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]Ignored unknown kwarg option direction\n",
      "datasets.arrow_dataset: Caching processed dataset at /home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-f535fcec85096ec8.arrow\n",
      "Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00, 29.93ba/s]\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-90c7d45defef1156.arrow\n",
      "__main__: Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [101, 1996, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 3943, 5530, 1997, 5491, 2041, 16992, 9989, 2114, 2032, 1012, 102, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 1037, 3943, 1011, 3931, 19238, 3661, 2000, 1996, 2457, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "__main__: Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [101, 18178, 8661, 4584, 2551, 2005, 1996, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 2279, 4465, 1005, 1055, 4883, 2602, 1999, 2162, 1011, 7950, 18178, 2818, 17238, 1012, 102, 4584, 1999, 18178, 2818, 17238, 1005, 1055, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1010, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 4465, 1005, 1055, 4883, 2602, 1999, 1996, 2162, 1011, 25537, 2555, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "__main__: Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [101, 3115, 1004, 3532, 1005, 1055, 3156, 4518, 5950, 17795, 6430, 1018, 1012, 2871, 2685, 2000, 5818, 2509, 1012, 2753, 1010, 2096, 17235, 2850, 4160, 17795, 3062, 1020, 1012, 1019, 2685, 2000, 1015, 1010, 18744, 1012, 2753, 1012, 102, 1996, 3115, 1004, 3532, 1005, 1055, 3156, 5950, 2001, 2039, 1015, 1012, 4293, 2685, 1010, 2030, 1014, 1012, 2324, 3867, 1010, 2000, 5989, 2581, 1012, 6273, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-87d3b5a1597ccd6d.arrow\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-f535fcec85096ec8.arrow\n",
      "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Running tokenizer on dataset: 100%|██████████| 2/2 [00:00<00:00, 17.51ba/s]\n",
      "[INFO|trainer.py:554] 2022-10-19 22:00:50,805 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1.\n",
      "/home/ec2-user/aws_neuron_venv_pytorch_p37/lib64/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1244] 2022-10-19 22:00:50,812 >> ***** Running training *****\n",
      "[INFO|trainer.py:1245] 2022-10-19 22:00:50,812 >>   Num examples = 3668\n",
      "[INFO|trainer.py:1246] 2022-10-19 22:00:50,812 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:1247] 2022-10-19 22:00:50,812 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1248] 2022-10-19 22:00:50,812 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:1249] 2022-10-19 22:00:50,812 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1250] 2022-10-19 22:00:50,812 >>   Total optimization steps = 1150\n",
      "/home/ec2-user/aws_neuron_venv_pytorch_p37/lib64/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2022-10-19 22:00:51.000573: INFO ||NCC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_13827541762401764204/MODULE_SyncTensorsGraph.206_13827541762401764204/0fdcb379-5870-4594-9af2-5f1270e36192/MODULE_SyncTensorsGraph.206_13827541762401764204.neff. Exiting with a successfully compiled graph\n",
      "  0%|          | 0/1150 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  0%|          | 1/1150 [00:00<07:29,  2.55it/s]2022-10-19 22:00:52.000650: INFO ||NCC_WRAPPER||: No candidate found under /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_14402530733840298482.\n",
      "2022-10-19 22:00:52.000651: INFO ||NCC_WRAPPER||: Cache dir for the neff: /var/tmp/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_14402530733840298482/MODULE_1_SyncTensorsGraph.22421_14402530733840298482_ip-172-31-51-63.us-west-2.compute.internal-5ff0aa0-6952-5eb6a579e045a/ad8e2a58-45eb-4da8-aed1-419b6a965317\n",
      "............."
     ]
    }
   ],
   "source": [
    "print(\"Train model\")\n",
    "RUN_CMD = f\"\"\"{env_var_options} torchrun --nproc_per_node={num_workers} ./run_glue.py \\\n",
    "--model_name_or_path {model_name} \\\n",
    "--task_name {task_name} \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--max_seq_length {max_seq_length} \\\n",
    "--per_device_train_batch_size {batch_size} \\\n",
    "--learning_rate {learning_rate} \\\n",
    "--num_train_epochs {num_train_epochs} \\\n",
    "--overwrite_output_dir \\\n",
    "--output_dir {model_base_name}-{task_name}-{num_workers}w-{batch_size}bs |& tee log_train_{model_base_name}-{task_name}-{num_workers}w-{batch_size}bs\"\"\"\n",
    "\n",
    "print(f'Running command: \\n{RUN_CMD}')\n",
    "! {RUN_CMD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10792d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
