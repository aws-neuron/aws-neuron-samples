{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# VisionTextDualEncoder and CLIP-base model training - Pytorch\n",
                "This notebook shows how to fine-tune a pretrained HuggingFace CLIP-base PyTorch model with AWS Trainium (trn1 instances) using NeuronSDK.\n",
                "The original implementation is provided by HuggingFace.\n",
                "\n",
                "The example has 2 stages:\n",
                "1. First compile the model using the utility `neuron_parallel_compile` to compile the model to run on the AWS Trainium device.\n",
                "1. Run the fine-tuning script to train the model based on image classification task. The training job will use 32 workers with data parallel to speed up the training.\n",
                "\n",
                "It has been tested and run on trn1.32xlarge instance\n",
                "\n",
                "**Reference:** \n",
                "\n",
                "https://huggingface.co/openai/clip-vit-base-patch32"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1) Install dependencies"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Verify that this Jupyter notebook is running the Python kernel environment that was set up according to the [PyTorch Installation Guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/torch-neuronx.html#setup-torch-neuronx). You can select the kernel from the 'Kernel -> Change Kernel' option on the top of this Jupyter notebook page."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Supresses tokenizer warnings making errors easier to detect\n",
                "%env TOKENIZERS_PARALLELISM=True\n",
                "#Install Neuron Compiler and Neuron/XLA packages\n",
                "%pip install -U \"numpy<=1.20.0\" \"protobuf<4\" \"transformers==4.27.3\" datasets scikit-learn \n",
                "# use --force-reinstall if you're facing some issues while loading the modules\n",
                "# now restart the kernel again"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2) Set the parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Common Parameters\n",
                "text_model_name = \"roberta-base\"\n",
                "env_var_options = \"MALLOC_ARENA_MAX=64 XLA_USE_BF16=1 NEURON_CC_FLAGS=\\\"--cache_dir=./compiler_cache\\\"\"\n",
                "num_workers = 32\n",
                "task_name = \"contrastive-image-text\"\n",
                "dataset_name = \"ydshieh/coco_dataset_script\"\n",
                "transformers_version = \"4.27.3\"\n",
                "learning_rate = 5e-5\n",
                "dataloader_num_workers = 2\n",
                "device_prefetch_size = 2\n",
                "host_to_device_transfer_threads = 4"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Parameters for HuggingFace CLIP-base model\n",
                "model_name = \"openai/clip-vit-base-patch32\"\n",
                "model_base_name = \"clip-base\"\n",
                "per_device_train_batch_size = 64\n",
                "per_device_eval_batch_size = 64"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3) Download COCO dataset\n",
                "This example uses COCO dataset (2017) through a custom dataset script, which requires users to manually download the COCO dataset before training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!mkdir -p data\n",
                "!wget http://images.cocodataset.org/zips/train2017.zip -P data\n",
                "!wget http://images.cocodataset.org/zips/val2017.zip -P data\n",
                "!wget http://images.cocodataset.org/zips/test2017.zip -P data\n",
                "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -P data\n",
                "!wget http://images.cocodataset.org/annotations/image_info_test2017.zip -P data"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4) Compile the model with neuron_parallel_compile"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "import subprocess\n",
                "print(\"Compile model\")\n",
                "COMPILE_CMD = f\"\"\"{env_var_options} neuron_parallel_compile torchrun --nproc_per_node={num_workers} \\\n",
                "    run_clip.py \\\n",
                "    --model_name_or_path {model_name} \\\n",
                "    --text_model_name_or_path {text_model_name} \\\n",
                "    --data_dir $PWD/data \\\n",
                "    --dataset_config_name=2017 \\\n",
                "    --dataset_name {dataset_name} \\\n",
                "    --image_column image_path \\\n",
                "    --caption_column caption \\\n",
                "    --do_train \\\n",
                "    --max_steps 10 \\\n",
                "    --num_train_epochs 2 \\\n",
                "    --per_device_train_batch_size {per_device_train_batch_size} \\\n",
                "    --per_device_eval_batch_size {per_device_eval_batch_size} \\\n",
                "    --learning_rate {learning_rate} \\\n",
                "    --warmup_steps 0 \\\n",
                "    --weight_decay 0.1 \\\n",
                "    --save_strategy epoch \\\n",
                "    --save_total_limit 1 \\\n",
                "    --seed 1337 \\\n",
                "    --remove_unused_columns False \\\n",
                "    --overwrite_output_dir \\\n",
                "    --output_dir {model_base_name}-{task_name} \\\n",
                "    --device_prefetch_size {device_prefetch_size} \\\n",
                "    --host_to_device_transfer_threads {host_to_device_transfer_threads}\"\"\"\n",
                "\n",
                "print(f'Running command: \\n{COMPILE_CMD}')\n",
                "if subprocess.check_call(COMPILE_CMD,shell=True):\n",
                "   print(\"There was an error with the compilation command\")\n",
                "else:\n",
                "   print(\"Compilation Success!!!\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5) Fine-tune the model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "print(\"Train model\")\n",
                "RUN_CMD = f\"\"\"{env_var_options} torchrun --nproc_per_node={num_workers} \\\n",
                "    run_clip.py \\\n",
                "    --model_name_or_path {model_name} \\\n",
                "    --text_model_name_or_path roberta-base \\\n",
                "    --data_dir $PWD/data \\\n",
                "    --dataset_config_name=2017 \\\n",
                "    --dataset_name {dataset_name} \\\n",
                "    --image_column image_path \\\n",
                "    --caption_column caption \\\n",
                "    --do_train \\\n",
                "    --do_eval \\\n",
                "    --num_train_epochs 2 \\\n",
                "    --per_device_train_batch_size {per_device_train_batch_size} \\\n",
                "    --per_device_eval_batch_size {per_device_eval_batch_size} \\\n",
                "    --learning_rate {learning_rate} \\\n",
                "    --warmup_steps 0 \\\n",
                "    --weight_decay 0.1 \\\n",
                "    --save_strategy epoch \\\n",
                "    --save_total_limit 1 \\\n",
                "    --seed 1337 \\\n",
                "    --remove_unused_columns False \\\n",
                "    --overwrite_output_dir \\\n",
                "    --output_dir {model_base_name}-{task_name} \\\n",
                "    --device_prefetch_size {device_prefetch_size} \\\n",
                "    --host_to_device_transfer_threads {host_to_device_transfer_threads}\"\"\"\n",
                "\n",
                "print(f'Running command: \\n{RUN_CMD}')\n",
                "if subprocess.check_call(RUN_CMD,shell=True):\n",
                "   print(\"There was an error with the fine-tune command\")\n",
                "else:\n",
                "   print(\"Fine-tune Successful!!!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (torch-neuronx)",
            "language": "python",
            "name": "aws_neuron_venv_pytorch"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
