{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 - Pytorch\n",
    "This notebook shows how to fine-tune a \"gpt2\" PyTorch model with AWS Trainium (trn1 instances) using NeuronSDK. The original implementation is provided by HuggingFace.\n",
    "\n",
    "The example has 2 stages:\n",
    "1. First compile the model using the utility `neuron_parallel_compile` to compile the model to run on the AWS Trainium device.\n",
    "1. Run the fine-tuning script to train the model based on causal language modeling (CLM) loss. The training job will use 2 workers with data parallel to speed up the training. If you have a larger instance (trn1.32xlarge) you can increase the worker count to 8 or 32.\n",
    "\n",
    "It has been tested and run on a trn1.2xlarge\n",
    "\n",
    "**Reference:** https://huggingface.co/gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pip repository  to point to the Neuron repository\n",
    "%pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com\n",
    "# now restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Neuron Compiler and Neuron/XLA packages\n",
    "%pip install -U torch-neuronx==\"1.11.0.1.*\" \"numpy<=1.20.0\" \"protobuf<4\" \"transformers==4.16.2\" datasets sklearn\n",
    "# use --force-reinstall if you're facing some issues while loading the modules\n",
    "# now restart the kernel again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "model_name = \"gpt2\"\n",
    "extra_pip_packages = \"\"\n",
    "extra_yum_packages = \"\"\n",
    "env_var_options = \"XLA_USE_BF16=1 NEURON_CC_FLAGS=\\\"--cache_dir=./compiler_cache --model-type=transformer\\\"\"\n",
    "num_workers = 2\n",
    "task_name = \"clm\"\n",
    "dataset_name = \"wikitext\"\n",
    "dataset_config_name = \"wikitext-2-raw-v1\"\n",
    "work_dir = \"/home/ec2-user/language_modeling\"\n",
    "transformers_version = \"4.16.2\"\n",
    "model_base_name = \"gpt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Compile the model with neuron_parallel_compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile model\n",
      "Running command: \n",
      "XLA_USE_BF16=1 NEURON_CC_FLAGS=\"--cache_dir=./compiler_cache --model-type=transformer\" neuron_parallel_compile torchrun --nproc_per_node=2 ./run_clm.py     --model_name_or_path gpt2     --dataset_name wikitext     --dataset_config_name wikitext-2-raw-v1     --per_device_train_batch_size 4     --per_device_eval_batch_size 4     --do_train     --overwrite_output_dir     --output_dir gpt2-clm |& tee log_compile_gpt2-clm-precompile\n",
      "2022-10-20 08:52:16.000555: INFO ||PARALLEL_COMPILE||: Running trial run (add option to terminate trial run early; also ignore trial run's generated outputs, i.e. loss, checkpoints)\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "__main__: Process rank: 1, device: xla:0, n_gpu: 0distributed training: True, 16-bits training: False\n",
      "__main__: Process rank: 0, device: xla:1, n_gpu: 0distributed training: True, 16-bits training: False\n",
      "__main__: Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=gpt2-clm/runs/Oct20_08-52-20_ip-172-31-33-220.us-west-2.compute.internal,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=gpt2-clm,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=gpt2-clm,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "datasets.info: Loading Dataset Infos from /home/ec2-user/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
      "datasets.builder: Reusing dataset wikitext (/home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1206.42it/s]\n",
      "datasets.builder: Overwrite dataset info from restored data version.\n",
      "datasets.info: Loading Dataset info from /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
      "datasets.builder: Reusing dataset wikitext (/home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "datasets.info: Loading Dataset info from /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
      "100%|██████████| 3/3 [00:00<00:00, 1340.03it/s]\n",
      "[INFO|configuration_utils.py:644] 2022-10-20 08:52:21,124 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "[INFO|configuration_utils.py:680] 2022-10-20 08:52:21,125 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:336] 2022-10-20 08:52:21,384 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:644] 2022-10-20 08:52:21,652 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "[INFO|configuration_utils.py:680] 2022-10-20 08:52:21,653 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-20 08:52:23,492 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /home/ec2-user/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-20 08:52:23,492 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /home/ec2-user/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-20 08:52:23,492 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /home/ec2-user/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-20 08:52:23,492 >> loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-20 08:52:23,492 >> loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-20 08:52:23,492 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:644] 2022-10-20 08:52:23,761 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "[INFO|configuration_utils.py:680] 2022-10-20 08:52:23,761 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1427] 2022-10-20 08:52:24,064 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /home/ec2-user/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
      "[INFO|modeling_utils.py:1694] 2022-10-20 08:52:25,290 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:1703] 2022-10-20 08:52:25,290 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-ea01c5c1a9516ec4.arrow\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-20186e78bbd5f962.arrow\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-2323331079437bec.arrow\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-db28f3f9a817a2e3.arrow\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-d999d6c7e07e0a26.arrow\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-1162bb89ee1e8f9c.arrow\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-ea01c5c1a9516ec4.arrow\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-20186e78bbd5f962.arrow\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-2323331079437bec.arrow\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-db28f3f9a817a2e3.arrow\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-d999d6c7e07e0a26.arrow\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-1162bb89ee1e8f9c.arrow\n",
      "/home/ec2-user/aws_neuron_venv_pytorch_p37/lib64/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1244] 2022-10-20 08:52:26,151 >> ***** Running training *****\n",
      "[INFO|trainer.py:1245] 2022-10-20 08:52:26,151 >>   Num examples = 2318\n",
      "[INFO|trainer.py:1246] 2022-10-20 08:52:26,151 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1247] 2022-10-20 08:52:26,151 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:1248] 2022-10-20 08:52:26,151 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1249] 2022-10-20 08:52:26,151 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1250] 2022-10-20 08:52:26,151 >>   Total optimization steps = 870\n",
      "/home/ec2-user/aws_neuron_venv_pytorch_p37/lib64/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "  0%|          | 0/870 [00:00<?, ?it/s]2022-10-20 08:52:26.000987: DEBUG ||NCC_WRAPPER||: Compiling HLO: /tmp/MODULE_SyncTensorsGraph.164_14973442252103645712.hlo.pb \n",
      "2022-10-20 08:52:26.000992: INFO ||NCC_WRAPPER||: No candidate found under compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_14973442252103645712.\n",
      "2022-10-20 08:52:26.000995: INFO ||NCC_WRAPPER||: Cache dir for the neff: compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_14973442252103645712/MODULE_SyncTensorsGraph.164_14973442252103645712/5d3a8fac-ab01-4666-b00e-bd4b60229b3d\n",
      "2022-10-20 08:52:27.000003: INFO ||NCC_WRAPPER||: Extracting graphs for ahead-of-time parallel compilation. Nocompilation was done.\n",
      "  0%|          | 1/870 [00:00<02:12,  6.58it/s]2022-10-20 08:52:27.000793: DEBUG ||NCC_WRAPPER||: Compiling HLO: /tmp/MODULE_SyncTensorsGraph.17137_15887835957493241746.hlo.pb \n",
      "2022-10-20 08:52:27.000793: INFO ||NCC_WRAPPER||: No candidate found under compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_15887835957493241746.\n",
      "2022-10-20 08:52:27.000795: INFO ||NCC_WRAPPER||: Cache dir for the neff: compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_15887835957493241746/MODULE_SyncTensorsGraph.17137_15887835957493241746/815b8f36-4ea1-4499-bd3f-8c2d89c77604\n",
      "2022-10-20 08:52:27.000800: INFO ||NCC_WRAPPER||: Extracting graphs for ahead-of-time parallel compilation. Nocompilation was done.\n",
      "  0%|          | 2/870 [00:01<12:57,  1.12it/s]2022-10-20 08:52:29.000262: DEBUG ||NCC_WRAPPER||: Compiling HLO: /tmp/MODULE_SyncTensorsGraph.16849_1762241719395940600.hlo.pb \n",
      "2022-10-20 08:52:29.000262: INFO ||NCC_WRAPPER||: No candidate found under compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_1762241719395940600.\n",
      "2022-10-20 08:52:29.000266: INFO ||NCC_WRAPPER||: Cache dir for the neff: compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_1762241719395940600/MODULE_SyncTensorsGraph.16849_1762241719395940600/26842b02-8e44-4691-ba62-aa0a6fa4b28e\n",
      "2022-10-20 08:52:29.000271: INFO ||NCC_WRAPPER||: Extracting graphs for ahead-of-time parallel compilation. Nocompilation was done.\n",
      " 57%|█████▋    | 498/870 [00:25<00:16, 22.57it/s]2022-10-20 08:52:52.000955: DEBUG ||NCC_WRAPPER||: Compiling HLO: /tmp/MODULE_SyncTensorsGraph.4_11145057064457806766.hlo.pb \n",
      "2022-10-20 08:52:52.000957: INFO ||NCC_WRAPPER||: No candidate found under compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_11145057064457806766.\n",
      "2022-10-20 08:52:52.000960: INFO ||NCC_WRAPPER||: Cache dir for the neff: compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_11145057064457806766/MODULE_SyncTensorsGraph.4_11145057064457806766/abe93737-9061-4bca-bfd3-ffebd5b52214\n",
      "2022-10-20 08:52:52.000965: INFO ||NCC_WRAPPER||: Extracting graphs for ahead-of-time parallel compilation. Nocompilation was done.\n",
      "{'loss': 0.0, 'learning_rate': 2.1264367816091954e-05, 'epoch': 1.72}\n",
      " 57%|█████▋    | 500/870 [00:26<00:16, 22.57it/s][INFO|trainer.py:2060] 2022-10-20 08:52:53,203 >> Saving model checkpoint to gpt2-clm/checkpoint-500\n",
      "[INFO|configuration_utils.py:430] 2022-10-20 08:52:53,206 >> Configuration saved in gpt2-clm/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1074] 2022-10-20 08:52:54,603 >> Model weights saved in gpt2-clm/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2074] 2022-10-20 08:52:54,603 >> tokenizer config file saved in gpt2-clm/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2080] 2022-10-20 08:52:54,603 >> Special tokens file saved in gpt2-clm/checkpoint-500/special_tokens_map.json\n",
      "2022-10-20 08:52:56.000848: DEBUG ||NCC_WRAPPER||: Compiling HLO: /tmp/MODULE_SyncTensorsGraph.6_4327322914518251210.hlo.pb \n",
      "2022-10-20 08:52:56.000849: INFO ||NCC_WRAPPER||: No candidate found under compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_4327322914518251210.\n",
      "2022-10-20 08:52:56.000849: INFO ||NCC_WRAPPER||: Cache dir for the neff: compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_4327322914518251210/MODULE_SyncTensorsGraph.6_4327322914518251210/9836c7a2-b997-40f2-a541-d67b14dbcfde\n",
      "2022-10-20 08:52:56.000854: INFO ||NCC_WRAPPER||: Extracting graphs for ahead-of-time parallel compilation. Nocompilation was done.\n",
      "100%|█████████▉| 869/870 [00:46<00:00, 25.19it/s][INFO|trainer.py:1473] 2022-10-20 08:53:13,899 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "2022-10-20 08:53:14.000218: DEBUG ||NCC_WRAPPER||: Compiling HLO: /tmp/MODULE_SyncTensorsGraph.4645_4437454503677049592.hlo.pb \n",
      "2022-10-20 08:53:14.000218: INFO ||NCC_WRAPPER||: No candidate found under compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_4437454503677049592.\n",
      "2022-10-20 08:53:14.000219: INFO ||NCC_WRAPPER||: Cache dir for the neff: compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_4437454503677049592/MODULE_SyncTensorsGraph.4645_4437454503677049592/2686a7b7-c29b-4b63-930a-2d55a1d2af53\n",
      "2022-10-20 08:53:14.000224: INFO ||NCC_WRAPPER||: Extracting graphs for ahead-of-time parallel compilation. Nocompilation was done.\n",
      "{'train_runtime': 48.2186, 'train_samples_per_second': 144.218, 'train_steps_per_second': 18.043, 'train_loss': -2.3852819683908047e-05, 'epoch': 3.0}\n",
      "100%|██████████| 870/870 [00:47<00:00, 18.36it/s]\n",
      "[INFO|trainer.py:2060] 2022-10-20 08:53:14,372 >> Saving model checkpoint to gpt2-clm\n",
      "[INFO|configuration_utils.py:430] 2022-10-20 08:53:15,388 >> Configuration saved in gpt2-clm/config.json\n",
      "2022-10-20 08:53:16.000140: DEBUG ||NCC_WRAPPER||: Compiling HLO: /tmp/MODULE_SyncTensorsGraph.16105_8742604621763275.hlo.pb \n",
      "2022-10-20 08:53:16.000140: INFO ||NCC_WRAPPER||: No candidate found under compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_8742604621763275.\n",
      "2022-10-20 08:53:16.000141: INFO ||NCC_WRAPPER||: Cache dir for the neff: compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_8742604621763275/MODULE_SyncTensorsGraph.16105_8742604621763275/a030f5f1-55a5-4019-85b7-ebd3b0f08acd\n",
      "2022-10-20 08:53:16.000146: INFO ||NCC_WRAPPER||: Extracting graphs for ahead-of-time parallel compilation. Nocompilation was done.\n",
      "[INFO|modeling_utils.py:1074] 2022-10-20 08:53:18,388 >> Model weights saved in gpt2-clm/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2074] 2022-10-20 08:53:18,390 >> tokenizer config file saved in gpt2-clm/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2080] 2022-10-20 08:53:18,390 >> Special tokens file saved in gpt2-clm/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  train_loss               =       -0.0\n",
      "  train_runtime            = 0:00:48.21\n",
      "  train_samples            =       2318\n",
      "  train_samples_per_second =    144.218\n",
      "  train_steps_per_second   =     18.043\n",
      "[INFO|modelcard.py:460] 2022-10-20 08:53:18,755 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'dataset': {'name': 'wikitext wikitext-2-raw-v1', 'type': 'wikitext', 'args': 'wikitext-2-raw-v1'}}\n",
      "2022-10-20 08:53:22.000446: INFO ||PARALLEL_COMPILE||: Starting parallel compilations of the extracted graphs\n",
      "2022-10-20 08:53:22.000447: INFO ||PARALLEL_COMPILE||: Compiling /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.164_14973442252103645712.hlo.pb using following command: neuronx-cc compile --target=trn1 --framework XLA /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.164_14973442252103645712.hlo.pb --model-type=transformer --verbose=35 --output /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.164_14973442252103645712.neff\n",
      "2022-10-20 08:53:22.000448: INFO ||PARALLEL_COMPILE||: Compiling /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.17137_15887835957493241746.hlo.pb using following command: neuronx-cc compile --target=trn1 --framework XLA /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.17137_15887835957493241746.hlo.pb --model-type=transformer --verbose=35 --output /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.17137_15887835957493241746.neff\n",
      "2022-10-20 08:53:22.000450: INFO ||PARALLEL_COMPILE||: Compiling /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.16849_1762241719395940600.hlo.pb using following command: neuronx-cc compile --target=trn1 --framework XLA /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.16849_1762241719395940600.hlo.pb --model-type=transformer --verbose=35 --output /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.16849_1762241719395940600.neff\n",
      "2022-10-20 08:53:22.000452: INFO ||PARALLEL_COMPILE||: Compiling /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.4_11145057064457806766.hlo.pb using following command: neuronx-cc compile --target=trn1 --framework XLA /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.4_11145057064457806766.hlo.pb --model-type=transformer --verbose=35 --output /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.4_11145057064457806766.neff\n",
      "2022-10-20 08:53:22.000453: INFO ||PARALLEL_COMPILE||: Compiling /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.6_4327322914518251210.hlo.pb using following command: neuronx-cc compile --target=trn1 --framework XLA /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.6_4327322914518251210.hlo.pb --model-type=transformer --verbose=35 --output /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.6_4327322914518251210.neff\n",
      "2022-10-20 08:53:22.000454: INFO ||PARALLEL_COMPILE||: Compiling /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.4645_4437454503677049592.hlo.pb using following command: neuronx-cc compile --target=trn1 --framework XLA /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.4645_4437454503677049592.hlo.pb --model-type=transformer --verbose=35 --output /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.4645_4437454503677049592.neff\n",
      "2022-10-20 08:53:22.000456: INFO ||PARALLEL_COMPILE||: Compiling /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.16105_8742604621763275.hlo.pb using following command: neuronx-cc compile --target=trn1 --framework XLA /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.16105_8742604621763275.hlo.pb --model-type=transformer --verbose=35 --output /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.16105_8742604621763275.neff\n",
      ".......\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "................................\n",
      "Compiler status PASS\n",
      "......................................................................................................................................................................................................................................................\n",
      "Compiler status PASS\n",
      "..........\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "2022-10-20 09:25:21.000717: INFO ||PARALLEL_COMPILE||: Compilation summary:\n",
      "2022-10-20 09:25:21.000717: INFO ||PARALLEL_COMPILE||: Compilation of /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.4_11145057064457806766.hlo.pb was successful. Plugged neff in the cache dir: /home/ec2-user/aws_github/AWSWinder/aws-neuron-samples-staging/torch-neuronx/training/hf_language_modeling/gpt2/compiler_cache\n",
      "2022-10-20 09:25:21.000717: INFO ||PARALLEL_COMPILE||: Compilation of /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.6_4327322914518251210.hlo.pb was successful. Plugged neff in the cache dir: /home/ec2-user/aws_github/AWSWinder/aws-neuron-samples-staging/torch-neuronx/training/hf_language_modeling/gpt2/compiler_cache\n",
      "2022-10-20 09:25:21.000717: INFO ||PARALLEL_COMPILE||: Compilation of /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.164_14973442252103645712.hlo.pb was successful. Plugged neff in the cache dir: /home/ec2-user/aws_github/AWSWinder/aws-neuron-samples-staging/torch-neuronx/training/hf_language_modeling/gpt2/compiler_cache\n",
      "2022-10-20 09:25:21.000717: INFO ||PARALLEL_COMPILE||: Compilation of /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.4645_4437454503677049592.hlo.pb was successful. Plugged neff in the cache dir: /home/ec2-user/aws_github/AWSWinder/aws-neuron-samples-staging/torch-neuronx/training/hf_language_modeling/gpt2/compiler_cache\n",
      "2022-10-20 09:25:21.000717: INFO ||PARALLEL_COMPILE||: Compilation of /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.16105_8742604621763275.hlo.pb was successful. Plugged neff in the cache dir: /home/ec2-user/aws_github/AWSWinder/aws-neuron-samples-staging/torch-neuronx/training/hf_language_modeling/gpt2/compiler_cache\n",
      "2022-10-20 09:25:21.000717: INFO ||PARALLEL_COMPILE||: Compilation of /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.17137_15887835957493241746.hlo.pb was successful. Plugged neff in the cache dir: /home/ec2-user/aws_github/AWSWinder/aws-neuron-samples-staging/torch-neuronx/training/hf_language_modeling/gpt2/compiler_cache\n",
      "2022-10-20 09:25:21.000717: INFO ||PARALLEL_COMPILE||: Compilation of /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.16849_1762241719395940600.hlo.pb was successful. Plugged neff in the cache dir: /home/ec2-user/aws_github/AWSWinder/aws-neuron-samples-staging/torch-neuronx/training/hf_language_modeling/gpt2/compiler_cache\n",
      "2022-10-20 09:25:21.000717: INFO ||PARALLEL_COMPILE||: Total graphs: 7\n",
      "2022-10-20 09:25:21.000717: INFO ||PARALLEL_COMPILE||: Total successful compilations: 7\n",
      "2022-10-20 09:25:21.000717: INFO ||PARALLEL_COMPILE||: Total failed compilations: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Compile model\")\n",
    "COMPILE_CMD = f\"\"\"{env_var_options} neuron_parallel_compile torchrun --nproc_per_node={num_workers} ./run_clm.py \\\n",
    "    --model_name_or_path {model_name} \\\n",
    "    --dataset_name {dataset_name} \\\n",
    "    --dataset_config_name {dataset_config_name} \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --do_train \\\n",
    "    --overwrite_output_dir \\\n",
    "    --output_dir {model_base_name}-{task_name} |& tee log_compile_{model_base_name}-{task_name}-precompile\"\"\"\n",
    "\n",
    "print(f'Running command: \\n{COMPILE_CMD}')\n",
    "! {COMPILE_CMD}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model\n",
      "Running command: \n",
      "XLA_USE_BF16=1 NEURON_CC_FLAGS=\"--cache_dir=./compiler_cache --model-type=transformer\" torchrun --nproc_per_node=2 run_clm.py     --model_name_or_path gpt2     --dataset_name wikitext     --dataset_config_name wikitext-2-raw-v1     --per_device_train_batch_size 4     --per_device_eval_batch_size 4     --do_train     --do_eval     --overwrite_output_dir     --output_dir gpt2-clm |& tee log_run_gpt2-clm\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "__main__: Process rank: 1, device: xla:0, n_gpu: 0distributed training: True, 16-bits training: False\n",
      "__main__: Process rank: 0, device: xla:1, n_gpu: 0distributed training: True, 16-bits training: False\n",
      "__main__: Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=gpt2-clm/runs/Oct20_09-25-41_ip-172-31-33-220.us-west-2.compute.internal,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=gpt2-clm,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=gpt2-clm,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "datasets.info: Loading Dataset Infos from /home/ec2-user/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
      "datasets.builder: Overwrite dataset info from restored data version.\n",
      "datasets.info: Loading Dataset info from /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
      "datasets.builder: Reusing dataset wikitext (/home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1228.20it/s]\n",
      "datasets.builder: Reusing dataset wikitext (/home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "datasets.info: Loading Dataset info from /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
      "100%|██████████| 3/3 [00:00<00:00, 1316.62it/s]\n",
      "[INFO|configuration_utils.py:644] 2022-10-20 09:25:42,022 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "[INFO|configuration_utils.py:680] 2022-10-20 09:25:42,023 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:336] 2022-10-20 09:25:42,286 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:644] 2022-10-20 09:25:42,555 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "[INFO|configuration_utils.py:680] 2022-10-20 09:25:42,556 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-20 09:25:44,407 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /home/ec2-user/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-20 09:25:44,407 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /home/ec2-user/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-20 09:25:44,407 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /home/ec2-user/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-20 09:25:44,407 >> loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-20 09:25:44,407 >> loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-10-20 09:25:44,407 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:644] 2022-10-20 09:25:44,668 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "[INFO|configuration_utils.py:680] 2022-10-20 09:25:44,669 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1427] 2022-10-20 09:25:44,997 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /home/ec2-user/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
      "[INFO|modeling_utils.py:1694] 2022-10-20 09:25:46,229 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:1703] 2022-10-20 09:25:46,229 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-ea01c5c1a9516ec4.arrow\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-20186e78bbd5f962.arrow\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-2323331079437bec.arrow\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-db28f3f9a817a2e3.arrow\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-d999d6c7e07e0a26.arrow\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-1162bb89ee1e8f9c.arrow\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-ea01c5c1a9516ec4.arrow\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-20186e78bbd5f962.arrow\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-2323331079437bec.arrow\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-db28f3f9a817a2e3.arrow\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-d999d6c7e07e0a26.arrow\n",
      "datasets.arrow_dataset: Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-1162bb89ee1e8f9c.arrow\n",
      "/home/ec2-user/aws_neuron_venv_pytorch_p37/lib64/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1244] 2022-10-20 09:25:47,107 >> ***** Running training *****\n",
      "[INFO|trainer.py:1245] 2022-10-20 09:25:47,107 >>   Num examples = 2318\n",
      "[INFO|trainer.py:1246] 2022-10-20 09:25:47,107 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1247] 2022-10-20 09:25:47,107 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:1248] 2022-10-20 09:25:47,107 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1249] 2022-10-20 09:25:47,107 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1250] 2022-10-20 09:25:47,107 >>   Total optimization steps = 870\n",
      "/home/ec2-user/aws_neuron_venv_pytorch_p37/lib64/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "  0%|          | 0/870 [00:00<?, ?it/s]2022-10-20 09:25:47.000937: INFO ||NCC_WRAPPER||: Using a cached neff at compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_14973442252103645712/MODULE_SyncTensorsGraph.164_14973442252103645712/9d23cc6e-504c-4aae-aeba-5d8dd6e0645f/MODULE_SyncTensorsGraph.164_14973442252103645712.neff. Exiting with a successfully compiled graph\n",
      "  0%|          | 1/870 [00:00<02:00,  7.23it/s]2022-10-20 09:25:48.000778: INFO ||NCC_WRAPPER||: Using a cached neff at compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_15887835957493241746/MODULE_SyncTensorsGraph.17137_15887835957493241746/2a77923b-e492-4ea1-b6e2-2f64bf155013/MODULE_SyncTensorsGraph.17137_15887835957493241746.neff. Exiting with a successfully compiled graph\n",
      "  0%|          | 2/870 [00:01<13:53,  1.04it/s]\n",
      "ip-172-31-33-220:63036:63370 [1] init.cc:101 NCCL WARN OFI plugin initNet() failed is EFA enabled?\n",
      "2022-10-20 09:26:44.000775: INFO ||NCC_WRAPPER||: Using a cached neff at compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_1762241719395940600/MODULE_SyncTensorsGraph.16849_1762241719395940600/85955d12-0a40-41f5-a6ff-b8365201c109/MODULE_SyncTensorsGraph.16849_1762241719395940600.neff. Exiting with a successfully compiled graph\n",
      " 57%|█████▋    | 500/870 [05:26<02:40,  2.31it/s]2022-10-20 09:31:15.000751: INFO ||NCC_WRAPPER||: Using a cached neff at compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_11145057064457806766/MODULE_SyncTensorsGraph.4_11145057064457806766/3d31fd86-3f41-4f1b-9719-3bfd66ae125e/MODULE_SyncTensorsGraph.4_11145057064457806766.neff. Exiting with a successfully compiled graph\n",
      "{'loss': 3.296, 'learning_rate': 2.1264367816091954e-05, 'epoch': 1.72}\n",
      " 57%|█████▋    | 500/870 [05:27<02:40,  2.31it/s][INFO|trainer.py:2060] 2022-10-20 09:31:15,784 >> Saving model checkpoint to gpt2-clm/checkpoint-500\n",
      "[INFO|configuration_utils.py:430] 2022-10-20 09:31:15,786 >> Configuration saved in gpt2-clm/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1074] 2022-10-20 09:31:18,757 >> Model weights saved in gpt2-clm/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2074] 2022-10-20 09:31:18,758 >> tokenizer config file saved in gpt2-clm/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2080] 2022-10-20 09:31:18,758 >> Special tokens file saved in gpt2-clm/checkpoint-500/special_tokens_map.json\n",
      "2022-10-20 09:31:23.000889: INFO ||NCC_WRAPPER||: Using a cached neff at compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_4327322914518251210/MODULE_SyncTensorsGraph.6_4327322914518251210/9c2231af-9c66-46fb-b977-2c61de22701e/MODULE_SyncTensorsGraph.6_4327322914518251210.neff. Exiting with a successfully compiled graph\n",
      "100%|██████████| 870/870 [08:14<00:00,  2.33it/s][INFO|trainer.py:1473] 2022-10-20 09:34:02,582 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "2022-10-20 09:34:03.000451: INFO ||NCC_WRAPPER||: Using a cached neff at compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_4437454503677049592/MODULE_SyncTensorsGraph.4645_4437454503677049592/c38cac5b-5686-4c1f-af2b-7c04f06e0cf5/MODULE_SyncTensorsGraph.4645_4437454503677049592.neff. Exiting with a successfully compiled graph\n",
      "{'train_runtime': 499.9389, 'train_samples_per_second': 13.91, 'train_steps_per_second': 1.74, 'train_loss': 3.2, 'epoch': 3.0}\n",
      "100%|██████████| 870/870 [08:19<00:00,  1.74it/s]\n",
      "[INFO|trainer.py:2060] 2022-10-20 09:34:07,049 >> Saving model checkpoint to gpt2-clm\n",
      "[INFO|configuration_utils.py:430] 2022-10-20 09:34:07,052 >> Configuration saved in gpt2-clm/config.json\n",
      "2022-10-20 09:34:07.000992: INFO ||NCC_WRAPPER||: Using a cached neff at compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_8742604621763275/MODULE_SyncTensorsGraph.16105_8742604621763275/cf055aa9-af90-4d03-b3eb-833d57328d5f/MODULE_SyncTensorsGraph.16105_8742604621763275.neff. Exiting with a successfully compiled graph\n",
      "[INFO|modeling_utils.py:1074] 2022-10-20 09:35:02,691 >> Model weights saved in gpt2-clm/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2074] 2022-10-20 09:35:02,763 >> tokenizer config file saved in gpt2-clm/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2080] 2022-10-20 09:35:02,764 >> Special tokens file saved in gpt2-clm/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  train_loss               =        3.2\n",
      "  train_runtime            = 0:08:19.93\n",
      "  train_samples            =       2318\n",
      "  train_samples_per_second =      13.91\n",
      "  train_steps_per_second   =       1.74\n",
      "__main__: *** Evaluate ***\n",
      "[INFO|trainer.py:2340] 2022-10-20 09:35:02,856 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2342] 2022-10-20 09:35:02,857 >>   Num examples = 240\n",
      "[INFO|trainer.py:2345] 2022-10-20 09:35:02,857 >>   Batch size = 4\n",
      "2022-10-20 09:35:03.000855: INFO ||NCC_WRAPPER||: No candidate found under compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_6476357059408075762.\n",
      "2022-10-20 09:35:03.000858: INFO ||NCC_WRAPPER||: Cache dir for the neff: compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_6476357059408075762/MODULE_7_SyncTensorsGraph.15511_6476357059408075762_ip-172-31-33-220.us-west-2.compute.internal-da0c936e-63036-5eb740a32c0aa/23aac144-4680-4520-92a4-947613c12139\n",
      "..................................................................................\n",
      "Compiler status PASS\n",
      "2022-10-20 10:02:16.000688: INFO ||NCC_WRAPPER||: Exiting with a successfully compiled graph\n",
      "2022-10-20 10:02:16.000689: INFO ||NCC_WRAPPER||: No candidate found under compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_7791424960436981652.\n",
      "2022-10-20 10:02:16.000691: INFO ||NCC_WRAPPER||: Cache dir for the neff: compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_7791424960436981652/MODULE_8_SyncTensorsGraph.15515_7791424960436981652_ip-172-31-33-220.us-west-2.compute.internal-688561ea-63036-5eb740a36f268/aa8454ad-5e40-4925-a368-01e5ef654f7c\n",
      "...............................................................\n",
      "ip-172-31-33-220:63036:63370 [1] include/socket.h:505 NCCL WARN Timeout waiting for RX (waited 1200 sec) - retrying\n",
      ".........................\n",
      "Compiler status PASS\n",
      "2022-10-20 10:31:25.000533: INFO ||NCC_WRAPPER||: Exiting with a successfully compiled graph\n",
      "2022-10-20 10:31:25.000535: INFO ||NCC_WRAPPER||: No candidate found under compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_2289729240270316021.\n",
      "2022-10-20 10:31:25.000536: INFO ||NCC_WRAPPER||: Cache dir for the neff: compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_2289729240270316021/MODULE_9_SyncTensorsGraph.3248_2289729240270316021_ip-172-31-33-220.us-west-2.compute.internal-b9b86d25-63036-5eb746b9dbf38/273983bf-b57f-4395-bbff-c76adaacc40b\n",
      "...........\n",
      "Compiler status PASS\n",
      "2022-10-20 10:34:48.000284: INFO ||NCC_WRAPPER||: Exiting with a successfully compiled graph\n",
      "2022-10-20 10:34:53.000180: INFO ||NCC_WRAPPER||: No candidate found under compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_13536073418748836400.\n",
      "2022-10-20 10:34:53.000185: INFO ||NCC_WRAPPER||: Cache dir for the neff: compiler_cache/neuron-compile-cache/USER_neuroncc-2.2.0.46+c9e7f6a72/MODULE_13536073418748836400/MODULE_10_SyncTensorsGraph.4_13536073418748836400_ip-172-31-33-220.us-west-2.compute.internal-c475ec99-63036-5eb74e0221411/e2380e29-e97b-4f02-a7ea-b789e0940648\n",
      ".\n",
      "Compiler status PASS\n",
      "2022-10-20 10:34:56.000438: INFO ||NCC_WRAPPER||: Exiting with a successfully compiled graph\n",
      "100%|██████████| 30/30 [00:05<00:00,  5.34it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_loss               =     3.1023\n",
      "  eval_runtime            = 0:59:59.22\n",
      "  eval_samples            =        240\n",
      "  eval_samples_per_second =      0.067\n",
      "  eval_steps_per_second   =      0.008\n",
      "  perplexity              =      22.25\n",
      "[INFO|modelcard.py:460] 2022-10-20 10:35:02,366 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'dataset': {'name': 'wikitext wikitext-2-raw-v1', 'type': 'wikitext', 'args': 'wikitext-2-raw-v1'}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Train model\")\n",
    "RUN_CMD = f\"\"\"{env_var_options} torchrun --nproc_per_node={num_workers} run_clm.py \\\n",
    "    --model_name_or_path {model_name} \\\n",
    "    --dataset_name {dataset_name} \\\n",
    "    --dataset_config_name {dataset_config_name} \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --overwrite_output_dir \\\n",
    "    --output_dir {model_base_name}-{task_name} |& tee log_run_{model_base_name}-{task_name}\"\"\"\n",
    "\n",
    "print(f'Running command: \\n{RUN_CMD}')\n",
    "! {RUN_CMD}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Neuron PyTorch)",
   "language": "python",
   "name": "pytorch_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
