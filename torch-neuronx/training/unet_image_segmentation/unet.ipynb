{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET training - Pytorch 2.1\n",
    "This notebook shows how to fine-tune a pretrained UNET PyTorch model with AWS Trainium (trn1 instances) using NeuronSDK.\\\n",
    "The model implementation is provided by milesial/Pytorch-UNet. \n",
    "\n",
    "\n",
    "\n",
    "The example has 2 stages:\n",
    "1. First compile the model using the utility `neuron_parallel_compile` to compile the model to run on the AWS Trainium device.\n",
    "1. Run the fine-tuning script to train the model based on image segmentaion task. The training job will use 32 workers with data parallel to speed up the training.\n",
    "\n",
    "It has been tested and run on trn1.32xlarge instance using 256 x 256 input image for binary segmentation with batch size 4.\n",
    "\n",
    "**Reference:** \n",
    "milesial, U-Net: Semantic segmentation with PyTorch, GitHub repository\n",
    "https://github.com/milesial/Pytorch-UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Neuron Compiler and Neuron/XLA packages\n",
    "%pip install -U \"timm\" \"tensorboard\" torchvision==0.16.*\n",
    "%pip install -U \"Pillow\" \"glob2\" \"scikit-learn\" \n",
    "# use --force-reinstall if you're facing some issues while loading the modules\n",
    "# now restart the kernel again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Download Carvana dataset\n",
    "This example uses Carvana dataset which requires users to manually download the dataset before training.\\\n",
    " https://www.kaggle.com/competitions/carvana-image-masking-challenge/data \n",
    "\n",
    "1. Download train.zip and train_masks.zip \n",
    "2. Unzip\n",
    "3. Create a carvana directory\n",
    "4. Directory structure\\\n",
    "carvana/train/\\\n",
    "carvana/train_masks/\n",
    "\n",
    "dataset_path = \\<Path to Carvana directory\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "num_workers = 32\n",
    "dataloader_num_workers = 2\n",
    "image_dim = 256\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 2e-4\n",
    "batch_size = 4\n",
    "env_var_options = \"NEURON_RT_ASYNC_EXEC_MAX_INFLIGHT_REQUESTS=3  \" + \\\n",
    "    \"NEURON_CC_FLAGS=\\'--cache_dir=./compiler_cache --model-type=cnn-training\\'\"\n",
    "dataset_path = \"./carvana/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Compile the model with neuron_parallel_compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import subprocess\n",
    "print(\"Compile model\")\n",
    "COMPILE_CMD = f\"\"\"{env_var_options} neuron_parallel_compile torchrun --nproc_per_node={num_workers} \\\n",
    "   train.py \\\n",
    "    --num_workers {dataloader_num_workers} \\\n",
    "    --image_dim {image_dim} \\\n",
    "    --num_epochs 2 \\\n",
    "    --batch_size {batch_size} \\\n",
    "    --drop_last \\\n",
    "    --data_dir {dataset_path} \\\n",
    "    --lr {learning_rate}\"\"\"\n",
    "\n",
    "print(f'Running command: \\n{COMPILE_CMD}')\n",
    "if subprocess.check_call(COMPILE_CMD,shell=True):\n",
    "   print(\"There was an error with the compilation command\")\n",
    "else:\n",
    "   print(\"Compilation Success!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Compile and Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import subprocess\n",
    "print(\"Compile model\")\n",
    "COMPILE_CMD = f\"\"\"{env_var_options} torchrun --nproc_per_node={num_workers} \\\n",
    "    train.py \\\n",
    "    --num_workers {dataloader_num_workers} \\\n",
    "    --image_dim {image_dim} \\\n",
    "    --num_epochs {num_epochs} \\\n",
    "    --batch_size {batch_size} \\\n",
    "    --do_eval \\\n",
    "    --drop_last \\\n",
    "    --data_dir {dataset_path} \\\n",
    "    --lr {learning_rate}\"\"\"\n",
    "\n",
    "print(f'Running command: \\n{COMPILE_CMD}')\n",
    "if subprocess.check_call(COMPILE_CMD,shell=True):\n",
    "   print(\"There was an error with the fine-tune command\")\n",
    "else:\n",
    "   print(\"Fine-tune Successful!!!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
