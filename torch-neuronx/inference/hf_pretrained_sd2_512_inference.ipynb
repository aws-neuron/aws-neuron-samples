{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_neuronx\n",
    "import os\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "from diffusers.models.unet_2d_condition import UNet2DConditionOutput\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image as mpimg\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from diffusers.models.cross_attention import CrossAttention\n",
    "import numpy as np\n",
    "\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If this is the first time you trace a model, it will need to be compiled and optimized for Inf2 through neuron. This step is only required once. After the model is compiled, it is loaded onto the NeuronCores and a single prompt is given** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UNetWrap(nn.Module):\n",
    "    def __init__(self, unet):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "\n",
    "    def forward(self, sample, timestep, encoder_hidden_states, cross_attention_kwargs=None):\n",
    "        out_tuple = self.unet(sample, timestep, encoder_hidden_states, return_dict=False)\n",
    "        return out_tuple\n",
    "\n",
    "class NeuronUNet(nn.Module):\n",
    "    def __init__(self, unetwrap):\n",
    "        super().__init__()\n",
    "        self.unetwrap = unetwrap\n",
    "        self.config = unetwrap.unet.config\n",
    "        self.in_channels = unetwrap.unet.in_channels\n",
    "        self.device = unetwrap.unet.device\n",
    "\n",
    "    def forward(self, sample, timestep, encoder_hidden_states, cross_attention_kwargs=None):\n",
    "        sample = self.unetwrap(sample, timestep.float().expand((sample.shape[0],)), encoder_hidden_states)[0]\n",
    "        return UNet2DConditionOutput(sample=sample)\n",
    "\n",
    "def get_attention_scores(self, query, key, attn_mask):       \n",
    "    dtype = query.dtype\n",
    "\n",
    "    if self.upcast_attention:\n",
    "        query = query.float()\n",
    "        key = key.float()\n",
    "\n",
    "    if(query.size() == key.size()):\n",
    "        attention_scores = cust_badbmm(\n",
    "            key,\n",
    "            query.transpose(-1, -2)\n",
    "        )\n",
    "\n",
    "        if self.upcast_softmax:\n",
    "            attention_scores = attention_scores.float()\n",
    "\n",
    "        attention_probs = torch.nn.functional.softmax(attention_scores, dim=1).permute(0,2,1)\n",
    "        attention_probs = attention_probs.to(dtype)\n",
    "\n",
    "    else:\n",
    "        attention_scores = cust_badbmm(\n",
    "            query,\n",
    "            key.transpose(-1, -2)\n",
    "        )\n",
    "\n",
    "        if self.upcast_softmax:\n",
    "            attention_scores = attention_scores.float()\n",
    "\n",
    "        attention_probs = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = attention_probs.to(dtype)\n",
    "        \n",
    "    return attention_probs\n",
    "\n",
    "def cust_badbmm(a, b):\n",
    "    bmm = torch.bmm(a, b)\n",
    "    scaled = bmm * 0.125\n",
    "    return scaled\n",
    "\n",
    "\n",
    "# For saving compiler artifacts\n",
    "COMPILER_WORKDIR_ROOT = 'sd2_compile_dir'\n",
    "\n",
    "model_id = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float32)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "# Replace original cross-attention module with custom cross-attention module for better performance\n",
    "CrossAttention.get_attention_scores = get_attention_scores\n",
    "\n",
    "pipe.unet = NeuronUNet(UNetWrap(pipe.unet))\n",
    "\n",
    "# Compile unet - FP32\n",
    "sample_1b = torch.randn([1, 4, 64, 64])\n",
    "timestep_1b = torch.tensor(999).float().expand((1,))\n",
    "encoder_hidden_states_1b = torch.randn([1, 77, 1024])\n",
    "example_inputs = sample_1b, timestep_1b, encoder_hidden_states_1b\n",
    "\n",
    "pipe.unet.unetwrap = torch_neuronx.trace(\n",
    "    pipe.unet.unetwrap,\n",
    "    example_inputs,\n",
    "    compiler_workdir=os.path.join(COMPILER_WORKDIR_ROOT, 'unet'),\n",
    "    compiler_args=[\"--model-type=unet-inference\"]\n",
    ")\n",
    "\n",
    "# save compiled unet\n",
    "unet_filename = os.path.join(COMPILER_WORKDIR_ROOT, 'unet/model.pt')\n",
    "torch.jit.save(pipe.unet.unetwrap, unet_filename)\n",
    "\n",
    "# load previously compiled unet\n",
    "# unet_filename = os.path.join(COMPILER_WORKDIR_ROOT, 'unet/model.pt')\n",
    "# pipe.unet.unetwrap = torch.jit.load(unet_filename)\n",
    "\n",
    "# Load the compiled UNet onto two neuron cores.\n",
    "device_ids = [0,1]\n",
    "pipe.unet.unetwrap = torch_neuronx.DataParallel(pipe.unet.unetwrap, device_ids, set_dynamic_batching=False)\n",
    "\n",
    "# # Compile vae post_quant_conv\n",
    "post_quant_conv_in = torch.randn([1, 4, 64, 64])\n",
    "pipe.vae.post_quant_conv = torch_neuronx.trace(\n",
    "    pipe.vae.post_quant_conv, \n",
    "    post_quant_conv_in,\n",
    "    compiler_workdir=os.path.join(COMPILER_WORKDIR_ROOT, 'vae_post_quant_conv'),\n",
    ")\n",
    "\n",
    "# # Save the compiled vae post_quant_conv\n",
    "post_quant_conv_filename = os.path.join(COMPILER_WORKDIR_ROOT, 'vae_post_quant_conv/model.pt')\n",
    "torch.jit.save(pipe.vae.post_quant_conv, post_quant_conv_filename)\n",
    "\n",
    "# Load the previously-compiled vae post_quant_conv\n",
    "# post_quant_conv_filename = os.path.join(COMPILER_WORKDIR_ROOT, 'vae_post_quant_conv/model_cast_fp32.pt')\n",
    "# pipe.vae.post_quant_conv = torch.jit.load(post_quant_conv_filename)\n",
    "\n",
    "# Compile vae decoder\n",
    "decoder_in = torch.randn([1, 4, 64, 64])\n",
    "pipe.vae.decoder = torch_neuronx.trace(\n",
    "    pipe.vae.decoder, \n",
    "    decoder_in, \n",
    "    compiler_workdir=os.path.join(COMPILER_WORKDIR_ROOT, 'vae_decoder'),\n",
    ")\n",
    "\n",
    "# Save the compiled vae decoder\n",
    "decoder_filename = os.path.join(COMPILER_WORKDIR_ROOT, 'vae_decoder/model.pt')\n",
    "torch.jit.save(pipe.vae.decoder, decoder_filename)\n",
    "\n",
    "# # Load the previously-compiled vae decoder\n",
    "# decoder_filename = os.path.join(COMPILER_WORKDIR_ROOT, 'vae_decoder/model.pt')\n",
    "# pipe.vae.decoder = torch.jit.load(decoder_filename)\n",
    "\n",
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "image = pipe(prompt).images[0]\n",
    "image.save(\"image.png\")\n",
    "\n",
    "plt.title(\"Image\")\n",
    "plt.xlabel(\"X pixel scaling\")\n",
    "plt.ylabel(\"Y pixels scaling\")\n",
    "\n",
    "image = mpimg.imread(\"image.png\")\n",
    "plt.imshow(image)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that the model is compiled, you can reload it with any number of prompts. Edit the Prompts below to see what you can create.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# load previously compiled unet\n",
    "unet_filename = os.path.join(COMPILER_WORKDIR_ROOT, 'unet/model.pt')\n",
    "pipe.unet.unetwrap = torch.jit.load(unet_filename)\n",
    "\n",
    "# Load the compiled UNet onto two neuron cores.\n",
    "device_ids = [0,1]\n",
    "pipe.unet.unetwrap = torch_neuronx.DataParallel(pipe.unet.unetwrap, device_ids, set_dynamic_batching=False)\n",
    "\n",
    "# Load the previously-compiled vae post_quant_conv\n",
    "post_quant_conv_filename = os.path.join(COMPILER_WORKDIR_ROOT, 'vae_post_quant_conv/model.pt')\n",
    "pipe.vae.post_quant_conv = torch.jit.load(post_quant_conv_filename)\n",
    "\n",
    " # Load the previously-compiled vae decoder\n",
    "decoder_filename = os.path.join(COMPILER_WORKDIR_ROOT, 'vae_decoder/model.pt')\n",
    "pipe.vae.decoder = torch.jit.load(decoder_filename)\n",
    "\n",
    "prompt = [\"a photo of an astronaut riding a horse on mars\",\n",
    "          \"sonic on the moon\",\n",
    "          \"elvis playing guitar while eating a hotdog\",\n",
    "          \"saved by the bell\",\n",
    "          \"engineers eating lunch at the opera\",\n",
    "          \"panda eating bamboo on a plane\",\n",
    "          \"A digital illustration of a steampunk flying machine in the sky with cogs and mechanisms, 4k, detailed, trending in artstation, fantasy vivid colors\",\n",
    "          \"kids playing soccer at the FIFA World Cup\"\n",
    "         ]\n",
    "\n",
    "plt.title(\"Image\")\n",
    "plt.xlabel(\"X pixel scaling\")\n",
    "plt.ylabel(\"Y pixels scaling\")\n",
    "\n",
    "total_time = 0\n",
    "for x in prompt:\n",
    "    start_time = time.time()\n",
    "    image = pipe(x).images[0]\n",
    "    total_time = total_time + (time.time()-start_time)\n",
    "    image.save(\"image.png\")\n",
    "    image = mpimg.imread(\"image.png\")\n",
    "    #clear_output(wait=True)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "print(\"Average time: \", np.round((total_time/len(prompt)), 2), \"seconds\")\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now have Fun**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_input = \"\"\n",
    "print(\"Enter Prompt, type exit to quit\")\n",
    "while user_input != \"exit\": \n",
    "    total_time = 0\n",
    "    user_input = input(\"What prompt would you like to give?  \")\n",
    "    if user_input == \"exit\":\n",
    "        break\n",
    "    start_time = time.time()\n",
    "    image = pipe(user_input).images[0]\n",
    "    total_time = total_time + (time.time()-start_time)\n",
    "    image.save(\"image.png\")\n",
    "\n",
    "    plt.title(\"Image\")\n",
    "    plt.xlabel(\"X pixel scaling\")\n",
    "    plt.ylabel(\"Y pixels scaling\")\n",
    "\n",
    "    image = mpimg.imread(\"image.png\")\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    print(\"time: \", np.round(total_time, 2), \"seconds\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-neuronx)",
   "language": "python",
   "name": "aws_neuron_venv_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
