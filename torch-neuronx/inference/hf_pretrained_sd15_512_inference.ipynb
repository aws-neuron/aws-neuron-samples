{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "**Requirements:**\n",
    "* This notebook need to be run on a instance that features second-generation Neuron hardware (NeuronCore-v2), we recommend `inf2.8xlarge` instances or larger.\n",
    "\n",
    "In this notebook, you will compile and run the [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5) text-to-image pipeline (Stable Diffusion 1.5) on AWS Neuron hardware to \n",
    "generate images of size 512x512 pixels.\n",
    "To run the pipeline, you will use the dedicated [`StableDiffutionPipeline`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline) \n",
    "abstraction from HuggingFace's [`diffusers`](https://huggingface.co/docs/diffusers/index) library.\n",
    "\n",
    "Diffusion models are peculiar in the sense that they do not consist of a single model but of a collection of models and stateless components orcherstrated into a pipeline. Running a diffusion pipeline on Neuron therefore \n",
    "involves compiling multiple models while ensuring that the compiled models can still be handled by the pipeline. \n",
    "\n",
    "The [`model_index.json` file](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model_index.json) \n",
    "from the HuggingFace model repository lists the different components of the `runwayml/stable-diffusion-v1-5` pipeline:\n",
    "* The CLIP text encoder and its tokenizer,\n",
    "* The U-Net,\n",
    "* The variational auto-encoder (VAE),\n",
    "* The safety checker,\n",
    "* The scheduler (default scheduler is the [Pseudo Numerical Methods for Diffusion Models (PNDM) scheduler](https://huggingface.co/docs/diffusers/api/schedulers/pndm))\n",
    "* The feature extractor\n",
    "\n",
    "Now, not all these components will be run using Neuron devices. The tokenizer and the feature extractor are typically run on CPU. The scheduler and the feature extractor are stateless \n",
    "components and will be run on CPU as well. The other components will be compiled and run on Neuron hardware.\n",
    "\n",
    "Under the hood, the VAE consists of [4 different model components](https://github.com/huggingface/diffusers/blob/v0.20.0/src/diffusers/models/autoencoder_kl.py#L77C8-L77C8): the encoder, the \n",
    "conv layer, the post conv layer and the decoder. Looking at the Pipeline's [`__call__`](https://github.com/huggingface/diffusers/blob/v0.20.0/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L524) \n",
    "method, you notice that only the VAE's `decode` method is actually called when performing inference. By studying the VAE's [`decode`](https://github.com/huggingface/diffusers/blob/v0.20.0/src/diffusers/models/autoencoder_kl.py#L265) \n",
    "method, you can see that only the its post conv and decoder components are used. You therefore need to compile the VAE's decoder and post conv layer only.\n",
    "\n",
    "Similarly, the safety checker consists of [2 different model components](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py#L40): the vison model \n",
    "and the visual projection model.\n",
    "\n",
    "To sum it up, here is the list of the model components you will compile, attach to the Stable Diffusion pipeline and run on Neuron devices:\n",
    "* The CLIP text encoder\n",
    "* The U-Net\n",
    "* The VAE decoder and post quant conv layer\n",
    "* The safety checker vision model and visual projection model\n",
    "\n",
    "## Model compilation\n",
    "First, you will load the pipeline using the `StableDiffusionPipeline.from_pretrained` API. Each model of interest will then be compiled using PyTorch Neuron (`torch-neuronx`) \n",
    "[Tracing API](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/api-reference-guide/inference/api-torch-neuronx-trace.html). Concretely, a given model is passed \n",
    "to `torch_neuronx.trace` together with example input tensors. The `trace` function then traces the model, i.e. extracts the graph of tensor operations, converts it into a intermediate representation \n",
    "in the HLO format which is then fed to the Neuron Compiler (`neuronx-cc`). The Neuron Compiler then compiles the model. The `trace` function then creates a Neuron-specific TorchScript program that \n",
    "can be either directly used to run the model on Neuron hardware or serialized for a later use. This TorchScript program is embedded in a [`torch.jit.ScriptModule`](https://pytorch.org/docs/master/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule) \n",
    "object. The `trace` function finally returns the `ScriptModule` object.\n",
    "\n",
    "The compiled model includes both the compiled execution code and the model weights. The compiler has a a type casting \n",
    "capability which by default downcasts all the weights involved in matrix multiplications into the BF16 type. You will keep this capability enabled and use the FP32 checkpoints to compile the model.\n",
    "\n",
    "**Notice:** The Neuron Compiler does not support complex output types some HuggingFace models may use. For these models, you will modify their `forward` method so that it only returns \"simple\" types such \n",
    "as individual or tuple of tensors. Cf. Section 2. Compile the models.\n",
    "\n",
    "## Runtime compatibility\n",
    "For each model to be run on Neuron hardware, its corresponding PyTorch module object in the `StableDiffusionPipeline` is replaced by the `ScriptModule` object created at compile time. A `torch.jit.ScriptModule` \n",
    "object implements the same interface as the familiar `torch.nn.Module`. In both cases, a model inference is performed by calling the object's `__call__` magic method (which later calls their `forward` \n",
    "method), i.e. by \"calling the object\". Using a `ScriptModule` therefore feels like calling a function with the following runtime peculiarities:\n",
    "* The `ScriptModule` object only accepts positonal arguments. The number of arguments and the shape of input tensors must match with the argument count and tensor shapes used at compile time.\n",
    "* The `ScriptModule` object you will produce returns tensors or tuple of tensors. These return types may not match the interface expected by the `StableDiffusionPipeline`.\n",
    "\n",
    "To ensure runtime compatibility, e.g. by making all arguments positional ones, by ensuring tensor shapes are adequate, by discarding irrelevant arguments passed by the `StableDiffusionPipeline`, by wrapping returned tensors in \n",
    "expected return types, etc., you may require to modify a `ScriptModule`'s `forward` method accordingly. Cf. Section 3. Load the models.\n",
    "\n",
    "## Dependencies\n",
    "This tutorial requires the following pip packages to be installed:\n",
    "- `torch-neuronx`\n",
    "- `neuronx-cc`\n",
    "- `diffusers==0.29.2`\n",
    "- `transformers==4.42.3`\n",
    "- `accelerate==0.31.0`\n",
    "- `matplotlib`\n",
    "\n",
    "At this point, your PyTorch Neuron environment should be already set up, the latest versions of `torch-neuronx` and `neuronx-cc` should therefore already be installed in your environement (see the [PyTorch Neuron setup guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/torch-neuronx.html#setup-torch-neuronx) from the docs). The remaining dependencies can be installed by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install diffusers==0.29.2 transformers==4.42.3 accelerate==0.31.0 matplotlib Pillow --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from typing import Any, Callable, Dict, Optional, Tuple\n",
    "\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers.models.attention_processor import Attention\n",
    "from diffusers.models.unets.unet_2d_condition import UNet2DConditionModel, UNet2DConditionOutput\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.jit\n",
    "import torch.nn\n",
    "import torch_neuronx\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPooling\n",
    "from transformers.models.clip.modeling_clip import CLIPTextModel, CLIPVisionModel\n",
    "\n",
    "try:\n",
    "    from neuronxcc.nki._private_kernels.attention import attention_isa_kernel  # noqa: E402\n",
    "except ImportError:\n",
    "    from neuronxcc.nki.kernels.attention import attention_isa_kernel  # noqa: E402\n",
    "from torch_neuronx.xla_impl.ops import nki_jit  # noqa: E402\n",
    "import diffusers\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "\n",
    "_flash_fwd_call = nki_jit()(attention_isa_kernel)\n",
    "def attention_wrapper_without_swap(query, key, value):\n",
    "    bs, n_head, q_len, d_head = query.shape  # my change\n",
    "    k_len = key.shape[2]\n",
    "    v_len = value.shape[2]\n",
    "    q = query.clone().permute(0, 1, 3, 2).reshape((bs * n_head, d_head, q_len))\n",
    "    k = key.clone().permute(0, 1, 3, 2).reshape((bs * n_head, d_head, k_len))\n",
    "    v = value.clone().reshape((bs * n_head, v_len, d_head))\n",
    "    attn_output = torch.zeros((bs * n_head, q_len, d_head), dtype=torch.bfloat16, device=q.device)\n",
    "\n",
    "    scale = 1 / math.sqrt(d_head)\n",
    "    _flash_fwd_call(q, k, v, scale, attn_output, kernel_name=\"AttentionMMSoftmaxMMWithoutSwap\")\n",
    "\n",
    "    attn_output = attn_output.reshape((bs, n_head, q_len, d_head))\n",
    "\n",
    "    return attn_output\n",
    "class KernelizedAttnProcessor2_0:\n",
    "    r\"\"\"\n",
    "    Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        if not hasattr(F, \"scaled_dot_product_attention\"):\n",
    "            raise ImportError(\"AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.\")\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn: Attention,\n",
    "        hidden_states: torch.Tensor,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        temb: Optional[torch.Tensor] = None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> torch.Tensor:\n",
    "        if len(args) > 0 or kwargs.get(\"scale\", None) is not None:\n",
    "            deprecation_message = \"The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.\"\n",
    "            diffusers.utils.deprecate(\"scale\", \"1.0.0\", deprecation_message)\n",
    "\n",
    "        residual = hidden_states\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            batch_size, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
    "\n",
    "        batch_size, sequence_length, _ = (\n",
    "            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n",
    "        )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
    "            # scaled_dot_product_attention expects attention_mask shape to be\n",
    "            # (batch, heads, source_length, target_length)\n",
    "            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        query = attn.to_q(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states\n",
    "        elif attn.norm_cross:\n",
    "            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "\n",
    "        inner_dim = key.shape[-1]\n",
    "        head_dim = inner_dim // attn.heads\n",
    "\n",
    "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        # the output of sdp = (batch, num_heads, seq_len, head_dim)\n",
    "        # TODO: add support for attn.scale when we move to Torch 2.1\n",
    "        if attention_mask is not None or query.shape[3] > query.shape[2] or query.shape[3] > 128 or value.shape[2] == 77:\n",
    "            hidden_states = F.scaled_dot_product_attention(\n",
    "                query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n",
    "            )\n",
    "        else:\n",
    "            hidden_states = attention_wrapper_without_swap(query, key, value)\n",
    "\n",
    "        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
    "        hidden_states = hidden_states.to(query.dtype)\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n",
    "\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEURON_COMPILER_WORKDIR = Path(\"neuron_compiler_workdir\")\n",
    "NEURON_COMPILER_WORKDIR.mkdir(exist_ok=True)\n",
    "NEURON_COMPILER_OUTPUT_DIR = Path(\"compiled_models\")\n",
    "NEURON_COMPILER_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MODEL_ID = \"runwayml/stable-diffusion-v1-5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Notice that the 512x512 output image size coincides with the default output image size:\n",
    "* The default height is indeed equal to `unet_sample_size * vae_scaling_factor`, i.e. `64*8=512`.\n",
    "* Same goes for the default width.\n",
    "\n",
    "The default output image format is therefore a square image of size 512x512 pixels.\n",
    "\n",
    "***Remarks:***\n",
    "* The `unet_sample_size` is part of the [U-Net configuration values](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/unet/config.json),\n",
    "* The `vae_scaling_factor` is equal to $2^{nb\\_vae\\_decoder\\_blocks-1}=8$ (cf. the [VAE configuration](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/vae/config.json)).\n",
    "\n",
    "**WARNING**: In this notebook, you are going to compile compute-heavy components of the Stable Diffusion pipeline. The compilation step \n",
    "compiles the execution path traced ***at compile-time*** into a static Neuron-optimized set of instructions. To avoid runtime errors, tensor \n",
    "shapes supplied to compiled models must be identical at compile and runtime. It is therefore highly recommended to use the same loading and generation \n",
    "parameters at compile and runtime. If not possible, be very cautious when modifying parameters that impact the shape of input tensors, e.g. \n",
    "batch size, `num_images_per_prompt`, `guidance_scale`, etc. Parameters that do not influence tensor shapes can safely take different values \n",
    "between compile and runtime, e.g. `num_inference_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = WIDTH = 512\n",
    "DTYPE = torch.float32\n",
    "BATCH_SIZE = 1\n",
    "NUM_IMAGES_PER_PROMPT = 1\n",
    "\n",
    "PIPELINE_LOADING_CONFIG = {\n",
    "    \"revision\": \"main\", # FP32 checkpoints\n",
    "    \"use_safetensors\": True,\n",
    "    \"low_cpu_mem_usage\": True,\n",
    "}\n",
    "\n",
    "PIPELINE_GENERATION_CONFIG = {\n",
    "    \"height\": HEIGHT,\n",
    "    \"width\": WIDTH,\n",
    "    \"num_inference_steps\": 50,\n",
    "    \"num_images_per_prompt\": NUM_IMAGES_PER_PROMPT,\n",
    "    \"guidance_scale\": 8.0,\n",
    "    \"output_type\": \"pil\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron compiler configuration\n",
    "\n",
    "# Reminder: By default, the Neuron Compiler (neuronx-cc) casts FP32 operations that use the Neuron matrix-multiplication \n",
    "# engine (--auto-cast matmul) to the lower-precision BF16 data type (--auto-cast-type bf16). Supported lower-precision\n",
    "# data types are: tf32, fp16, bf16 and fp8_e4m3.\n",
    "NEURON_COMPILER_TYPE_CASTING_CONFIG = [\n",
    "    \"--auto-cast=matmult\",\n",
    "    f\"--auto-cast-type=bf16\"\n",
    "]\n",
    "\n",
    "NEURON_COMPILER_CLI_ARGS = [\n",
    "    \"--target=inf2\",\n",
    "    \"--enable-fast-loading-neuron-binaries\",\n",
    "    *NEURON_COMPILER_TYPE_CASTING_CONFIG,\n",
    "]\n",
    "\n",
    "# https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/api-reference-guide/training/torch-neuron-envvars.html\n",
    "os.environ[\"NEURON_FUSE_SOFTMAX\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. List compile time and runtime requirements\n",
    "In this section, you will:\n",
    "* Load the Stable Diffusion pipeline\n",
    "* Make the `forward` method of the models you will compile more verbose using a decorator function\n",
    "* Perform a trial pipeline run on CPU to gather the input/output information you will need in the next sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_args(args: Tuple[Any]) -> None:\n",
    "    print(f\"Positional args (count: {len(args)})\")\n",
    "    for i, arg in enumerate(args):\n",
    "        if isinstance(arg, torch.Tensor):\n",
    "            arg = f\"Tensor{tuple(arg.shape)}\"\n",
    "        print(f\"  - arg({i}): {arg}\")\n",
    "\n",
    "def print_kwargs(kwargs: Dict[str, Any]) -> None:\n",
    "    print(f\"Keyword args (count: {len(kwargs)})\")\n",
    "    for i, (kwarg_name, kwarg_value) in enumerate(kwargs.items()):\n",
    "        if isinstance(kwarg_value, torch.Tensor):\n",
    "            kwarg_value = f\"Tensor{tuple(kwarg_value.shape)}\"\n",
    "        print(f\"  - kwarg({i}): {kwarg_name}={kwarg_value}\")\n",
    "\n",
    "def print_output(output: Any) -> None:\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        print(f\"Output: Tensor{tuple(output.shape)}\")\n",
    "    elif isinstance(output, BaseModelOutputWithPooling):\n",
    "        print(f\"Output type: {type(output).__name__}\")\n",
    "        print(f\"  last_hidden_state=Tensor{tuple(output.last_hidden_state.shape)}\")\n",
    "        print(f\"  pooler_output=Tensor{tuple(output.pooler_output.shape)}\")\n",
    "    elif isinstance(output, UNet2DConditionOutput):\n",
    "        print(f\"Output type: {type(output).__name__}\")\n",
    "        print(f\"  sample=Tensor{tuple(output.sample.shape)}\")\n",
    "    else:\n",
    "        print(f\"Output type: {type(output).__name__}\")\n",
    "\n",
    "def make_forward_verbose(model: torch.nn.Module, model_name: str) -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    The `make_forward_verbose` function is implemented as a Python decorator function with custom arguments.\n",
    "    The `make_forward_verbose` decorates an input model. \n",
    "    Model decoration consists in:\n",
    "        1. Decorating the model's forward method using the `make_verbose` decorator function,\n",
    "        2. Monkey-patching the orginal method with the decorated one.\n",
    "    \"\"\"\n",
    "    def make_verbose(f: Callable) -> Callable:\n",
    "        def decorated_forward_method(*args, **kwargs) -> Any:\n",
    "            print(\"-\"*50)\n",
    "            print(f\"Model: {model_name}\")\n",
    "            print(f\"Model type: {type(model).__name__}\")\n",
    "            print_args(args)\n",
    "            print_kwargs(kwargs)\n",
    "            output = f(*args, **kwargs)\n",
    "            print_output(output)\n",
    "            return output\n",
    "        return decorated_forward_method\n",
    "    model.forward = make_verbose(model.forward)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(MODEL_ID, **PIPELINE_LOADING_CONFIG)\n",
    "\n",
    "pipe.text_encoder = make_forward_verbose(model=pipe.text_encoder, model_name=\"CLIP text encoder\")\n",
    "pipe.unet = make_forward_verbose(model=pipe.unet, model_name=\"U-Net\")\n",
    "pipe.vae.decoder = make_forward_verbose(model=pipe.vae.decoder, model_name=\"VAE (decoder)\")\n",
    "pipe.vae.post_quant_conv = make_forward_verbose(model=pipe.vae.post_quant_conv, model_name=\"VAE (post_quant_conv)\")\n",
    "pipe.safety_checker.vision_model = make_forward_verbose(model=pipe.safety_checker.vision_model, model_name=\"Safety checker (vision_model)\")\n",
    "pipe.safety_checker.visual_projection = make_forward_verbose(model=pipe.safety_checker.visual_projection, model_name=\"Safety checker (visual_projection)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {**PIPELINE_GENERATION_CONFIG, \"num_inference_steps\": 1}\n",
    "pipe(\"a photo of an astronaut riding a horse on mars\", **generation_config)\n",
    "del pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Notices:***\n",
    "* A pipeline loaded using `.from_pretrained` is automatically set in evaluation mode [by default](https://huggingface.co/docs/diffusers/api/diffusion_pipeline#diffusers.DiffusionPipeline.from_pretrained), ([`model.eval()`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval)),\n",
    "* Gradient calculation is automatically disabled when executing a pipeline by calling its `__call__` magic method ([`torch.no_grad()`](https://pytorch.org/docs/stable/generated/torch.no_grad.html)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compile the models\n",
    "***Warning***: Compiling models for Neuron consumes a lot of memory. The following cells therefore carefully manage their variables. For each model to be compiled, you will only keep a copy of the model, the rest of the pipeline will be discarded to save memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. The CLIP text encoder\n",
    "From the information gathered above, you can see that at runtime, the CLIP text encoder:\n",
    "* Gets a single input tensor of size `(batch_size, model_max_length)`, i.e. `(1, 77)`, the tensor of input token IDs. Cf. the `model_max_length` \n",
    "[tokenizer configuration](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/tokenizer/tokenizer_config.json)).\n",
    "* Returns a object of type `transformers.modeling_outputs.BaseModelOutputWithPooling`. This object features 2 non-empty indexable attributes:\n",
    "  * `last_hidden_state`: Tensor of shape `(batch_size, max_input_seq_length, encoder_projection_dim)`, i.e. `(1, 77, 768)`.\n",
    "  * `pooler_output`: Tensor of shape `(batch_size, encoder_projection_dim)`, i.e. `(1, 768)`.\n",
    "\n",
    "***Notices:***\n",
    "* The text encoder can actually take two input tensors which correspond to the two tensors returned by the tokenizer, i.e. the tensor of token IDs and the \n",
    "corresponding attention mask. In the present case, we are ensured that the attention mask is never used, i.e. that the value of the `attention_mask` argument \n",
    "is always `None`. Using an attention mask or not is indeed a property of the text encoder. The attention mask is used [only if](https://github.com/huggingface/diffusers/blob/v0.20.0/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L328) \n",
    "the configuration of the text encoder includes a `use_attention_mask` entry and if this entry is set to `True`. \n",
    "[This is not the case for the CLIP text encoder](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/text_encoder/config.json), the example input tensors therefore do not include an attention mask tensor.\n",
    "* `model_max_length` is the maximum input tokenized sequence length. Longer sequences are trucated by the tokenizer. Shorter sequences are right-padded \n",
    "by the tokenizer.\n",
    "* `encoder_projection_dim` is the text embedding size. Cf. the [text encoder configuration](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/text_encoder/config.json).\n",
    "\n",
    "To be able to compile the `torch.nn.Module` corresponding to the CLIP text encoder, you will modify its `forward` method so that:\n",
    "* It returns a tuple of tensors instead of a `BaseModelOutputWithPooling` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_ENCODER_COMPILATION_DIR = NEURON_COMPILER_WORKDIR / \"text_encoder\"\n",
    "TEXT_ENCODER_COMPILATION_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def ensure_text_encoder_forward_neuron_compilable(model: CLIPTextModel) -> CLIPTextModel:\n",
    "    def decorate_forward_method(f: Callable) -> Callable:\n",
    "        def decorated_forward_method(*args, **kwargs) -> Tuple[torch.Tensor]:\n",
    "            kwargs.update({\"return_dict\": False})\n",
    "            output = f(*args, **kwargs)\n",
    "            return output\n",
    "        return decorated_forward_method\n",
    "    model.forward = decorate_forward_method(model.forward)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To minimze memory pressure, you only keep the model being compiled in RAM\n",
    "pipe = StableDiffusionPipeline.from_pretrained(MODEL_ID, **PIPELINE_LOADING_CONFIG)\n",
    "\n",
    "text_encoder = copy.deepcopy(pipe.text_encoder)\n",
    "text_encoder = ensure_text_encoder_forward_neuron_compilable(text_encoder)\n",
    "\n",
    "VOCAB_SIZE = pipe.tokenizer.vocab_size\n",
    "MODEL_MAX_LENGTH = pipe.tokenizer.model_max_length\n",
    "\n",
    "del pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution time: ~1-2mins\n",
    "example_input_ids = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, MODEL_MAX_LENGTH), dtype=torch.int64)\n",
    "\n",
    "with torch.no_grad(): \n",
    "    text_encoder_neuron = torch_neuronx.trace(\n",
    "        text_encoder,\n",
    "        example_input_ids, \n",
    "            compiler_workdir=TEXT_ENCODER_COMPILATION_DIR,\n",
    "            compiler_args=[*NEURON_COMPILER_CLI_ARGS, f'--logfile={TEXT_ENCODER_COMPILATION_DIR}/log-neuron-cc.txt'],\n",
    "            )\n",
    "\n",
    "# Free up memory\n",
    "del example_input_ids, text_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize a representation of the graph operations of the `forward` method of the `torch.jit.ScriptModule` returned by `torch_neuronx.trace`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_encoder_neuron.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can clearly see that the call to the `forward` method:\n",
    "* Takes a single input tensor,\n",
    "* Consists of a single Neuron computation step,\n",
    "* Returns a 2-tuple of tensors.\n",
    "\n",
    "Now, let's enable both lazy and asynchronous loading for a better model loading performance and serialize the Neuron `ScriptModule` by simply using `torch.jit.save`. \n",
    "For more information, refer to the documentation of the [PyTorch Neuron Lazy and Asynchronous Loading API](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/api-reference-guide/inference/api-torch-neuronx-async-lazy-load.html#torch-neuronx-lazy-async-load-api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_neuronx.async_load(text_encoder_neuron)\n",
    "torch_neuronx.lazy_load(text_encoder_neuron)\n",
    "\n",
    "torch.jit.save(text_encoder_neuron, NEURON_COMPILER_OUTPUT_DIR / \"text_encoder.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "del text_encoder_neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. The U-Net\n",
    "#### 2.2.1. Negative prompting\n",
    "The U-Net predicts the amount of noise from a input sample (latent) at the given time step `t`. The predicted noise is then used by the scheduler to generate the (denoised) \n",
    "sample corresponding to the the previous time step. The noise prediction is conditioned on the input text prompt. To better guide the model in generating the desired image, the \n",
    "`StableDiffusionPipeline` allows the user to provide what they don't want to see in the generated image using a separate text prompt called \"negative prompt\" (`negative_prompt` argument).\n",
    "When using negative prompts, two U-Net forward passes are performed at each time step: one to generate a noise prediction conditionned on the prompt, another to generate a noise \n",
    "prediction conditioned on the negative prompt. The final noise prediction for a given time step is created from both predictions and using a guidance coefficient (\"classifier-free guidance\"). \n",
    "The guidance coefficient (`guidance_scale` generation parameter) is a number greater than 1, the higher the value, the closer the generated image is from the input prompt.\n",
    "\n",
    "For example the following `(prompt, negative_prompt)` pair, `(\"portrait of a man\", \"mustache\")` would aim at generating portraits of men without a mustache and would probably \n",
    "do a better job than a single `\"portrait of a man without mustache\"` prompt.\n",
    "\n",
    "***Notice:*** The value of the `guidance_scale` actually determines whether classifier-free guidance, to be understood here as \"guidance using negative prompts\", is used or not. \n",
    "If `guidance_scale` is greater than `1.0`, then denoised samples are generated using classifier-free guidance, and two U-Net forward passes are performed at each time step. If \n",
    "no `negative_prompt` is supplied, a fully padded negative prompt is actually used under the hood. If `guidance_scale` is lower or equal to `1.0`, then classifier-free guidance \n",
    "is disabled: a single U-Net forward pass is performed at each time step, supplied `negative_prompts` are ignored.\n",
    "\n",
    "From the tensor operations perpective, classifier-free guidance / negative prompts have the following impact:\n",
    "* The text encoder is used twice: once on the prompt(s), once on the negative prompt(s).\n",
    "* Two U-Net forward passes are performed at each time step. For increased efficiency, the `diffusers` implementation concatenates the encodings of both prompts in a single tensor \n",
    "and a new sample input tensor is created by repeating the sample tensor twice. This trick allows to compute both noise predictions in a single forward pass. **The U-Net input \n",
    "tensors therefore do not have the same shape depending on whether the user use classifier-free guidance or not**.\n",
    "\n",
    "#### 2.2.2. Tensor shapes\n",
    "The first dimension of U-Net input tensor is:\n",
    "* If classifier-free guidance (negative prompts) is disabled: `batch_size*num_images_per_prompt`.\n",
    "* If classifier-free guidance (negative prompts) is enabled: `2*batch_size*num_images_per_prompt` (cf. above for more information about the `2` factor).\n",
    "\n",
    "In the present case, classifier-free guidance is **enabled**.\n",
    "\n",
    "From the information gathered in the first section, you can see that at runtime, the U-Net:\n",
    "* Gets three input tensors:\n",
    "  * A `sample` tensor (arg 0) of shape `(2*batch_size*num_images_per_prompt, unet_in_channels, height // vae_scaling_factor, width // vae_scaling_factor)`, i.e. `(2*1*1, 4, 512//8, 512//8)`, i.e. `(2, 4, 64, 64)`:\n",
    "    * `unet_in_channels` is a [U-Net configuration](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/unet/config.json),\n",
    "    * The VAE scaling factor is $2^{nb\\_vae\\_decoder\\_blocks-1}=8$ (cf. the [VAE configuration](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/vae/config.json)).\n",
    "  * A 0-dim `timestep` tensor (arg 1). You will need to make it a 1-dim tensor, i.e. a tensor of shape `(2*batch_size*num_images_per_prompt,)`, i.e. `(2,)`.\n",
    "  * A `encoder_hidden_states` tensor (kwarg 0) of shape `(2*batch_size*num_images_per_prompt, max_input_seq_length, d_model)`, i.e. `(2, 77, 768)`. For each prompt kind, text encoding are repeated `num_images_per_prompt` times.\n",
    "  This tensor concatenates the representations of both positive and negative prompts along its dimension 0.\n",
    "* Returns a object of type `diffusers.models.unet_2d_condition.UNet2DConditionOutput`. This object features a single non-empty indexable attribute:\n",
    "  * `sample`: Tensor of shape indentical to the input `sample` tensor.\n",
    "\n",
    "To be able to compile the `torch.nn.Module` corresponding to the CLIP text encoder, you will modify its `forward` method so that:\n",
    "* It returns a plain tensor instead of a `UNet2DConditionOutput` object.\n",
    "\n",
    "#### 2.2.3. Compilation options\n",
    "Assuming `num_images_per_prompt=1` for simplicity, one faces the following alternative when compiling the U-Net model:\n",
    "* Using example inputs of size `2*batch_size` along their dimension 0. In that case, the compiled U-Net will compute the noise prediction for both positive and negative text conditioning.\n",
    "* Using example inputs of size `batch_size` along their dimension 0 and apply model parallelism (parallelism degree of 2) to the compiled U-Net. Computation would be split over two model replicas, \n",
    "therefore making a better use of the available NeuronCores (there are indeed at least 2 NeuronCores available per instance) and would contribute to increasing throughput.\n",
    "\n",
    "**This notebooks opts for the second option.** Actual input tensor shapes will therefore be:\n",
    "* `sample`: `(1, 4, 64, 64)`\n",
    "* `timestep`: `(1,)`\n",
    "* `encoder_hidden_states`: `(1, 77, 768)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNET_COMPILATION_DIR = NEURON_COMPILER_WORKDIR / \"unet\"\n",
    "UNET_COMPILATION_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def ensure_unet_forward_neuron_compilable(model: UNet2DConditionModel) -> UNet2DConditionModel:\n",
    "    def decorate_forward_method(f: Callable) -> Callable:\n",
    "        def decorated_forward_method(*args, **kwargs) -> torch.Tensor:\n",
    "            kwargs.update({\"return_dict\": False})\n",
    "            output_sample, = f(*args, **kwargs)\n",
    "            return output_sample\n",
    "        return decorated_forward_method\n",
    "    model.forward = decorate_forward_method(model.forward)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To minimze memory pressure, you only keep the model being compiled in RAM\n",
    "pipe = StableDiffusionPipeline.from_pretrained(MODEL_ID, **PIPELINE_LOADING_CONFIG)\n",
    "diffusers.models.attention_processor.AttnProcessor2_0.__call__ = KernelizedAttnProcessor2_0.__call__\n",
    "unet = copy.deepcopy(pipe.unet)\n",
    "unet = ensure_unet_forward_neuron_compilable(unet)\n",
    "\n",
    "UNET_IN_CHANNELS = pipe.unet.config.in_channels\n",
    "VAE_SCALING_FACTOR = 2**(len(pipe.vae.config.block_out_channels)-1)\n",
    "ENCODER_PROJECTION_DIM = pipe.text_encoder.config.hidden_size\n",
    "MODEL_MAX_LENGTH = pipe.tokenizer.model_max_length\n",
    "\n",
    "del pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention computation is a particularly compute-heavy operation. In the following cell, you monkey-patch the attention computation with an implementation that ensures it is matched \n",
    "to a highly-optimized kernel by the Neuron Compiler. \n",
    "\n",
    "You actually only monkey-patch the `get_attention_scores` method of the attention processors used by the cross-attention blocks of the \n",
    "UNet (`UNet2DConditionModel`), i.e. `diffuser.models.attention_processor.Attention`. Here are the key differences with the `diffusers` implementation:\n",
    "* The code related to the attention mask has been removed. In the first section, you indeed noticed that no attention mask was actually passed to the Unet's `forward` method.\n",
    "* The native implementation uses the biased `torch.baddbmm` function which performs the following operation: `beta*attention_mask+scaling_factor*(query@key)`. Since no attention mask \n",
    "is used, only the `scaling_factor*(query@key)` part is relevant and this is exactly what the simpler `torch.bmm` does.\n",
    "* The `torch.nn.functional.softmax` function is used instead of the `torch.Tensor.softmax` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_scores(self, query: torch.Tensor, key: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "    dtype = query.dtype\n",
    "\n",
    "    if self.upcast_attention:\n",
    "        query = query.float()\n",
    "        key = key.float()\n",
    "\n",
    "    attention_scores = self.scale * torch.bmm(query, key.transpose(-1, -2))\n",
    "    # output = torch.bmm(input_1, input_2) where input_1 of shape (b, n, m), input_2 of shape (b, m, p) and output of shape (b, n, p)\n",
    "    # Attention: query of shape (b, model_length, d_q), key of shape (b, model_length, d_k), assuming d_q=d_k, expected \n",
    "    # bmm(query, key) = attention tensor of shape (b, model_length, model_length) -> Key tensor dims needs to be rearranged into\n",
    "    # (b, d_k, model_length), i.e. swapping dim -1 & -2, i.e. key.transpose(-1, -2)\n",
    "    \n",
    "    if self.upcast_softmax:\n",
    "        attention_scores = attention_scores.float()\n",
    "\n",
    "    attention_probs = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "    del attention_scores\n",
    "    attention_probs = attention_probs.to(dtype)\n",
    "        \n",
    "    return attention_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monkey-patching\n",
    "Attention.get_attention_scores = get_attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution time: ~10mins\n",
    "example_input_sample = torch.randn((BATCH_SIZE*NUM_IMAGES_PER_PROMPT, UNET_IN_CHANNELS, HEIGHT//VAE_SCALING_FACTOR, WIDTH//VAE_SCALING_FACTOR), dtype=DTYPE)\n",
    "example_timestep = torch.randint(0, 1000, (BATCH_SIZE*NUM_IMAGES_PER_PROMPT,), dtype=DTYPE)\n",
    "example_encoder_hidden_states = torch.randn((BATCH_SIZE*NUM_IMAGES_PER_PROMPT, MODEL_MAX_LENGTH, ENCODER_PROJECTION_DIM), dtype=DTYPE)\n",
    "example_inputs = (example_input_sample, example_timestep, example_encoder_hidden_states)\n",
    "\n",
    "with torch.no_grad():\n",
    "    unet_neuron = torch_neuronx.trace(\n",
    "        unet,\n",
    "        example_inputs,\n",
    "        compiler_workdir=UNET_COMPILATION_DIR,\n",
    "        compiler_args=[*NEURON_COMPILER_CLI_ARGS, f'--logfile={UNET_COMPILATION_DIR}/log-neuron-cc.txt', \"--model-type=unet-inference\"],\n",
    "    )\n",
    "\n",
    "# Free up memory\n",
    "del example_input_sample, example_timestep, example_encoder_hidden_states, example_inputs, unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize a representation of the graph operations of the `forward` method of the `torch.jit.ScriptModule` returned by `torch_neuronx.trace`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unet_neuron.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can clearly see that the call to the `forward` method:\n",
    "* Takes three input tensors,\n",
    "* Consists of a single Neuron computation step,\n",
    "* Returns a single tensor.\n",
    "\n",
    "Now, let's enable both lazy and asynchronous loading for a better model loading performance and serialize the Neuron `ScriptModule` by simply using `torch.jit.save`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_neuronx.async_load(unet_neuron)\n",
    "torch_neuronx.lazy_load(unet_neuron)\n",
    "\n",
    "torch.jit.save(unet_neuron, NEURON_COMPILER_OUTPUT_DIR / \"unet.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "del unet_neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. The VAE\n",
    "At inference time, only the VAE's decoder is used. The decoder actually consists of two models that you need to compile: the post_quant_conv model and the decoder itself.\n",
    "\n",
    "From the information gathered in the first section, you can see that at runtime, the `forward` method of both elements takes a single tensor as input and returns a single \n",
    "tensor as output. There is therefore no need to modify the `forward` method using decoration/monkey-patching for compilation. Since the post_quant_conv model is very lightweight, \n",
    "you will compile both at the same time.\n",
    "\n",
    "The post_quant_conv model input and output tensors are of shape `(batch_size*num_images_per_prompt, unet_out_channels, height // vae_scaling_factor, width // vae_scaling_factor)`, i.e. `(1, 4, 64, 64)`.\n",
    "\n",
    "***Notice***: The value of the `unet_out_channels` U-Net configuration is identical to the value of the `vae_latent_channels` [VAE configuration](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/vae/config.json).\n",
    "\n",
    "For the decoder model:\n",
    "* Input tensor is identical to the output tensor of the post_quant_conv model.\n",
    "* Output tensor is of shape `(batch_size*num_images_per_prompt, vae_out_channels, height, width)`, i.e. `(1, 3, 512, 512)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE_COMPILATION_DIR = NEURON_COMPILER_WORKDIR / \"vae\"\n",
    "VAE_COMPILATION_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To minimze memory pressure, you only keep the model being compiled in RAM\n",
    "pipe = StableDiffusionPipeline.from_pretrained(MODEL_ID, **PIPELINE_LOADING_CONFIG)\n",
    "\n",
    "vae_post_quant_conv = copy.deepcopy(pipe.vae.post_quant_conv)\n",
    "vae_decoder = copy.deepcopy(pipe.vae.decoder)\n",
    "\n",
    "LATENT_CHANNELS = pipe.vae.config.latent_channels\n",
    "VAE_SCALING_FACTOR = 2**(len(pipe.vae.config.block_out_channels)-1)\n",
    "\n",
    "del pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution time: ~7-8mins\n",
    "example_latent_sample = torch.randn((BATCH_SIZE*NUM_IMAGES_PER_PROMPT, LATENT_CHANNELS, HEIGHT//VAE_SCALING_FACTOR, WIDTH//VAE_SCALING_FACTOR), dtype=DTYPE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    VAE_POST_QUANT_COMPILATION_DIR = VAE_COMPILATION_DIR / \"post_quant_conv\"\n",
    "    vae_post_quant_conv_neuron = torch_neuronx.trace(\n",
    "        vae_post_quant_conv,\n",
    "        example_latent_sample,\n",
    "        compiler_workdir=VAE_POST_QUANT_COMPILATION_DIR,\n",
    "        compiler_args=[*NEURON_COMPILER_CLI_ARGS, f'--logfile={VAE_POST_QUANT_COMPILATION_DIR}/log-neuron-cc.txt'],\n",
    "    )\n",
    "\n",
    "    VAE_DECODER_COMPILATION_DIR = VAE_COMPILATION_DIR / \"decoder\"\n",
    "    vae_decoder_neuron = torch_neuronx.trace(\n",
    "        vae_decoder,\n",
    "        example_latent_sample,\n",
    "        compiler_workdir=VAE_DECODER_COMPILATION_DIR / \"decoder\",\n",
    "        compiler_args=[*NEURON_COMPILER_CLI_ARGS, \"--model-type=unet-inference\", f'--logfile={VAE_DECODER_COMPILATION_DIR}/log-neuron-cc.txt'],\n",
    "    )\n",
    "\n",
    "# Free up memory\n",
    "del vae_post_quant_conv, vae_decoder, example_latent_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize a representation of the graph operations of the `forward` method of the `torch.jit.ScriptModule` returned by `torch_neuronx.trace`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vae_post_quant_conv_neuron.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vae_decoder_neuron.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can clearly see that in both cases, the call to the `forward` method:\n",
    "* Takes a single input tensor,\n",
    "* Consists of a single Neuron computation step,\n",
    "* Returns a single tensor.\n",
    "\n",
    "Now, let's enable both lazy and asynchronous loading for a better model loading performance and serialize the Neuron `ScriptModule`s by simply using `torch.jit.save`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for neuron_model, file_name in zip((vae_post_quant_conv_neuron, vae_decoder_neuron), (\"vae_post_quant_conv.pt\", \"vae_decoder.pt\")):\n",
    "    torch_neuronx.async_load(neuron_model)\n",
    "    torch_neuronx.lazy_load(neuron_model)\n",
    "    torch.jit.save(neuron_model, NEURON_COMPILER_OUTPUT_DIR / file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "del vae_post_quant_conv_neuron, vae_decoder_neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. The safety model\n",
    "Like the VAE, the safety checker actually consist of two model components that you need to compile separately: the vision model and the visual projection.\n",
    "\n",
    "Regarding the vision model, from the information gathered in the first section, the safety model is a CLIP vision transformer that:\n",
    "* Gets a single input tensor of size `(batch_size*num_images_per_prompt, vae_out_channels, feature_extractor_crop_height, feature_extractor_crop_width)`, i.e. `(1, 3, 224, 224)` \n",
    "(cf. the `crop_size` [feature extractor configuration](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/feature_extractor/preprocessor_config.json)). \n",
    "* Returns a object of type `transformers.modeling_outputs.BaseModelOutputWithPooling`. This object features 2 non-empty indexable attributes:\n",
    "  * `last_hidden_state`: Tensor of shape `(batch_size*num_images_per_prompt, ?, hidden_size)`, i.e. `(1, 257, 1024)`.\n",
    "  * `pooler_output`: Tensor of shape `(batch_size*num_images_per_prompt, hidden_size)`, i.e. `(1, 1024)`.\n",
    "\n",
    "Similarly to the CLIP text encoder, to be able to compile the `torch.nn.Module` corresponding to the safety checker, you will modify its `forward` method so that:\n",
    "* It returns a tuple of tensors instead of a `BaseModelOutputWithPooling` object.\n",
    "\n",
    "Regarding the visual projection, it is actually a simple `torch.nn.Linear` layer that:\n",
    "* Gets a single input tensor of size `(batch_size*num_images_per_prompt, hidden_size)`, i.e. `(1, 1024)` (the `pooler_output` from the vision model).\n",
    "* Returns a single tensor of size `(batch_size*num_images_per_prompt, projection_dim)`, i.e. `(1, 768)`.\n",
    "\n",
    "The `forward` method of the visual projection model can therefore be directly traced and compiled by `torch_neuronx.trace`. Since the visual projection model is very lightweight, \n",
    "you will compile both models at the same time.\n",
    "\n",
    "***Notice:*** Cf. the `hidden_size` and `projection_dim` [safety checker configurations](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/safety_checker/config.json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAFETY_CHECKER_COMPILATION_DIR = NEURON_COMPILER_WORKDIR / \"safety_checker\"\n",
    "SAFETY_CHECKER_COMPILATION_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def ensure_vision_model_forward_neuron_compilable(model: CLIPVisionModel) -> CLIPVisionModel:\n",
    "    def decorate_forward_method(f: Callable) -> Callable:\n",
    "        def decorated_forward_method(*args, **kwargs) -> Tuple[torch.Tensor]:\n",
    "            kwargs.update({\"return_dict\": False})\n",
    "            output = f(*args, **kwargs)\n",
    "            return output\n",
    "        return decorated_forward_method\n",
    "    model.forward = decorate_forward_method(model.forward)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To minimze memory pressure, you only keep the model being compiled in RAM\n",
    "pipe = StableDiffusionPipeline.from_pretrained(MODEL_ID, **PIPELINE_LOADING_CONFIG)\n",
    "\n",
    "safety_checker_vision_model = copy.deepcopy(pipe.safety_checker.vision_model)\n",
    "safety_checker_visual_projection = copy.deepcopy(pipe.safety_checker.visual_projection)\n",
    "safety_checker_vision_model = ensure_vision_model_forward_neuron_compilable(safety_checker_vision_model)\n",
    "\n",
    "VAE_OUT_CHANNELS = pipe.vae.config.out_channels\n",
    "FEATURE_EXTRACTOR_CROP_HEIGHT, FEATURE_EXTRACTOR_CROP_WIDTH = pipe.feature_extractor.crop_size.values()\n",
    "VISION_MODEL_HIDDEN_DIM = pipe.safety_checker.config.vision_config.hidden_size\n",
    "\n",
    "del pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution time: ~2-3mins\n",
    "example_safety_checker_vision_model_input = torch.randn((BATCH_SIZE*NUM_IMAGES_PER_PROMPT, VAE_OUT_CHANNELS, FEATURE_EXTRACTOR_CROP_HEIGHT, FEATURE_EXTRACTOR_CROP_WIDTH), dtype=DTYPE)\n",
    "example_safety_checker_visual_projection_input = torch.randn((BATCH_SIZE*NUM_IMAGES_PER_PROMPT, VISION_MODEL_HIDDEN_DIM), dtype=DTYPE)\n",
    "\n",
    "with torch.no_grad(): \n",
    "    SAFETY_CHECKER_VISION_MODEL_COMPILATION_DIR = SAFETY_CHECKER_COMPILATION_DIR / \"vision_model\"\n",
    "    safety_checker_vision_model_neuron = torch_neuronx.trace(\n",
    "        safety_checker_vision_model,\n",
    "        example_safety_checker_vision_model_input, \n",
    "            compiler_workdir=SAFETY_CHECKER_VISION_MODEL_COMPILATION_DIR,\n",
    "            compiler_args=[*NEURON_COMPILER_CLI_ARGS, f'--logfile={SAFETY_CHECKER_VISION_MODEL_COMPILATION_DIR}/log-neuron-cc.txt'],\n",
    "            )\n",
    "\n",
    "    SAFETY_CHECKER_VISUAL_PROJECTION_DIR = SAFETY_CHECKER_COMPILATION_DIR / \"visual_projection\"\n",
    "    safety_checker_visual_projection_neuron = torch_neuronx.trace(\n",
    "        safety_checker_visual_projection,\n",
    "        example_safety_checker_visual_projection_input, \n",
    "            compiler_workdir=SAFETY_CHECKER_VISUAL_PROJECTION_DIR,\n",
    "            compiler_args=[*NEURON_COMPILER_CLI_ARGS, f'--logfile={SAFETY_CHECKER_VISUAL_PROJECTION_DIR}/log-neuron-cc.txt'],\n",
    "            )\n",
    "\n",
    "# Free up memory\n",
    "del safety_checker_vision_model, example_safety_checker_vision_model_input, safety_checker_visual_projection, example_safety_checker_visual_projection_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize a representation of the graph operations of the `forward` method of the `torch.jit.ScriptModule` returned by `torch_neuronx.trace`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(safety_checker_vision_model_neuron.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(safety_checker_visual_projection_neuron.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the vision model, one can clearly see that the call to the `forward` method:\n",
    "* Takes a single input tensor,\n",
    "* Consists of a single Neuron computation step,\n",
    "* Returns a 2-tuple of tensors.\n",
    "\n",
    "The case of the visual projection model is very similar but only returns a single tensor.\n",
    "\n",
    "Now, let's enable both lazy and asynchronous loading for a better model loading performance and serialize the Neuron `ScriptModule`s by simply using `torch.jit.save`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for neuron_model, file_name in zip((safety_checker_vision_model_neuron, safety_checker_visual_projection_neuron), (\"safety_checker_vision_model.pt\", \"safety_checker_visual_projection.pt\")):\n",
    "    torch_neuronx.async_load(neuron_model)\n",
    "    torch_neuronx.lazy_load(neuron_model)\n",
    "    torch.jit.save(neuron_model, NEURON_COMPILER_OUTPUT_DIR / file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "del safety_checker_vision_model_neuron, safety_checker_visual_projection_neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load the models\n",
    "In this section you will:\n",
    "1. Load the Stable Diffusion pipeline,\n",
    "2. Load the serialized compiled models,\n",
    "3. When required, modify the `forward` method of the loaded Neuron `torch.jit.ScriptModule` so that they are adapted to the `StableDiffusionPipeline` (cf. below).\n",
    "4. Loading the compiled models to your host's NeuronCores.\n",
    "\n",
    "## 3.1. Ensuring runtime compatibility\n",
    "In practice, running Stable Diffusion on Neuron devices consists in replacing the model components of the `StableDiffusionPipeline`. The `StableDiffusionPipeline` handles \n",
    "the different model components to generate an image from input text. In the first section, you extracted for each model to be compiled and run on Neuron:\n",
    "* The input positional and keyword arguments injected to their `forward` method by the `StableDiffusionPipeline`,\n",
    "* The output type of their `forward` method, i.e. the object type the `StableDiffusionPipeline` then expects for further processing.\n",
    "\n",
    "On the other hand, the `forward` method of compiled models only expect positional arguments and returns a single tensor or a sequence of tensors. Depending on the model, there \n",
    "is a mismatch between the interface of the compiled model and the interface set by the `StableDiffusionPipeline`. When studying the output of the first section, one can notice \n",
    "that the compiled VAE models can be used as is. For the other models however (text encoder, U-Net, safety checker vision model), their `forward` function need to be adapted. More concretely, \n",
    "you will use adaptor functions that 1) decorate (i.e. modify) a model's `forward` method, 2) monkey-patches the model's `torch.jit.ScriptModule` with the decorated `forward` method.\n",
    "\n",
    "## 3.2 U-Net data parallelism\n",
    "The following code applies data parallelism to the compiled U-Net. The data parallelism degree equals 2, i.e. `torch-neuronx` creates 2 model replicas. Input tensor are \n",
    "automatically chunked along their first dimension (dim 0). Since dynamic batching is disabled, input tensor must fulfill the following condition: `runtime_input.shape[0]/data_parallelism_degree == compile_time_input.shape[0]`. \n",
    "This condition is fulfilled since for dim 0, compile time size is 1, runtime size is 2 and data parallelism degree is 2.\n",
    "\n",
    "## 3.3 Dynamic batching\n",
    "[Dynamic batching](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/api-reference-guide/inference/api-torch-neuronx-trace.html#dynamic-batching) enables \n",
    "a compiled Neuron model to be called with variable sized batches. When disabled, input tensor shapes must fulfill the condition detailed in section 3.2. If not, a runtime error will be raised. \n",
    "Dynamic batching allows to consume input tensors that do not respect this condition without requiring to recompile.\n",
    "\n",
    "In the present case, dynamic batching would allow to use (possibly variable sized) batch sizes and/or a number of images per prompt greater than 1 (compile-time value). However, the property \n",
    "must be enabled for all compiled models.\n",
    "\n",
    "Dynamic batching can be enabled [at compile time](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/api-reference-guide/inference/api-torch-neuronx-trace.html#dynamic-batching) \n",
    "(disabled by default) or enabled/disabled [at load time](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/api-reference-guide/inference/api-torch-neuronx-data-parallel.html) when \n",
    "using the PyTorch Neuron DataParallel API (enabled by default) only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice: Text encoder kwargs are discared since 1) Neuron\n",
    "\n",
    "def text_encoder_runtime_decorator_factory(neuron_model: torch.jit.ScriptModule) -> torch.jit.ScriptModule:\n",
    "    def text_encoder_decorator(model: CLIPTextModel) -> CLIPTextModel:\n",
    "        def neuron_forward_method(*args, **kwargs) -> BaseModelOutputWithPooling:\n",
    "            input_ids, *_ = args # Ensures that only a single (the first) arg is supplied to the compiled model, kwargs are discarded\n",
    "            last_hidden_state, pooler_output = neuron_model(input_ids)\n",
    "            return BaseModelOutputWithPooling(last_hidden_state=last_hidden_state, pooler_output=pooler_output)\n",
    "        model.forward = neuron_forward_method\n",
    "        return model\n",
    "    return text_encoder_decorator\n",
    "\n",
    "\n",
    "def unet_runtime_decorator_factory(neuron_model: torch_neuronx.xla_impl.data_parallel.DataParallel) -> torch_neuronx.xla_impl.data_parallel.DataParallel:\n",
    "    def unet_decorator(model: UNet2DConditionModel) -> UNet2DConditionModel:\n",
    "        def neuron_forward_method(*args, **kwargs) -> UNet2DConditionOutput:\n",
    "            sample, timestep, *_ = args\n",
    "            dim_0 = sample.shape[0]\n",
    "            if isinstance(timestep, (int, float)):\n",
    "                timestep = torch.Tensor([timestep]*dim_0).type(DTYPE)\n",
    "            else:\n",
    "                # Can either be tensor([999.]) (1-dim) or tensor(999.) (0-dim)\n",
    "                timestep = timestep.type(DTYPE).repeat(dim_0)\n",
    "            encoder_hidden_states = kwargs[\"encoder_hidden_states\"]\n",
    "            output_sample = neuron_model(sample, timestep, encoder_hidden_states)\n",
    "            return UNet2DConditionOutput(sample=output_sample)\n",
    "        model.forward = neuron_forward_method\n",
    "        return model\n",
    "    return unet_decorator\n",
    "\n",
    "\n",
    "def vision_model_runtime_decorator_factory(neuron_model: torch.jit.ScriptModule) -> torch.jit.ScriptModule:\n",
    "    def vision_model_checker_decorator(model: CLIPVisionModel) -> CLIPVisionModel:\n",
    "        def neuron_forward_method(*args, **kwargs) -> BaseModelOutputWithPooling:\n",
    "            pixel_values, *_ = args # Ensures that only a single (the first) arg is supplied to the compiled model\n",
    "            last_hidden_state, pooler_output = neuron_model(pixel_values)\n",
    "            return BaseModelOutputWithPooling(last_hidden_state=last_hidden_state, pooler_output=pooler_output)\n",
    "        model.forward = neuron_forward_method\n",
    "        return model\n",
    "    return vision_model_checker_decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1. Load the StableDiffusionPipeline ----\n",
    "pipe = StableDiffusionPipeline.from_pretrained(MODEL_ID, **PIPELINE_LOADING_CONFIG)\n",
    "\n",
    "# ---- 2. Load the serialized compiled models ----\n",
    "# The VAE compiled models and the safety checker's visual projection compiled model can be directly used by the StableDiffusionPipeline\n",
    "pipe.vae.decoder = torch.jit.load(NEURON_COMPILER_OUTPUT_DIR / \"vae_decoder.pt\")\n",
    "pipe.vae.post_quant_conv = torch.jit.load(NEURON_COMPILER_OUTPUT_DIR / \"vae_post_quant_conv.pt\")\n",
    "\n",
    "pipe.safety_checker.visual_projection = torch.jit.load(NEURON_COMPILER_OUTPUT_DIR / \"safety_checker_visual_projection.pt\")\n",
    "\n",
    "text_encoder_neuron = torch.jit.load(NEURON_COMPILER_OUTPUT_DIR / \"text_encoder.pt\")\n",
    "unet_neuron = torch.jit.load(NEURON_COMPILER_OUTPUT_DIR / \"unet.pt\")\n",
    "safety_checker_vision_neuron = torch.jit.load(NEURON_COMPILER_OUTPUT_DIR / \"safety_checker_vision_model.pt\")\n",
    "# Since lazy loading has been enabled for all compiled models, nothing has been allocated on the NeuronCores at this point\n",
    "\n",
    "# ---- 3. Adapt the compiled models to the StableDiffusionPipeline ----\n",
    "ensure_text_encoder_forward_neuron_runnable = text_encoder_runtime_decorator_factory(neuron_model=text_encoder_neuron)\n",
    "pipe.text_encoder = ensure_text_encoder_forward_neuron_runnable(pipe.text_encoder)\n",
    "ensure_vision_model_forward_neuron_runnable = vision_model_runtime_decorator_factory(neuron_model=safety_checker_vision_neuron)\n",
    "pipe.safety_checker.vision_model = ensure_vision_model_forward_neuron_runnable(pipe.safety_checker.vision_model)\n",
    "\n",
    "# Since the U-Net is the pipeline's bottleneck, you apply data parallelism to the U-Net by creating two model replicas, each \n",
    "# on its own NeuronCore\n",
    "unet_neuron = torch_neuronx.DataParallel(unet_neuron, device_ids=[0, 1], set_dynamic_batching=False)\n",
    "ensure_unet_forward_neuron_runnable = unet_runtime_decorator_factory(neuron_model=unet_neuron)\n",
    "pipe.unet = ensure_unet_forward_neuron_runnable(pipe.unet)\n",
    "\n",
    "# ---- 4. Trigger the loading of Neuron models with a warmup generation ----\n",
    "generation_config = {**PIPELINE_GENERATION_CONFIG, \"num_inference_steps\": 1}\n",
    "pipe(\"a photo of an astronaut riding a horse on mars\", **generation_config);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Notice:*** When loading multiple models like here, the default behavior of the Neuron Runtime is to (automatically) evenly distribute models across all available NeuronCores. \n",
    "The Neuron Runtime places models on the NeuronCore that has the fewest models loaded to it first. Multiple models can therefore be automatically be placed on the same NeuronCore. \n",
    "However, a NeuronCore can only execute one model at a time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Run the Stable Diffusion pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional, for reproducibility, see also:\n",
    "#   - https://huggingface.co/docs/diffusers/using-diffusers/reusing_seeds\n",
    "#   - https://huggingface.co/docs/diffusers/using-diffusers/reproducibility\n",
    "\n",
    "DETERMINISTIC_GENERATION_ENABLED = True\n",
    "if DETERMINISTIC_GENERATION_ENABLED: \n",
    "    generators = [torch.Generator().manual_seed(i) for i in range(NUM_IMAGES_PER_PROMPT)]\n",
    "    PIPELINE_GENERATION_CONFIG.update({\"generator\": generators})\n",
    "elif not DETERMINISTIC_GENERATION_ENABLED and \"generator\" in PIPELINE_GENERATION_CONFIG:\n",
    "    PIPELINE_GENERATION_CONFIG.pop(\"generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    # \"Positive\" prompt only\n",
    "    (\"a photo of an astronaut riding a horse on mars\", None),\n",
    "    (\"sonic on the moon\", None),\n",
    "    (\"elvis playing guitar while eating a hotdog\", None),\n",
    "    (\"saved by the bell\", None),\n",
    "    (\"engineers eating lunch at the opera\", None),\n",
    "    # \"Positive\" & negative prompts\n",
    "    (\"panda eating bamboo on an aircraft\", \"lowres, out of frame, bad face\"),\n",
    "    (\"a digital illustration of a steampunk flying machine in the sky with cogs and mechanisms, 4k, detailed, trending in artstation, fantasy vivid colors\", \"dark, out of frame, photorealistic\"),\n",
    "    (\"kids playing soccer at the FIFA World Cup\", \"disfigured, poorly drawn face, oversaturated\"),\n",
    "    (\"a smiling tomato, cartoon style\", \"distorted, faded\"),\n",
    "    (\"ad for a burger, burger in the center, eiffel tower in the background\", \"underexposed, boring background\"),\n",
    "    ]\n",
    "\n",
    "plt.xlabel(\"Width (pixels)\")\n",
    "plt.ylabel(\"Height (pixels)\")\n",
    "\n",
    "generation_times = []\n",
    "for prompt, negative_prompt in prompts:\n",
    "    start_time = datetime.datetime.now()\n",
    "    image = pipe(prompt=prompt, negative_prompt=negative_prompt, **PIPELINE_GENERATION_CONFIG).images[0]\n",
    "    generation_times.append((datetime.datetime.now() - start_time).total_seconds())\n",
    "    plt.imshow(np.asarray(image))\n",
    "    plt.show()\n",
    "\n",
    "total_runtime, mean_runtime, std_runtime = sum(generation_times), np.mean(generation_times), np.std(generation_times)\n",
    "print(f\"Runtimes (in seconds) - Total: {total_runtime:3.3f} - Mean {mean_runtime:3.3f} (std: {std_runtime:3.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Runtime performance\n",
    "Runtime performance is assessed both in terms of latency (P95 or other relevant quantile of the distribution of generation \n",
    "times) and throughput (number of generated images per second).\n",
    "\n",
    "Performance numbers must be communicated together with the performance impacting configurations. In the case of Stable \n",
    "Diffusion pipelines, these parameters include:\n",
    "* Number of denoising steps (`num_inference_steps`)\n",
    "* Number of images generated for each prompt (`num_images_per_prompt`)\n",
    "* Batch size\n",
    "* Data type (including type casting options of the Neuron Compiler, e.g. matrix multiplications cast to BF16)\n",
    "* Whether a safety checker model is included since it can technically be omitted\n",
    "* Whether classifier-free guidance (CFG) is enabled (`guidance_scale` value)\n",
    "\n",
    "More performance numbers for vision models on Inf2 are available on the [Inf2 Performance page of the Neuron docs](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/benchmarks/inf2/inf2-performance.html#vision-models-inference-performance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Disabling classifier-free guidance\n",
    "The compiled models can work with disabled classifier-free guidance (CFG), i.e. for `guidance_scale` values smaller or equal to `1.0`. Negative prompts should not be supplied since they will be ignored. In that case however, the application \n",
    "of data parallelism to the U-Net model looses part of its benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "image = pipe(\n",
    "    prompt=\"An erupting volcano, Claude Monet, impressionist\", \n",
    "    **{**PIPELINE_GENERATION_CONFIG, \"guidance_scale\": 0.0}\n",
    "    ).images[0]\n",
    "plt.imshow(np.asarray(image))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_CACHE_DIR = Path.home() / \".cache\" / \"huggingface\" / \"hub\"\n",
    "shutil.rmtree(HF_CACHE_DIR / \"--\".join([\"models\", *MODEL_ID.split(\"/\")]))\n",
    "shutil.rmtree(NEURON_COMPILER_WORKDIR)\n",
    "shutil.rmtree(NEURON_COMPILER_OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuron_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
