{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace PixArt Alpha (256x256, 512x512 square resolution) Inference on Trn1 / Inf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "This notebook demonstrates how to compile and run the HuggingFace PixArt Alpha (256x256 && 512x512 square resolution) models for accelerated inference on Neuron.\n",
    "\n",
    "This Jupyter notebook should be run on an Inf2 instance (`inf2.8xlarge` or larger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that this Jupyter notebook is running the Python kernel environment that was set up according to the [PyTorch Installation Guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/torch-neuronx.html#setup-torch-neuronx). You can select the kernel from the 'Kernel -> Change Kernel' option on the top of this Jupyter notebook page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install Dependencies**\n",
    "\n",
    "This tutorial requires the following pip packages to be installed:\n",
    "- `torch-neuronx`\n",
    "- `neuronx-cc`\n",
    "- `diffusers==0.29.2`\n",
    "- `sentencepiece==0.2.0`\n",
    "- `transformers==4.36.2`\n",
    "- `ml_dtypes`\n",
    "\n",
    "`torch-neuronx` and `neuronx-cc` will be installed when you configure your environment following the Inf2 setup guide. The remaining dependencies can be installed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TOKENIZERS_PARALLELISM=True #Supresses tokenizer warnings making errors easier to detect\n",
    "!pip install diffusers==0.29.2 transformers==4.36.2 sentencepiece==0.2.0 matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"NEURON_FUSE_SOFTMAX\"] = \"1\"\n",
    "os.environ[\"NEURON_CUSTOM_SILU\"] = \"1\"\n",
    "\n",
    "import copy\n",
    "import diffusers\n",
    "import math\n",
    "import numpy as npy\n",
    "import time\n",
    "import torch\n",
    "import torch_neuronx\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from diffusers import PixArtAlphaPipeline\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import image as mpimg\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "# Define datatype\n",
    "DTYPE = torch.bfloat16\n",
    "\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define utility classes and functions**\n",
    "\n",
    "The following section defines some utility classes and functions. In particular, we define a double-wrapper for the backbone transformer and another wrapper for the text encoder. These wrappers enable `torch_neuronx.trace` to trace the wrapped models for compilation with the Neuron compiler. In addition, the `neuron_scaled_dot_product_attention` utility function performs optimized attention score calculation and is used to replace the original `scaled_dot_product_attention` function via a monkey patch (see the next code block under \"Compile backbone and save\" for usage). Finally, a `f32Wrapper` module is defined and performs the norm operations in fp32 precision (see the last code block under for usage) for the vae decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers.models.t5.modeling_t5 import T5EncoderModel\n",
    "from diffusers import Transformer2DModel\n",
    "from diffusers.models.autoencoders.vae import Decoder\n",
    "\n",
    "import math\n",
    "\n",
    "def neuron_scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=None, is_causal=None):\n",
    "  orig_shape = None\n",
    "  if len(query.shape) == 4:\n",
    "    orig_shape = query.shape\n",
    "    def to3d(x):\n",
    "      return x.reshape(-1, x.shape[2], x.shape[3])\n",
    "    query, key, value = map(to3d, [query, key, value])\n",
    "  if query.size() == key.size():\n",
    "    attention_scores = torch.bmm(key, query.transpose(-1, -2)) * (\n",
    "      1 / math.sqrt(query.size(-1))\n",
    "    )\n",
    "    attention_probs = attention_scores.softmax(dim=1).permute(0, 2, 1)\n",
    "  else:\n",
    "    attention_scores = torch.bmm(query, key.transpose(-1, -2)) * (\n",
    "      1 / math.sqrt(query.size(-1))\n",
    "    )\n",
    "    attention_probs = attention_scores.softmax(dim=-1)\n",
    "  attn_out = torch.bmm(attention_probs, value)\n",
    "  if orig_shape:\n",
    "    attn_out = attn_out.reshape(\n",
    "      orig_shape[0], orig_shape[1], attn_out.shape[1], attn_out.shape[2]\n",
    "    )\n",
    "  return attn_out\n",
    "\n",
    "class TracingT5Wrapper(nn.Module):\n",
    "  def __init__(self, t: T5EncoderModel, seqlen: int):\n",
    "    super().__init__()\n",
    "    self.t = t\n",
    "    self.device = t.device\n",
    "    precomputed_bias = self.t.encoder.block[0].layer[0].SelfAttention.compute_bias(seqlen, seqlen)\n",
    "    self.t.encoder.block[0].layer[0].SelfAttention.compute_bias = lambda *args, **kwargs: precomputed_bias\n",
    "  def forward(self, text_input_ids, prompt_attention_mask):\n",
    "    return self.t(\n",
    "      text_input_ids, \n",
    "      attention_mask=prompt_attention_mask\n",
    "    )\n",
    "\n",
    "class InferenceTextEncoderWrapper(nn.Module):\n",
    "  def __init__(self, dtype, t: T5EncoderModel, seqlen: int):\n",
    "    super().__init__()\n",
    "    self.dtype = dtype\n",
    "    self.device = t.device\n",
    "    self.t = t\n",
    "  def forward(self, text_input_ids, attention_mask=None):\n",
    "    return [self.t(text_input_ids, attention_mask)['last_hidden_state'].to(self.dtype)]\n",
    "\n",
    "class TracingTransformerWrapper(nn.Module):\n",
    "  def __init__(self, transformer):\n",
    "    super().__init__()\n",
    "    self.transformer = transformer\n",
    "    self.config = transformer.config\n",
    "    self.dtype = transformer.dtype\n",
    "    self.device = transformer.device    \n",
    "  def forward(self, hidden_states=None, encoder_hidden_states=None, timestep=None, encoder_attention_mask=None, **kwargs):\n",
    "    return self.transformer(\n",
    "      hidden_states=hidden_states, \n",
    "      encoder_hidden_states=encoder_hidden_states, \n",
    "      timestep=timestep, \n",
    "      encoder_attention_mask=encoder_attention_mask,\n",
    "      added_cond_kwargs={\"resolution\": None, \"aspect_ratio\": None},\n",
    "      return_dict=False)\n",
    "\n",
    "class InferenceTransformerWrapper(nn.Module):\n",
    "  def __init__(self, transformer: Transformer2DModel):\n",
    "    super().__init__()\n",
    "    self.transformer = transformer\n",
    "    self.config = transformer.config\n",
    "    self.dtype = transformer.dtype\n",
    "    self.device = transformer.device\n",
    "  def forward(self, hidden_states, encoder_hidden_states=None, timestep=None, \n",
    "              encoder_attention_mask=None, added_cond_kwargs=None,\n",
    "              return_dict=False):\n",
    "    output = self.transformer(\n",
    "      hidden_states, \n",
    "      encoder_hidden_states, \n",
    "      timestep, \n",
    "      encoder_attention_mask)\n",
    "    return output\n",
    "\n",
    "class SimpleWrapper(nn.Module):\n",
    "  def __init__(self, model):\n",
    "    super().__init__()\n",
    "    self.model = model\n",
    "  def forward(self, x):\n",
    "    output = self.model(x)\n",
    "    return output\n",
    "\n",
    "class f32Wrapper(nn.Module):\n",
    "  def __init__(self, original):\n",
    "    super().__init__()\n",
    "    self.original = original\n",
    "  def forward(self, x):\n",
    "    t = x.dtype\n",
    "    y = x.to(torch.float32)\n",
    "    output = self.original(y)\n",
    "    return output.type(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile the model into an optimized TorchScript and save the TorchScript**\n",
    "\n",
    "In the following section, we will compile parts of the PixArt Alpha pipeline for execution on Neuron. Note that this only needs to be done once: After you have compiled and saved the model by running the following section of code, you can reuse it any number of times without having to recompile. In particular, we will compile:\n",
    "1. The T5 text encoder;\n",
    "2. The VAE decoder;\n",
    "3. The backbone transformer, and\n",
    "4. The VAE post_quant_conv\n",
    "These blocks are chosen because they represent the bulk of the compute in the pipeline, and performance benchmarking has shown that running them on Neuron yields significant performance benefit.\n",
    "\n",
    "Several points worth noting are:\n",
    "1. In order to save RAM (these compiles need lots of RAM!), before tracing each model, we make a deepcopy of the part of the pipeline (i.e. the transformer or the VAE decoder) that is to be traced, and then delete the pipeline object from memory with `del pipe`. This trick allows the compile to succeed on instance types with a smaller amount of RAM.\n",
    "2. When compiling each part of the pipeline, we need to pass `torch_neuronx.trace` sample input(s), When there are multiple inputs, they are passed together as a tuple. For details on how to use `torch_neuronx.trace`, please refer to our documentation here: https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/api-reference-guide/inference/api-torch-neuronx-trace.html\n",
    "3. Note that while compiling the backbone transformer, we make use of the double-wrapper structure defined above. In addition, we also use the optimized `neuron_scaled_dot_product_attention` function to replace the original `scaled_dot_product_attention` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdpa_original = torch.nn.functional.scaled_dot_product_attention\n",
    "def attention_wrapper(query, key, value, attn_mask=None, dropout_p=None, is_causal=None):\n",
    "  if attn_mask is not None:\n",
    "    return sdpa_original(query, key, value, attn_mask=attn_mask, dropout_p=dropout_p, is_causal=is_causal)\n",
    "  else:\n",
    "    return neuron_scaled_dot_product_attention(query, key, value, attn_mask=attn_mask, dropout_p=dropout_p, is_causal=is_causal)\n",
    "\n",
    "torch.nn.functional.scaled_dot_product_attention = attention_wrapper\n",
    "\n",
    "def get_pipe(resolution, dtype):\n",
    "  if resolution == 256:\n",
    "    transformer: Transformer2DModel = Transformer2DModel.from_pretrained(\n",
    "      \"PixArt-alpha/PixArt-XL-2-256x256\", \n",
    "      subfolder=\"transformer\", \n",
    "      torch_dtype=dtype)\n",
    "    return PixArtAlphaPipeline.from_pretrained(\n",
    "      \"PixArt-alpha/PixArt-XL-2-512x512\", \n",
    "      transformer=transformer, \n",
    "      torch_dtype=dtype)\n",
    "  elif resolution == 512:\n",
    "    return PixArtAlphaPipeline.from_pretrained(\n",
    "      \"PixArt-alpha/PixArt-XL-2-512x512\", \n",
    "      torch_dtype=dtype)\n",
    "  else:\n",
    "    raise Exception(f\"Unsupport resolution {resolution} for pixart alpha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For saving compiler artifacts\n",
    "COMPILER_WORKDIR_ROOT = 'pixart_alpha_compile_dir'\n",
    "\n",
    "hidden_size = 4096\n",
    "seqlen = 120\n",
    "\n",
    "# Select the desired resolution.\n",
    "resolution = 256\n",
    "# resolution = 512\n",
    "\n",
    "height = resolution\n",
    "width = resolution\n",
    "\n",
    "height_latent_size = height // 8\n",
    "width_latent_size = width // 8\n",
    "\n",
    "torch.manual_seed(1)\n",
    "npy.random.seed(1)\n",
    "\n",
    "# --- Compile T5 text encoder and save ---\n",
    "pipe = get_pipe(resolution, DTYPE)\n",
    "text_encoder = copy.deepcopy(pipe.text_encoder)\n",
    "del pipe\n",
    "\n",
    "for block in text_encoder.encoder.block:\n",
    "  block.layer[1].DenseReluDense.act = torch.nn.GELU(approximate=\"tanh\")\n",
    "\n",
    "# Apply the wrapper to deal with custom return type\n",
    "text_encoder = TracingT5Wrapper(text_encoder, seqlen)\n",
    "sample_text_input_ids = torch.randint(low=0, high=18141, size=(1, seqlen))\n",
    "sample_prompt_attention_mask = torch.randint(low=0, high=2, size=(1, seqlen))\n",
    "sample_inputs = sample_text_input_ids, sample_prompt_attention_mask\n",
    "\n",
    "text_encoder_neuron = torch_neuronx.trace(\n",
    "  text_encoder,\n",
    "  sample_inputs,\n",
    "  compiler_workdir=os.path.join(COMPILER_WORKDIR_ROOT, 'text_encoder'),\n",
    "  compiler_args=[\"--enable-fast-loading-neuron-binaries\"])\n",
    "\n",
    "torch_neuronx.async_load(text_encoder_neuron)\n",
    "text_encoder_filename = os.path.join(COMPILER_WORKDIR_ROOT, 'text_encoder/model.pt')\n",
    "torch.jit.save(text_encoder_neuron, text_encoder_filename)\n",
    "\n",
    "del text_encoder\n",
    "del text_encoder_neuron\n",
    "\n",
    "# --- Compile the transformer backbone and save ---\n",
    "pipe = get_pipe(resolution, DTYPE)\n",
    "transformer = copy.deepcopy(pipe.transformer)\n",
    "del pipe\n",
    "\n",
    "transformer = TracingTransformerWrapper(transformer)\n",
    "sample_hidden_states = torch.rand([1, 4, height_latent_size, width_latent_size], dtype=DTYPE)\n",
    "sample_encoder_hidden_states = torch.rand([1, seqlen, hidden_size], dtype=DTYPE)\n",
    "sample_timestep = torch.ones((1,), dtype=torch.int64)\n",
    "sample_encoder_attention_mask = torch.randint(low=0, high=2, size=(1, seqlen), dtype=torch.int64)\n",
    "sample_inputs = sample_hidden_states, sample_encoder_hidden_states, sample_timestep, sample_encoder_attention_mask\n",
    "transformer_neuron = torch_neuronx.trace(\n",
    "  transformer,\n",
    "  sample_inputs,\n",
    "  compiler_workdir=os.path.join(COMPILER_WORKDIR_ROOT, 'transformer'),\n",
    "  compiler_args=[\"--model-type=transformer\", \"--enable-fast-loading-neuron-binaries\"]\n",
    ")\n",
    "\n",
    "# Enable asynchronous and lazy loading to speed up model load\n",
    "torch_neuronx.async_load(transformer_neuron)\n",
    "torch_neuronx.lazy_load(transformer_neuron)\n",
    "\n",
    "# save compiled transformer\n",
    "transformer_filename = os.path.join(COMPILER_WORKDIR_ROOT, 'transformer/model.pt')\n",
    "torch.jit.save(transformer_neuron, transformer_filename)\n",
    "\n",
    "del transformer\n",
    "del transformer_neuron\n",
    "\n",
    "# --- Compile the decoder and save ---\n",
    "pipe = get_pipe(resolution, DTYPE)\n",
    "decoder = copy.deepcopy(pipe.vae.decoder)\n",
    "post_quant_conv = copy.deepcopy(pipe.vae.post_quant_conv)\n",
    "del pipe\n",
    "\n",
    "for upblock in decoder.up_blocks:\n",
    "  for resnet in upblock.resnets:\n",
    "    orig_resnet_norm1 = resnet.norm1\n",
    "    orig_resnet_norm2 = resnet.norm2\n",
    "    resnet.norm1 = f32Wrapper(orig_resnet_norm1)\n",
    "    resnet.norm2 = f32Wrapper(orig_resnet_norm2)\n",
    "\n",
    "for resnet in decoder.mid_block.resnets:\n",
    "  orig_resnet_norm1 = resnet.norm1\n",
    "  orig_resnet_norm2 = resnet.norm2\n",
    "  resnet.norm1 = f32Wrapper(orig_resnet_norm1)\n",
    "  resnet.norm2 = f32Wrapper(orig_resnet_norm2)\n",
    "\n",
    "sample_inputs = torch.rand([1, 4, height_latent_size, width_latent_size], dtype=DTYPE)\n",
    "decoder_neuron = torch_neuronx.trace(\n",
    "  decoder, \n",
    "  sample_inputs, \n",
    "  compiler_workdir=os.path.join(COMPILER_WORKDIR_ROOT, 'vae_decoder'),\n",
    "  compiler_args=[\"--model-type=unet-inference\", \"--enable-fast-loading-neuron-binaries\"]\n",
    ")\n",
    "\n",
    "# Enable asynchronous loading to speed up model load\n",
    "torch_neuronx.async_load(decoder_neuron)\n",
    "\n",
    "# Save the compiled vae decoder\n",
    "decoder_filename = os.path.join(COMPILER_WORKDIR_ROOT, 'vae_decoder/model.pt')\n",
    "torch.jit.save(decoder_neuron, decoder_filename)\n",
    "\n",
    "# delete unused objects\n",
    "del decoder\n",
    "del decoder_neuron\n",
    "\n",
    "# --- Compile VAE post_quant_conv and save ---\n",
    "post_quant_conv_neuron = torch_neuronx.trace(\n",
    "  post_quant_conv, \n",
    "  sample_inputs,\n",
    "  compiler_workdir=os.path.join(COMPILER_WORKDIR_ROOT, 'vae_post_quant_conv'),\n",
    "  compiler_args=[\"--enable-fast-loading-neuron-binaries\"]\n",
    ")\n",
    "\n",
    "# Enable asynchronous loading to speed up model load\n",
    "torch_neuronx.async_load(post_quant_conv_neuron)\n",
    "\n",
    "# Save the compiled vae post_quant_conv\n",
    "post_quant_conv_filename = os.path.join(COMPILER_WORKDIR_ROOT, 'vae_post_quant_conv/model.pt')\n",
    "torch.jit.save(post_quant_conv_neuron, post_quant_conv_filename)\n",
    "\n",
    "# delete unused objects\n",
    "del post_quant_conv\n",
    "del post_quant_conv_neuron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the saved model and run it**\n",
    "\n",
    "Now that the model is compiled, you can reload it with any number of prompts. Note the use of the `torch_neuronx.DataParallel` API to load the backbone transformer onto two neuron cores for data-parallel inference. Currently the backbone transformer is the only part of the pipeline that runs data-parallel on two cores. All other parts of the pipeline runs on a single Neuron core.\n",
    "\n",
    "Edit the Prompts below to see what you can create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load all compiled models ---\n",
    "COMPILER_WORKDIR_ROOT = 'pixart_alpha_compile_dir'\n",
    "text_encoder_filename = os.path.join(COMPILER_WORKDIR_ROOT, 'text_encoder/model.pt')\n",
    "decoder_filename = os.path.join(COMPILER_WORKDIR_ROOT, 'vae_decoder/model.pt')\n",
    "transformer_filename = os.path.join(COMPILER_WORKDIR_ROOT, 'transformer/model.pt')\n",
    "post_quant_conv_filename = os.path.join(COMPILER_WORKDIR_ROOT, 'vae_post_quant_conv/model.pt')\n",
    "\n",
    "pipe = get_pipe(resolution, DTYPE)\n",
    "\n",
    "_neuronTextEncoder = InferenceTextEncoderWrapper(DTYPE, pipe.text_encoder, seqlen)\n",
    "_neuronTextEncoder.t = torch.jit.load(text_encoder_filename)\n",
    "pipe.text_encoder = _neuronTextEncoder\n",
    "assert pipe._execution_device is not None\n",
    "\n",
    "device_ids = [0, 1]\n",
    "_neuronTransformer = InferenceTransformerWrapper(pipe.transformer)\n",
    "_neuronTransformer.transformer = torch_neuronx.DataParallel(torch.jit.load(transformer_filename), device_ids, set_dynamic_batching=False)\n",
    "pipe.transformer = _neuronTransformer\n",
    "\n",
    "pipe.vae.decoder = SimpleWrapper(torch.jit.load(decoder_filename))\n",
    "pipe.vae.post_quant_conv = SimpleWrapper(torch.jit.load(post_quant_conv_filename))\n",
    "\n",
    "\n",
    "# Run pipeline\n",
    "prompt = [\n",
    "  \"a photo of an astronaut riding a horse on mars\",\n",
    "  \"sonic on the moon\",\n",
    "  \"elvis playing guitar while eating a hotdog\",\n",
    "  \"saved by the bell\",\n",
    "  \"engineers eating lunch at the opera\",\n",
    "  \"panda eating bamboo on a plane\",\n",
    "  \"A digital illustration of a steampunk flying machine in the sky with cogs and mechanisms, 4k, detailed, trending in artstation, fantasy vivid colors\",\n",
    "  \"kids playing soccer at the FIFA World Cup\"\n",
    "]\n",
    "\n",
    "# First do a warmup run so all the asynchronous loads can finish\n",
    "image_warmup = pipe(prompt[0]).images[0]\n",
    "\n",
    "plt.title(\"Image\")\n",
    "plt.xlabel(\"X pixel scaling\")\n",
    "plt.ylabel(\"Y pixels scaling\")\n",
    "\n",
    "total_time = 0\n",
    "for x in prompt:\n",
    "  start_time = time.time()\n",
    "  image = pipe(x).images[0]\n",
    "  total_time = total_time + (time.time()-start_time)\n",
    "  image.save(\"image.png\")\n",
    "  image = mpimg.imread(\"image.png\")\n",
    "  #clear_output(wait=True)\n",
    "  plt.imshow(image)\n",
    "  plt.show()\n",
    "print(\"Average time: \", npy.round((total_time/len(prompt)), 2), \"seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
