{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Pretrained Wav2Vec2-Conformer with Rotary Position Embeddings Inference on Inf2\n",
    "\n",
    "## Introduction\n",
    "This notebook demonstrates how to compile and run a HuggingFace ðŸ¤— Wav2Vec2-Conformer model with rotary position embeddings for accelerated inference on Neuron. This notebook will use the facebook/wav2vec2-conformer-rope-large-960h-ft model. \n",
    "\n",
    "This Jupyter notebook should be run on an Inf2 or Trn1 instance, of size Inf2.8xlarge or Trn1.2xlarge or larger.\n",
    "\n",
    "Note: for deployment, it is recommended to pre-compile the model on a compute instance using torch_neuronx.trace(), save the compiled model as a .pt file, and then distribute the .pt to Inf2.8xlarge instances for inference.\n",
    "\n",
    "Verify that this Jupyter notebook is running the Python kernel environment that was set up according to the PyTorch Installation Guide. You can select the kernel from the 'Kernel -> Change Kernel' option on the top of this Jupyter notebook page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies\n",
    "This tutorial requires the following pip packages:\n",
    "```\n",
    "torch-neuronx\n",
    "neuronx-cc\n",
    "transformers\n",
    "```\n",
    "Most of these packages will be installed when configuring your environment using the Trn1 setup guide. The additional dependencies must be installed here:\n",
    "\n",
    "```\n",
    "!pip install -U transformers datasets librosa\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the model into an AWS Neuron optimized TorchScript\n",
    "In the following section, we load the model, and input preprocessor, get a sample input, run inference on CPU, compile the model for Neuron using torch_neuronx.trace(), and save the optimized model as TorchScript.\n",
    "\n",
    "torch_neuronx.trace() expects a tensor or tuple of tensor inputs to use for tracing, so we unpack the input preprocessor's output. Additionally, the input shape that's used during compilation must match the input shape that's used during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_neuronx\n",
    "from datasets import load_dataset\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ConformerForCTC\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\")\n",
    "model = Wav2Vec2ConformerForCTC.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\")\n",
    "model.eval()\n",
    "\n",
    "# Take the first entry in the dataset as our input\n",
    "ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\", trust_remote_code=True)\n",
    "input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\", sampling_rate=16_000).input_values\n",
    "\n",
    "# RÃŸetrieve the result from cpu and decode to human-readable transcript \n",
    "output_cpu = model(input_values)\n",
    "def decode_to_transcript(logits):\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    return processor.batch_decode(predicted_ids)\n",
    "transcription_cpu = decode_to_transcript(output_cpu.logits)\n",
    "\n",
    "# Compile the model\n",
    "model_neuron = torch_neuronx.trace(model, input_values, compiler_args=\"--model-type=transformer\")\n",
    "\n",
    "# Save the TorchScript for inference deployment\n",
    "filename = 'model.pt'\n",
    "torch.jit.save(model_neuron, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run inference and compare results\n",
    "In this section we load the compiled model, run inference on Neuron, and compare the CPU and Neuron outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TorchScript compiled model\n",
    "model_neuron = torch.jit.load(filename)\n",
    "\n",
    "# Run inference using the Neuron model\n",
    "output_neuron = model_neuron(input_values)\n",
    "transcription_neuron = decode_to_transcript(output_neuron[\"logits\"])\n",
    "\n",
    "# Compare the results\n",
    "print(f\"CPU transcription:    {transcription_cpu}\")\n",
    "print(f\"Neuron transcription: {transcription_neuron}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
