{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "7e9a2b5b",
            "metadata": {},
            "source": [
                "# HuggingFace Pretrained ViT Inference on Trn1"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "2f4b19d5",
            "metadata": {},
            "source": [
                "## Introduction\n",
                "\n",
                "This notebook demonstrates how to compile and run a HuggingFace \ud83e\udd17 Google Vision Transformer (ViT) model for accelerated inference on Neuron. This notebook will use the [`google/vit-base-patch16-224`](https://huggingface.co/google/vit-base-patch16-224) model, which is primarily used for arbitrary image classification tasks.\n",
                "\n",
                "This Jupyter notebook should be run on a Inf2 or Trn1 instance (`inf2.xlarge` or `trn1.2xlarge` or larger)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Verify that this Jupyter notebook is running the Python kernel environment that was set up according to the [PyTorch Installation Guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/torch-neuronx.html#setup-torch-neuronx). You can select the kernel from the 'Kernel -> Change Kernel' option on the top of this Jupyter notebook page."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "23575b62",
            "metadata": {},
            "source": [
                "## Install Dependencies\n",
                "This tutorial requires the following pip packages:\n",
                "\n",
                "- `torch-neuronx`\n",
                "- `neuronx-cc`\n",
                "- `transformers`\n",
                "\n",
                "Most of these packages will be installed when configuring your environment using the Trn1 setup guide. The additional dependencies must be installed here:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4df33ddc",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -U transformers Pillow"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9dc8c2b1",
            "metadata": {},
            "source": [
                "## Compile the model into an AWS Neuron optimized TorchScript"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5ca0f1b2",
            "metadata": {},
            "source": [
                "In the following section, we load the model and feature extractor, get s sample input, run inference on CPU, compile the model for Neuron using `torch_neuronx.trace()` and save the optimized model as `TorchScript`.\n",
                "\n",
                "`torch_neuronx.trace()` expects a tensor or tuple of tensor inputs to use for tracing, so we unpack the feature extractor output. Additionally, the input shape that's used duing compilation must match the input shape that's used during inference. To handle this, we pad the inputs to the maximum size that we will see during inference."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7ce06089",
            "metadata": {},
            "outputs": [],
            "source": [
                "from PIL import Image\n",
                "import requests\n",
                "\n",
                "import torch\n",
                "import torch_neuronx\n",
                "from transformers import ViTImageProcessor, ViTForImageClassification\n",
                "\n",
                "# Create the feature extractor and model\n",
                "feature_extractor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
                "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', torchscript=True)\n",
                "model.eval()\n",
                "\n",
                "# Get an example input\n",
                "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
                "image = Image.open(requests.get(url, stream=True).raw)\n",
                "\n",
                "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
                "\n",
                "example = (inputs['pixel_values'],)\n",
                "\n",
                "# Run inference on CPU\n",
                "output_cpu = model(*example)\n",
                "\n",
                "# Compile the model\n",
                "model_neuron = torch_neuronx.trace(model, example)\n",
                "\n",
                "# Save the TorchScript for inference deployment\n",
                "filename = 'model.pt'\n",
                "torch.jit.save(model_neuron, filename)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "509f20ef",
            "metadata": {},
            "source": [
                "## Run inference and compare results\n",
                "\n",
                "In this section we load the compiled model, run inference on Neuron, and compare the CPU and Neuron outputs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c1832297",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the TorchScript compiled model\n",
                "model_neuron = torch.jit.load(filename)\n",
                "\n",
                "# Run inference using the Neuron model\n",
                "output_neuron = model_neuron(*example)\n",
                "\n",
                "# Compare the results\n",
                "print(f\"CPU tensor:            {output_cpu[0][0][0:10]}\")\n",
                "print(f\"Neuron tensor:         {output_neuron[0][0][0:10]}\")\n",
                "print(f\"CPU classification:    {model.config.id2label[output_cpu[0].argmax(-1).item()]}\")\n",
                "print(f\"Neuron classification: {model.config.id2label[output_neuron[0].argmax(-1).item()]}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (Neuron PyTorch)",
            "language": "python",
            "name": "pytorch_venv"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
