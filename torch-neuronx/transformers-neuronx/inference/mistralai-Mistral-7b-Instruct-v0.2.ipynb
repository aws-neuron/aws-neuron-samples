{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b73d8dd8-921b-4939-b2d9-2cc814b71384",
   "metadata": {},
   "source": [
    "# Run Hugging Face mistralai/Mistral-7B-Instruct-v0.2 autoregressive sampling on Inf2 & Trn1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c36be3e-473b-457b-b994-0f5dc7fe4e62",
   "metadata": {},
   "source": [
    "In this example we compile and deploy the Hugging Face [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) model for tensor parallel inference on Neuron using the `transformers-neuronx` package.\n",
    "\n",
    "The example has the following main sections:\n",
    "1. Set up the Jupyter Notebook\n",
    "1. Install dependencies\n",
    "1. Load the model\n",
    "1. Perform autoregressive sampling using tensor parallelism\n",
    "\n",
    "This Jupyter Notebook should be run on an Inf2 instance (`inf2.48xlarge`). To run on a larger Trn1 instance (`trn1.32xlarge`) will require changing the `tp_degree` specified in compilation section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6827638f-2c02-4c22-8b47-d827f7f7ae44",
   "metadata": {},
   "source": [
    "## Set up the Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e16db5-98ee-446b-aac3-e928642b355e",
   "metadata": {},
   "source": [
    "The following steps set up Jupyter Notebook and launch this tutorial:\n",
    "1. Clone the [AWS Neuron Samples](https://github.com/aws-neuron/aws-neuron-samples) repo to your instance using\n",
    "```\n",
    "git clone https://github.com/aws-neuron/aws-neuron-samples.git\n",
    "```\n",
    "2. Navigate to the `transformers-neuronx` inference samples folder\n",
    "```\n",
    "cd aws-neuron-samples/torch-neuronx/transformers-neuronx/inference\n",
    "```\n",
    "3. Follow the instructions in [Jupyter Notebook QuickStart](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html) to run Jupyter Notebook on your instance.\n",
    "4. Locate this tutorial in your Jupyter Notebook session (`mistralai-Mistral-7b-Instruct-v0.2.ipynb`) and launch it. Follow the rest of the instructions in this tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4fa60a-de2e-4587-a13b-6369b62a56a9",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "This tutorial requires the following pip packages:\n",
    "\n",
    " - `torch-neuronx`\n",
    " - `neuronx-cc`\n",
    " - `sentencepiece`\n",
    " - `transformers`\n",
    " - `transformers-neuronx`\n",
    "\n",
    "\n",
    "Most of these packages will be installed when configuring your environment using the [torch-neuronx inference setup guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/torch-neuronx.html#setup-torch-neuronx). The additional dependencies must be installed here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f3e6d-6e3d-45bc-a970-b9001a88fecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers-neuronx sentencepiece -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ad0970-6456-4313-a469-9d9dcd721c5c",
   "metadata": {},
   "source": [
    "# Load the model\n",
    "\n",
    "The memory required to host any model can be computed with:\n",
    "```\n",
    "total memory = bytes per parameter * number of parameters\n",
    "```\n",
    "When using `float16` casted weights for a 7 billion parameter model, this works out to `2 * 7B` or ~14GB of weights. In theory, this means it is possible to fit this model on a single NeuronCore (16GB capacity). In this example, we will show splitting the compute across 8 NeuronCores.\n",
    "\n",
    "Increasing the `tp_degree` beyond the minimum requirement for a model almost always results in a faster model. Increasing the tensor parallelism degree increases both available compute power and memory bandwidth which improve model performance. To minimize model latency, it is recommended to use the highest tensor parallelism degree that is supported by the instance.\n",
    "\n",
    "In the following code, we will use the `NeuronAutoModelForCausalLM` class to automatically load a checkpoint directly from the huggingface hub. The default model config supports sampling up to sequence length 2048. Tensor parallelism is enabled through the argument `tp_degree=8`. We enable `bfloat16` casting with the `amp='bf16'` flag. The model computational graph is compiled by `neuronx-cc` for optimized inference on Neuron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc75623-cf79-4003-ab79-e314902b593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers_neuronx import NeuronAutoModelForCausalLM\n",
    "\n",
    "name = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "\n",
    "model = NeuronAutoModelForCausalLM.from_pretrained(\n",
    "    name,           # The reference to the huggingface model\n",
    "    tp_degree=8,    # The Number of NeuronCores to shard the model across. Using 8 means 3 replicas can be used on a inf2.48xlarge\n",
    "    amp='bf16',     # Ensure the model weights/compute are bfloat16 for faster compute\n",
    ")\n",
    "model.to_neuron()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15969ced-cb05-47db-87bc-7a8d19131573",
   "metadata": {},
   "source": [
    "# Perform autoregressive sampling using tensor parallelism\n",
    "\n",
    "In this code we demonstrate using the model to answer prompts and stream the output results token-by-token as they are produced. Here we use Top-K sampling to select tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a6db21-5e3a-42b6-aadd-af9ddfb73592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, TextStreamer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "prompt = \"[INST] What is your favourite condiment? [/INST]\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generated_sequences = model.sample(input_ids, sequence_length=2048, top_k=50, streamer=streamer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
