{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfbcb86d",
   "metadata": {},
   "source": [
    "# Run Hugging Face `EleutherAI/gpt-j-6B` autoregressive sampling on Inf2 & Trn1 with Data Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3490c713",
   "metadata": {},
   "source": [
    "To make the most of this tutorial and use (24 cores) in three processes, use an Inf2.48xlarge or trn1.32xlarge.\n",
    "If you are using Inf2.24xlarge, modify the last section to run only two processes (16 cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b32cf9",
   "metadata": {},
   "source": [
    "## Set up the Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5da6a7",
   "metadata": {},
   "source": [
    "The following steps set up Jupyter Notebook and launch this tutorial:\n",
    "1. Clone the [AWS Neuron Samples](https://github.com/aws-neuron/aws-neuron-samples) repo to your instance using\n",
    "```\n",
    "git clone https://github.com/aws-neuron/aws-neuron-samples.git\n",
    "```\n",
    "2. Navigate to the `transformers-neuronx` inference samples folder\n",
    "```\n",
    "cd aws-neuron-samples/torch-neuronx/transformers-neuronx/inference\n",
    "```\n",
    "3. Follow the instructions in [Jupyter Notebook QuickStart](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html) to run Jupyter Notebook on your instance.\n",
    "4. Locate this tutorial in your Jupyter Notebook session (`gptj-6b-sampling.ipynb`) and launch it. Follow the rest of the instructions in this tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805bd56",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de11e14",
   "metadata": {},
   "source": [
    "This tutorial requires the following pip packages:\n",
    "\n",
    " - `torch-neuronx`\n",
    " - `neuronx-cc`\n",
    " - `transformers`\n",
    " - `transformers-neuronx`\n",
    "\n",
    "Most of these packages will be installed when configuring your environment using the [torch-neuronx inference setup guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/setup/setup-inference.html). The additional dependencies must be installed here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69d635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/aws-neuron/transformers-neuronx.git transformers -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a032b339",
   "metadata": {},
   "source": [
    "## Download and construct the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a93feb",
   "metadata": {},
   "source": [
    "We download and construct the `EleutherAI/gpt-j-6B` model using the Hugging Face `from_pretrained` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab87fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.auto import AutoModelForCausalLM\n",
    "\n",
    "hf_model = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-j-6B', low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a37ba2",
   "metadata": {},
   "source": [
    "## Split the model state_dict into multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c94e351",
   "metadata": {},
   "source": [
    "For the sake of reducing host memory usage, it is recommended to save the model `state_dict` as\n",
    "multiple files, as opposed to one monolithic file given by `torch.save`. This \"split-format\"\n",
    "`state_dict` can be created using the `save_pretrained_split` function. With this checkpoint format,\n",
    "the Neuron model loader can load parameters to the Neuron device high-bandwidth memory (HBM) directly\n",
    "by keeping at most one layer of model parameters in the CPU main memory.\n",
    "\n",
    "To reduce memory usage during compilation and deployment, we cast the attention and mlp to `float16` precision before saving them. We keep the layernorms in `float32`. To do this, we implement a callback function that casts each layer in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13d8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers_neuronx.module import save_pretrained_split\n",
    "\n",
    "def amp_callback(model, dtype):\n",
    "    # cast attention and mlp to low precisions only; layernorms stay as f32\n",
    "    for block in model.transformer.h:\n",
    "        block.attn.to(dtype)\n",
    "        block.mlp.to(dtype)\n",
    "    model.lm_head.to(dtype)\n",
    "\n",
    "amp_callback(hf_model, torch.float16)\n",
    "save_pretrained_split(hf_model, './gptj-6b-split')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee60f67a",
   "metadata": {},
   "source": [
    "Utilizing more cores is possible by running multiple processes (Data Parallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f4ef82",
   "metadata": {},
   "source": [
    "# Data Parallel Optimization for Throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b974d5b",
   "metadata": {},
   "source": [
    "This is an example to show case that it is possible to run the same program in multiple processes. For example running 2 or 3 proceeses with 8 cores each utiizes 24 cores instead of previously only 16 cores. This is useful to increase throughput. This code below runs a batch size of 64. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c13db08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_infer():\n",
    "    # load model to NeuronCores with 8-way tensor parallel and DP\n",
    "    load_compile_time = time.time()\n",
    "    neuron_model = GPTJForSampling.from_pretrained('./gptj-6b-split', n_positions=1024, batch_size=64, tp_degree=8, amp='f16')\n",
    "    neuron_model.to_neuron()\n",
    "    load_compile_elapsed = time.time() - load_compile_time\n",
    "    print(f'Model load & compile time in a single process  {load_compile_elapsed} seconds')\n",
    "\n",
    "    # construct a tokenizer and encode prompt text\n",
    "    tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-j-6B')\n",
    "\n",
    "    batch_prompts = [\n",
    "        \"I am specialized at sentence generation language models,\", \n",
    "    ]\n",
    "    batch_prompts = batch_prompts * 64\n",
    "\n",
    "    input_ids = torch.as_tensor([tokenizer.encode(text) for text in batch_prompts])\n",
    "\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # warmup\n",
    "        generated_sequences = neuron_model.sample(input_ids, sequence_length=1024)\n",
    "        \n",
    "        start = time.time()\n",
    "        for i in range(2):\n",
    "            generated_sequences = neuron_model.sample(input_ids, sequence_length=1024)\n",
    "        elapsed = (time.time() - start) / 2\n",
    "\n",
    "        generated_sequences = [tokenizer.decode(seq) for seq in generated_sequences]\n",
    "    print(f'Averaged Latency for one inference {elapsed} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a123f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Pool\n",
    "# If runtime is busy, shutdown any other running notebook and retry again.\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers_neuronx.gptj.model import GPTJForSampling\n",
    "from multiprocessing import Process\n",
    "if __name__ == '__main__':\n",
    "    os.environ['NEURON_RT_NUM_CORES']='8'\n",
    "    total_start = time.time()\n",
    "    p1 = Process(target=load_model_infer)\n",
    "    p2 = Process(target=load_model_infer)\n",
    "    p3 = Process(target=load_model_infer)\n",
    "    p1.start()\n",
    "    p2.start()\n",
    "    p3.start()\n",
    "    p1.join()\n",
    "    p2.join()\n",
    "    p3.join()\n",
    "    total_elapsed = time.time() - total_start\n",
    "    print(f'total processes time including compilation finished in {total_elapsed} seconds')\n",
    "    print(f'TPS {(30/total_elapsed)*64} ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
