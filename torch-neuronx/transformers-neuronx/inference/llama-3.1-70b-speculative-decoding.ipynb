{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "645e111e-a0e0-4ec3-aa50-38e395de9682",
   "metadata": {},
   "source": [
    "# Run Hugging Face `Llama-3.1-70B-Instruct` + `Llama-3.2-1B-Instruct` Speculative Decoding on Trn1 with `transformers-neuronx` and `vLLM`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6440d53-f525-488f-bd83-e7dc2bc82218",
   "metadata": {},
   "source": [
    "In this tutorial we use [transformers-neuronx](https://github.com/aws-neuron/transformers-neuronx) and the [vLLM](https://docs.vllm.ai/en/latest/) serving framework to compile and deploy two instruction-tuned [Llama models](https://www.llama.com/) for inference in a speculative decoding configuration.\n",
    "\n",
    "Speculative decoding is a token generation optimization technique that uses a small draft model to generate `K` tokens autoregressively and a larger target model to determine which draft tokens to accept, all in a combined forward pass. For more information on speculative decoding, please see:\n",
    "- Leviathan, Yaniv, Matan Kalman, and Yossi Matias. [\"Fast inference from transformers via speculative decoding.\"](https://arxiv.org/abs/2211.17192) International Conference on Machine Learning. PMLR, 2023.\n",
    "- Chen, Charlie, et al. [\"Accelerating large language model decoding with speculative sampling.\"](https://arxiv.org/pdf/2302.01318) arXiv preprint arXiv:2302.01318 (2023).\n",
    "\n",
    "In this exercise, we use the following models:\n",
    "\n",
    "- **Target Model**: [meta-llama/Meta-Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct)\n",
    "- **Draft Model**: [meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)\n",
    "\n",
    "This tutorial proceeds in the following main sections:\n",
    "\n",
    "1. Set up the Jupyter Notebook.\n",
    "2. Install dependencies.\n",
    "3. Access and download the target and draft models.\n",
    "4. Perform speculative decoding inference using `transformers-neuronx` and `vLLM`.\n",
    "\n",
    "This notebook is intended for a Trn1 `trn1.32xlarge` instance.\n",
    "\n",
    "*Note: The models in this tutorial require 315 GB total disk space - Please ensure that your instance has sufficient storage to download and store Llama-3.1-70B-Instruct and Llama-3.2-1B-Instruct before proceeding.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff950e3e-c570-4d4f-9cd5-4907555255b4",
   "metadata": {},
   "source": [
    "## Set Up the Jupyter Notebook\n",
    "\n",
    "The following steps set up Jupyter Notebook and launch this tutorial:\n",
    "1. Clone the [AWS Neuron Samples](https://github.com/aws-neuron/aws-neuron-samples) repo to your instance using:\n",
    "```\n",
    "git clone https://github.com/aws-neuron/aws-neuron-samples.git\n",
    "```\n",
    "2. Navigate to the `transformers-neuronx` inference samples folder:\n",
    "```\n",
    "cd aws-neuron-samples/torch-neuronx/transformers-neuronx/inference\n",
    "```\n",
    "3. Follow the instructions in [Jupyter Notebook QuickStart](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html) to run Jupyter Notebook on your instance.\n",
    "4. Locate this tutorial in your Jupyter Notebook session (`llama-3.1-70b-speculative-decoding.ipynb`) and launch it. Follow the rest of the instructions in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ec74b1-f863-41ac-9cb9-73914b1c5b7c",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "This tutorial requires the following `pip` packages:\n",
    "- `torch-neuronx`\n",
    "- `neuronx-cc`\n",
    "- `sentencepiece`\n",
    "- `transformers`\n",
    "- `transformers-neuronx`\n",
    "\n",
    "Most of these packages will be installed when configuring your environment using the [torch-neuronx Inference Setup Guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/torch-neuronx.html#setup-torch-neuronx). `transformers-neuronx` and additional dependencies can be installed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3cbff3-c4b2-4172-9aae-8e5a0bd0009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers-neuronx sentencepiece \"transformers>=4.43.3\" # Recent transformers version required for RoPE scaling in Llama 3.1/3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3ede7d-a25e-4b58-94bd-2e51ef17a4fd",
   "metadata": {},
   "source": [
    "### Installing vLLM\n",
    "Neuron maintains a fork of vLLM (v0.6.2) that contains the necessary changes to support inference with `transformers-neuronx`. Neuron is working with the vLLM community to upstream these changes to make them available in a future version.\n",
    "\n",
    "*Important: Please follow the vLLM installation instructions below. Do not install vLLM from PyPI or the official vLLM repository.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e36b616-7435-4cc7-8f5e-333d1b1a3066",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone -b v0.6.x-neuron https://github.com/aws-neuron/upstreaming-to-vllm.git\n",
    "!cd upstreaming-to-vllm && pip install -r requirements-neuron.txt\n",
    "!VLLM_TARGET_DEVICE=\"neuron\" cd upstreaming-to-vllm && pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1f3d20-399f-443b-98fc-ed8353c03726",
   "metadata": {},
   "source": [
    "## Access and Download the Target and Draft Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c422d905-2402-48a6-b99b-9055a289a44b",
   "metadata": {},
   "source": [
    "The Meta-Llama-3.1-70B-Instruct and Llama-3.2-1B-Instruct models must be downloaded prior to running this tutorial. \n",
    "\n",
    "**Meta-Llama-3.1-70B-Instruct:** Use of the Meta-Llama-3.1-70B-Instruct model is governed by the Llama 3.1 Community License Agreement. Please follow the steps described in [meta-llama/Meta-Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct) to gain access to this model.\n",
    "\n",
    "**Llama-3.2-1B-Instruct:** Use of the Llama-3.2-1B-Instruct model is governed by the Llama 3.2 Community License Agreement. Please follow the steps described in [meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct) to gain access to this model.\n",
    "\n",
    "*Note:* For this tutorial, we assume you have access to the Hugging Face models above and that they are saved in the following directories:\n",
    "- `Meta-Llama-3.1-70B-Instruct`\n",
    "- `Llama-3.2-1B-Instruct`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8481fea6-23ad-46e2-92f3-be6c797fdc7f",
   "metadata": {},
   "source": [
    "## Perform Speculative Decoding Inference Using `transformers-neuronx` and `vLLM`\n",
    "\n",
    "In this tutorial, we use `transformers-neuronx` and `vLLM`'s `LLM` entrypoint to perform offline batched inference. We apply speculative decoding by passing both the target and draft model paths as arguments to the entrypoint. The number of draft tokens to generate is specified by `num_speculative_tokens`.\n",
    "\n",
    "### Speculative Decoding Overview\n",
    "The core intuition behind speculative decoding is that a simple model (the draft) can generate certain elements in a sequence with the same level of accuracy and significantly faster than a more complex model (the target). \n",
    "\n",
    "In the speculative decoding procedure, the draft model first produces `num_speculative_tokens` auto-regressively. The target model then produces logits for each generated draft token (Note that the target model can produce all logits in a single forward pass). We then iterate through the generated sequence's draft and target logits and perform a rejection sampling procedure to accept or reject draft tokens in a manner that accords with the target model's per-token probability distribution (For more details on the rejection sampling procedure, please see Theorem 1 in [(Chen et al., 2023](https://arxiv.org/pdf/2302.01318))). When a draft token is rejected, the target distribution is sampled instead. If all draft tokens are accepted, a final token is sampled from the target model. Because of this, `[1, num_speculative_tokens + 1]` tokens are guaranteed to be sampled in a single iteration of speculative decoding and the sampled tokens are selected in accordance with the target distribution.\n",
    "\n",
    "### Creating the `LLM` Entrypoint\n",
    "As a first step, we create the `vLLM` `LLM` entrypoint. Internally, this compiles the Neuron draft and target models and prepares them for use with `vLLM`'s continuous batching system (For more information, see Kwon, Woosuk, et al. [\"Efficient memory management for large language model serving with pagedattention.\"](https://arxiv.org/pdf/2309.06180) Proceedings of the 29th Symposium on Operating Systems Principles. 2023.). Neuron currently supports `vLLM` continuous batching with a block size equal to the model's maximum sequence length, so we set `block_size`, `max_model_len`, and `speculative_max_model_len` to the same value (1024 tokens in this tutorial). We configure speculative decoding to sample 4 draft tokens per iteration by setting `num_speculative_tokens=4`. The maximum number of sequences `vLLM` will process concurrently is also set to 4 with `max_num_seqs=4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8931db97-0dcc-4058-b72f-265b96aea263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e76e6a7-8236-4578-8cc5-51a91588af88",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model_path = \"Meta-Llama-3.1-70B-Instruct\"\n",
    "draft_model_path = \"Llama-3.2-1B-Instruct\"\n",
    "\n",
    "max_model_len=1024\n",
    "\n",
    "llm = LLM(\n",
    "    model=target_model_path,\n",
    "    speculative_model=draft_model_path,\n",
    "    block_size=max_model_len,\n",
    "    device=\"neuron\",\n",
    "    dtype=\"bfloat16\",\n",
    "    max_model_len=max_model_len,\n",
    "    max_num_seqs=4,\n",
    "    num_speculative_tokens=4,\n",
    "    speculative_max_model_len=max_model_len,\n",
    "    swap_space=0,\n",
    "    tensor_parallel_size=32,\n",
    "    use_v2_block_manager=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8571181-bafa-4693-8574-d93e3b6fe2e2",
   "metadata": {},
   "source": [
    "### Generate Prompts\n",
    "\n",
    "After this step, the models are ready to be used for batched inference with `vLLM`. We now assemble a collection of prompts. The target and draft model are instruction-tuned, so we apply the Llama 3.1 prompt template to each prompt. We also initialize our vLLM `SamplingParameters`. For this exercise, we use greedy sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4659a5-9f13-4f4f-a65e-4410b971692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather sample prompts for batched inference.\n",
    "prompts = [\n",
    "    \"Who are you?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"What is the future of AI?\",\n",
    "    \"What is Llama?\"\n",
    "]\n",
    "\n",
    "# Apply the Llama 3.1 prompt template to each prompt.\n",
    "# See https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/prompt_format.md\n",
    "llama_prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{0}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "prompts = [llama_prompt_template.format(prompt) for prompt in prompts]\n",
    "\n",
    "# Set sampling parameters.\n",
    "sampling_params = SamplingParams(temperature=0, top_p=1.0, top_k=1, max_tokens=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6834a0d-1899-45b5-a115-78112c29704b",
   "metadata": {},
   "source": [
    "### Perform Batched Inference\n",
    "\n",
    "Finally, we use the `LLM` entrypoint to perform batched inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb100d5-f948-422d-b99e-eb1a9a090e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform offline batched inference\n",
    "start = time.time()\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}\\nGenerated text: {generated_text!r}\")\n",
    "    print()\n",
    "print('-' * 40)\n",
    "print(f\"Inference Elapsed Time: {elapsed:.3f} seconds\")\n",
    "print('-' * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
