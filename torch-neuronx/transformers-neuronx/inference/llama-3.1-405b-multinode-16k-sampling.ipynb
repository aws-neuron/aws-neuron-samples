{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59913016-f89e-4a0e-9afe-b3a06e9112d5",
   "metadata": {},
   "source": [
    "# Run Hugging Face `Llama 3.1 405B` autoregressive sampling on Trn1/Trn1n with 16k sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8454655-ec27-45e3-8da7-f82b744321ee",
   "metadata": {},
   "source": [
    "In this example we compile and deploy the Hugging Face [meta-llama/Meta-Llama-3.1-405B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct) model for tensor parallel inference on Neuron using the `transformers-neuronx` package. We use a sequence length of 16k.\n",
    "\n",
    "The example has the following main sections:\n",
    "1. Set up the Jupyter Notebook\n",
    "2. Install dependencies\n",
    "3. Download the model\n",
    "4. Perform autoregressive sampling using tensor parallelism\n",
    "\n",
    "This Jupyter Notebook can be run on 4 Trn1/Trn1n instances (`trn1.32xlarge`/`trn1n.32xlarge`) using multinode inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2b7693-2950-41fc-a038-17cba44bf003",
   "metadata": {},
   "source": [
    "## Set up the Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47ef383-0dea-4423-8c38-29c73927fd78",
   "metadata": {},
   "source": [
    "The following steps set up Jupyter Notebook and launch this tutorial:\n",
    "1. Clone the [AWS Neuron Samples](https://github.com/aws-neuron/aws-neuron-samples) repo to your instance using\n",
    "```\n",
    "git clone https://github.com/aws-neuron/aws-neuron-samples.git\n",
    "```\n",
    "2. Navigate to the `transformers-neuronx` inference samples folder\n",
    "```\n",
    "cd aws-neuron-samples/torch-neuronx/transformers-neuronx/inference\n",
    "```\n",
    "3. Follow the instructions in [Jupyter Notebook QuickStart](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html) to run Jupyter Notebook on your instance.\n",
    "4. Locate this tutorial in your Jupyter Notebook session (`llama-3.1-405b-multinode-16k-sampling.ipynb`) and launch it after setting the environment variables described below. Follow the rest of the instructions in this tutorial. Note that the notebook needs to be run on all 4 nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a727963e-8178-4d2a-a5cd-a4f2bf00197e",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "This tutorial requires the following pip packages:\n",
    "\n",
    " - `torch-neuronx`\n",
    " - `neuronx-cc`\n",
    " - `sentencepiece`\n",
    " - `transformers`\n",
    " - `transformers-neuronx`\n",
    "\n",
    "\n",
    "Most of these packages will be installed when configuring your environment using the [torch-neuronx inference setup guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/torch-neuronx.html#setup-torch-neuronx). The additional dependencies must be installed here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4899b2-39b2-4309-b7df-48fe74b56eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers-neuronx sentencepiece \n",
    "!pip install transformers>=4.43.3 # need recent transformers version for RoPE scaling in Llama 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5459cfd",
   "metadata": {},
   "source": [
    "## Access the model\n",
    "\n",
    "Use of the Llama 3.1 model is governed by the Meta license and must be downloaded prior to running this sample. Follow the steps described in [meta-llama/Meta-Llama-3.1-405B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct) to get access to the Llama 3.1 model from Meta.\n",
    "\n",
    "Note: For the purpose of this sample, we assume you have access to the model from Hugging Face and it is saved in the directory `Meta-Llama-3.1-405B-Instruct`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14400e26-2058-44b0-b680-b1cee57203aa",
   "metadata": {},
   "source": [
    "## Perform autoregressive sampling using tensor parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e233a69-5658-4180-8f6c-91f377a01001",
   "metadata": {},
   "source": [
    "Now we have all of the necessary files for running `meta-llama/Meta-Llama-3.1-405B-Instruct` autoregressive sampling.\n",
    "\n",
    "The memory required to host any model can be computed with:\n",
    "```\n",
    "total memory = bytes per parameter * number of parameters\n",
    "```\n",
    "When using `bfloat16` weights for a 8 billion parameter model, this works out to `2 * 405B` or ~810GB of weights. Each NeuronCore has 16GB of memory which means that a 810GB model would not fit on a single NeuronCore. In reality, the total space required is often greater than just the number of parameters due to caching attention layer projections (KV caching). This caching mechanism grows memory allocations linearly with sequence length and batch size.\n",
    "\n",
    "To get very large language models to fit on Trn1, tensor parallelism is used to split weights, data, and compute across multiple NeuronCores. The number of NeuronCores that the weights are split across can be controlled by setting the `tp_degree` parameter. This parallelism degree must be chosen to ensure that the memory usage per NeuronCore will be less than the physical 16GB limit. When configuring tensor parallelism, the memory per NeuronCore can be computed with:\n",
    "\n",
    "```\n",
    "memory per core = (bytes per parameter * number of parameters) / tp_degree\n",
    "```\n",
    "\n",
    "This can be used to compute the minimum instance sizing by ensuring that the value selected for `tp_degree` results in less than 16GB allocated per NeuronCore.\n",
    "\n",
    "Note that increasing the `tp_degree` beyond the minimum requirement almost always results in a faster model. Increasing the tensor parallelism degree improves memory bandwidth which improves model performance. To optimize performance it's recommended to use the highest tensor parallelism degree that is supported by the instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468191a7",
   "metadata": {},
   "source": [
    "## Multinode tensor parallelism\n",
    "\n",
    "For the 405B model, even trn1.32xlarge is not sufficient to fit the model. Therefore we use multinode inference using 4 trn1.32xlarge nodes (or trn1n.32xlarge for better cross-node network bandwidth). In this case we will have `tp_degree` of 128 (4 times 32). You can find details about configuring multinode inference in [trn1 multi-node setup guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/setup-trn1-multi-node-execution.html) and [transformers-neuronx developer guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/transformers-neuronx/transformers-neuronx-developer-guide.html). In short, the following environment variables and configuration need to be set:\n",
    "\n",
    "### EFA related\n",
    "- `FI_EFA_USE_DEVICE_RDMA=1`\n",
    "- `FI_PROVIDER=efa`\n",
    "- `FI_EFA_FORK_SAFE=1` (only needed for older Linux kernel, see [here](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-runtime/nrt-troubleshoot.html#fi-efa-fork-safe))\n",
    "- `CCOM_SOCKET_IFNAME=eth0` (only for containerized environments)\n",
    "\n",
    "### Setting up communication and rank id\n",
    "- `NEURON_RT_ROOT_COMM_ID=10.1.201.64:63423` (this is the node address and port for the master node, where the port can be any free port)\n",
    "- `NEURON_RANK_ID=0` or `1` or `2` or `3` (the master node has rank id 0 while the other three will have 1, 2, 3. This is the only environment variable that needs to be set differently across the nodes).\n",
    "- `NEURON_LOCAL_TP=32` (set to 32 because each node is a trn1.32xlarge)\n",
    "\n",
    "### Deterministic sampling across the nodes\n",
    "- To make sure the inputs/outputs at each step are consistent across the nodes, we set `torch.manual_seed`.\n",
    "\n",
    "### How to execute notebook/script on multiple nodes?\n",
    "The same script/notebook needs to be run on all the nodes after setting the above environment variables. This can be done manually, through a SLURM batch job, or using a containerized solution (e.g., kubernetes nodes in an EKS cluster). Note that all the nodes need access to the model path (either on local disk or on shared storage)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12edbbd",
   "metadata": {},
   "source": [
    "\n",
    "### Configuration\n",
    "\n",
    "We will use the Neuron `LlamaForSampling` class to implement tensor parallelism for the Llama based model. We supply the `n_positions` and `context_length_estimate` to precompile various possible prompt lengths. Tensor parallelism is enabled through the argument `tp_degree=128`. The model computational graph is compiled by `neuronx-cc` for optimized inference on Neuron.\n",
    "\n",
    "\n",
    "We also set some additional configurations to improve the performance and/or support longer context:\n",
    "- `attention_layout`: Layout to be used for attention computation. In this case, we use \"BSH\".\n",
    "- `fuse_qkv`: Fuses the QKV projection into a single matrix multiplication. It helps in improving the loading efficiency of Q/K/V weights.\n",
    "- `group_query_attention`: The KV cache sharding strategy. For more details on this, please refer [Grouped Query Attention in transformers neuronx](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/transformers-neuronx/transformers-neuronx-developer-guide.html#grouped-query-attention-gqa-support-beta).\n",
    "- `sequence_parallel_norm`: Use sequence parallel sharding for RMSNorm. This helps reduce the time taken for the norm and also reduces the memory requirements for the intermediate tensors.\n",
    "- `shard_over_sequence`: Shard the KV cache along the sequence dimention to avoid replicating KV cache for GQA models. This helps reduce the memory requirements and time for loading KV cache at higher sequence lengths.\n",
    "- `context_unroll`: Setting context unroll factor to 1 compiles only single layer of the context encoding model (which is then executed multiple times). This avoids OOM issues and improves compile time with only minimal impact on performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea21739-a65e-4a5c-9a10-7f963a99a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the model\n",
    "\n",
    "from transformers_neuronx import LlamaForSampling, NeuronConfig, GQA\n",
    "import torch\n",
    "\n",
    "model_path = \"Meta-Llama-3.1-405B-Instruct\"\n",
    "\n",
    "# load Meta-Llama-3.1-405B-Instruct to the NeuronCores with 128-way tensor parallelism and run compilation\n",
    "# we pass n_positions and context_length_estimate buckets that allows us to get low context encoding/token generation \n",
    "# latency across sequence lengths upto 16k\n",
    "buckets = [2048, 4096, 8192, 16384]\n",
    "\n",
    "# set manual seed properly to ensure each node using same inputs per sampling iteration\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "neuron_config = NeuronConfig(\n",
    "                    attention_layout='BSH',\n",
    "                    fuse_qkv=True,\n",
    "                    group_query_attention=GQA.REPLICATED_HEADS,\n",
    "                    sequence_parallel_norm=True,\n",
    "                    shard_over_sequence=True,\n",
    "              )\n",
    "\n",
    "neuron_model = LlamaForSampling.from_pretrained(model_path, n_positions=buckets, neuron_config=neuron_config, \\\n",
    "                                                context_length_estimate=buckets, context_unroll=1, \\\n",
    "                                                batch_size=1, tp_degree=128, amp='bf16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42383f3b",
   "metadata": {},
   "source": [
    "Notice that buckets are used via `n_positions` and `context_length_estimate` to improve the latency. For more details about how to effectively use bucketing, please refer the [developer guide for bucketing](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/transformers-neuronx/transformers-neuronx-developer-guide.html?highlight=bucketing#bucketing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eced43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model on neuron cores and compile\n",
    "\n",
    "neuron_model.to_neuron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d72de18-9fa6-4707-9d3b-1b562803d30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform autoregressive sampling\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import requests\n",
    "\n",
    "# construct a tokenizer and encode prompt text\n",
    "# For the prompt we take a Python library, and ask the model to write some tests.\n",
    "# The input length is ~13k tokens.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "prompt = requests.get(\"https://raw.githubusercontent.com/huggingface/transformers/e55b33ceb4b0ba3c8c11f20b6e8d6ca4b48246d4/src/transformers/generation/configuration_utils.py\").text\n",
    "prompt += \"\\n\\n## ========================THE END======================\\n\"\n",
    "prompt += \"Write 4-5 tests for the above codebase.\"\n",
    "# put in prompt format https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/#prompt-format\n",
    "prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|> {prompt} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\") \n",
    "num_input_tokens = len(input_ids[0]) # ~13k tokens\n",
    "print(f\"num_input_tokens: {num_input_tokens}\")\n",
    "\n",
    "# run inference with top-k sampling\n",
    "with torch.inference_mode():\n",
    "    start = time.time()\n",
    "    generated_sequences = neuron_model.sample(input_ids, sequence_length=16384, top_k=10)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "# display the new generated tokens\n",
    "generated_sequences = [tokenizer.decode(seq[num_input_tokens:]) for seq in generated_sequences]\n",
    "print(f'generated sequence {generated_sequences[0]} in {elapsed} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ac4991-7606-4c2f-90af-230998b0de20",
   "metadata": {},
   "source": [
    "## Save and load the compiled model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6a4ba9-40fd-4544-81ab-9fd249f22e4d",
   "metadata": {},
   "source": [
    "The ```save``` and ```load``` functions can be used to save and load compiled model artifacts respectively. Loading compiled model artifacts from a provided directory will avoid model recompilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07176c29-b30b-4d16-8291-3bd0142dc42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_model.save('./neuron_artifacts') # can be copied and used on a different neuron instance\n",
    "del neuron_model\n",
    "\n",
    "neuron_model = LlamaForSampling.from_pretrained(model_path, n_positions=buckets, neuron_config=neuron_config, \\\n",
    "                                                context_length_estimate=buckets, context_unroll=1, \\\n",
    "                                                batch_size=1, tp_degree=128, amp='bf16')\n",
    "\n",
    "neuron_model.load('neuron_artifacts') # Load the compiled Neuron artifacts\n",
    "neuron_model.to_neuron() # will skip compile\n",
    "\n",
    "with torch.inference_mode():\n",
    "    start = time.time()\n",
    "    generated_sequences = neuron_model.sample(input_ids, sequence_length=16384, top_k=10)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "generated_sequences = [tokenizer.decode(seq[num_input_tokens:]) for seq in generated_sequences]\n",
    "print(f'generated sequence {generated_sequences[0]} in {elapsed} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_transformers_neuronx",
   "language": "python",
   "name": "aws_neuronx_venv_transformers_neuronx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
