{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "645e111e-a0e0-4ec3-aa50-38e395de9682",
   "metadata": {},
   "source": [
    "# Run Hugging Face `Llama-3.1-70B-Instruct` EAGLE Speculative Decoding on Trn1 with `transformers-neuronx` and `vLLM`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ad1f64-222f-415a-a17c-15b5c487ef2c",
   "metadata": {},
   "source": [
    "In this tutorial we use [transformers-neuronx](https://github.com/aws-neuron/transformers-neuronx) and the [vLLM](https://docs.vllm.ai/en/latest/) serving framework to compile and deploy an instruction-tuned [Llama model](https://www.llama.com/) and corresponding EAGLE draft model for inference in an EAGLE speculative decoding configuration.\n",
    "\n",
    "Speculative decoding is a token generation optimization technique that uses a small draft model to generate `K` tokens autoregressively and a larger target model to determine which draft tokens to accept, all in a combined forward pass. For more information on speculative decoding, please see:\n",
    "- Leviathan, Yaniv, Matan Kalman, and Yossi Matias. [\"Fast inference from transformers via speculative decoding.\"](https://arxiv.org/abs/2211.17192) International Conference on Machine Learning. PMLR, 2023.\n",
    "- Chen, Charlie, et al. [\"Accelerating large language model decoding with speculative sampling.\"](https://arxiv.org/pdf/2302.01318) arXiv preprint arXiv:2302.01318 (2023).\n",
    "\n",
    "EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency) extends this technique by:\n",
    "- Reducing sampling uncertainty by using  the next auto-regressively sampled token and a current feature map as draft model inputs.\n",
    "- Utilizing a specially trained EAGLE draft model that predicts feature outputs through an Autoregression Head and next token outputs through an LM Head.\n",
    "\n",
    "For more information on EAGLE speculative decoding, please see:\n",
    "- Li, Yuhui, et al. [\"Eagle: Speculative sampling requires rethinking feature uncertainty.\"](https://arxiv.org/pdf/2401.15077) arXiv preprint arXiv:2401.15077 (2024).  \n",
    "\n",
    "In this exercise, we use the following models:\n",
    "\n",
    "- **Target Model**: [meta-llama/Meta-Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct)\n",
    "- **Draft Model**: [Llama-3.1-70B-Instruct EAGLE draft](https://huggingface.co/yuhuili/EAGLE-LLaMA3-Instruct-70B)\n",
    "    - Please note that this draft model is not an official Meta Llama release. It is provided by the EAGLE authors as compatible with a `Meta-Llama-3.1-70B-Instruct` target.\n",
    "\n",
    "This tutorial proceeds in the following main sections:\n",
    "\n",
    "1. Set up the Jupyter Notebook.\n",
    "2. Install dependencies.\n",
    "3. Access and download the target and draft models.\n",
    "4. Perform EAGLE speculative decoding inference using `transformers-neuronx` and `vLLM`.\n",
    "\n",
    "This notebook is intended for a Trn1 `trn1.32xlarge` instance.\n",
    "\n",
    "*Note: The models in this tutorial require 322.2 GB total disk space - Please ensure that your instance has sufficient storage to download and store Meta-Llama-3.1-70B-Instruct and the draft model before proceeding.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff950e3e-c570-4d4f-9cd5-4907555255b4",
   "metadata": {},
   "source": [
    "## Set Up the Jupyter Notebook\n",
    "\n",
    "The following steps set up Jupyter Notebook and launch this tutorial:\n",
    "1. Clone the [AWS Neuron Samples](https://github.com/aws-neuron/aws-neuron-samples) repo to your instance using:\n",
    "```\n",
    "git clone https://github.com/aws-neuron/aws-neuron-samples.git\n",
    "```\n",
    "2. Navigate to the `transformers-neuronx` inference samples folder:\n",
    "```\n",
    "cd aws-neuron-samples/torch-neuronx/transformers-neuronx/inference\n",
    "```\n",
    "3. Follow the instructions in [Jupyter Notebook QuickStart](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html) to run Jupyter Notebook on your instance.\n",
    "4. Locate this tutorial in your Jupyter Notebook session (`llama-3.1-70b-eagle-speculative-decoding.ipynb`) and launch it. Follow the rest of the instructions in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ec74b1-f863-41ac-9cb9-73914b1c5b7c",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "This tutorial requires the following `pip` packages:\n",
    "- `torch-neuronx`\n",
    "- `neuronx-cc`\n",
    "- `sentencepiece`\n",
    "- `transformers`\n",
    "- `transformers-neuronx`\n",
    "\n",
    "Most of these packages will be installed when configuring your environment using the [torch-neuronx Inference Setup Guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/torch-neuronx.html#setup-torch-neuronx). `transformers-neuronx` and additional dependencies can be installed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3cbff3-c4b2-4172-9aae-8e5a0bd0009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers-neuronx sentencepiece \"transformers>=4.43.3\" # Recent transformers version required for RoPE scaling in Llama 3.1/3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3ede7d-a25e-4b58-94bd-2e51ef17a4fd",
   "metadata": {},
   "source": [
    "### Installing vLLM\n",
    "Neuron maintains a fork of vLLM (v0.6.2) that contains the necessary changes to support inference with `transformers-neuronx`. Neuron is working with the vLLM community to upstream these changes to make them available in a future version.\n",
    "\n",
    "*Important: Please follow the vLLM installation instructions below. Do not install vLLM from PyPI or the official vLLM repository.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e36b616-7435-4cc7-8f5e-333d1b1a3066",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone -b v0.6.x-neuron https://github.com/aws-neuron/upstreaming-to-vllm.git\n",
    "!cd upstreaming-to-vllm && pip install -r requirements-neuron.txt\n",
    "!VLLM_TARGET_DEVICE=\"neuron\" cd upstreaming-to-vllm && pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1f3d20-399f-443b-98fc-ed8353c03726",
   "metadata": {},
   "source": [
    "## Access and Download the Target and Draft Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c422d905-2402-48a6-b99b-9055a289a44b",
   "metadata": {},
   "source": [
    "Meta-Llama-3.1-70B-Instruct and the draft model must be downloaded prior to running this tutorial. \n",
    "\n",
    "**Meta-Llama-3.1-70B-Instruct:** Use of the Meta-Llama 3.1 70B-Instruct model is governed by the Llama 3.1 Community License Agreement. Please follow the steps described in [meta-llama/Meta-Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct) to gain access to this model.\n",
    "\n",
    "**Llama-3.1-70B-Instruct EAGLE Draft:** Use of the Llama-3.1-70B-Instruct EAGLE Draft is governed by the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). Download the model from [Hugging Face](https://huggingface.co/yuhuili/EAGLE-LLaMA3-Instruct-70B).\n",
    "\n",
    "*Note:* For this sample, we assume you have access to the Hugging Face models above and that they are saved in the following directories:\n",
    "- `Meta-Llama-3.1-70B-Instruct`\n",
    "- `Llama-3.1-70B-Instruct-EAGLE-Draft`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8481fea6-23ad-46e2-92f3-be6c797fdc7f",
   "metadata": {},
   "source": [
    "## Perform EAGLE Speculative Decoding Using `transformers-neuronx` and `vLLM`\n",
    "\n",
    "In this tutorial, we use `transformers-neuronx` and `vLLM`'s `LLM` entrypoint to perform offline batched inference. We apply EAGLE speculative decoding to this entrypoint by passing both the target and draft model paths in as arguments. The number of draft tokens to generate is specified by `num_speculative_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8931db97-0dcc-4058-b72f-265b96aea263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from safetensors import safe_open\n",
    "from safetensors.torch import save_file\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "target_model_path = \"Meta-Llama-3.1-70B-Instruct\"\n",
    "draft_model_path = \"Llama-3.1-70B-Instruct-EAGLE-Draft\"\n",
    "\n",
    "max_model_len=1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03288c29-f7fa-470f-89fe-947ed06d05a9",
   "metadata": {},
   "source": [
    "## Add the Target LM Head to the EAGLE Draft\n",
    "\n",
    "This EAGLE draft checkpoint expects the target model LM head to be used as its own LM head. To achieve this, we copy the target model LM head to the draft model using the cell below:\n",
    "\n",
    "**Important Note** *The following code cell overwrites the draft model Hugging Face `model.safetensors` file with a new `safetensors` file that additionally contains the target LM head.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c6e83a-637c-406d-88b9-f6c2c494bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "DRAFT_MODEL_SAFETENSORS_NAME = \"model.safetensors\"\n",
    "LM_HEAD_WEIGHT_TENSOR_NAME = \"lm_head.weight\"\n",
    "TARGET_MODEL_SAFETENSORS_INDEX_NAME = \"model.safetensors.index.json\"\n",
    "\n",
    "def find_lm_head_safetensors_location(model_dir):\n",
    "    model_index_location_path = os.path.join(model_dir, TARGET_MODEL_SAFETENSORS_INDEX_NAME)\n",
    "\n",
    "    with open(model_index_location_path, 'r') as f:\n",
    "        model_index_locations = json.load(f)\n",
    "\n",
    "    lm_head_safetensors_name = model_index_locations[\"weight_map\"][LM_HEAD_WEIGHT_TENSOR_NAME]\n",
    "\n",
    "    return lm_head_safetensors_name\n",
    "\n",
    "# Find the target model `lm_head.weight` location in safetensors\n",
    "target_lm_head_safetensors_name = find_lm_head_safetensors_location(target_model_path)\n",
    "target_lm_head_safetensors_path = os.path.join(target_model_path, target_lm_head_safetensors_name)\n",
    "\n",
    "# Open the target model .safetensor containing `lm_head.weight`\n",
    "with safe_open(target_lm_head_safetensors_path, framework=\"pt\") as f:\n",
    "    target_lm_head = f.get_tensor(LM_HEAD_WEIGHT_TENSOR_NAME)\n",
    "\n",
    "# Collect all tensors in the draft model\n",
    "draft_model_safetensors_path = os.path.join(draft_model_path, DRAFT_MODEL_SAFETENSORS_NAME)\n",
    "tensors = {}\n",
    "with safe_open(draft_model_safetensors_path, framework=\"pt\") as f:\n",
    "    for key in f.keys():\n",
    "        tensors[key] = f.get_tensor(key)\n",
    "\n",
    "# Add the LM head weights and save out the new draft model.safetensors file\n",
    "tensors[LM_HEAD_WEIGHT_TENSOR_NAME] = target_lm_head.type(torch.float16)\n",
    "save_file(tensors, draft_model_safetensors_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b3591f-9a8d-47b0-98af-1f80cf93b72a",
   "metadata": {},
   "source": [
    "### Update the Draft Hugging Face JSON Configuration\n",
    "\n",
    "The `vLLM` fork uses a Boolean flag `is_eagle=True` within the HuggingFace `config.json` to determine whether the draft model is an EAGLE draft. If this flag is not present within the draft model `config.json`, run the cell below to add it. *Note: Please do not alter the target model Hugging Face configuration JSON file.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e0091-1263-44b2-b508-f03922b18e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "draft_model_config_path = os.path.join(draft_model_path, \"config.json\")\n",
    "\n",
    "with open(draft_model_config_path, 'r') as f:\n",
    "    draft_config = json.load(f)\n",
    "\n",
    "draft_config[\"is_eagle\"] = True\n",
    "\n",
    "with open(draft_model_config_path, 'w') as f:\n",
    "    json.dump(draft_config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff179c5-0742-48da-9aa0-7f7c3963cf9a",
   "metadata": {},
   "source": [
    "### Creating the `LLM` Entrypoint\n",
    "As a next step, we create the `vLLM` `LLM` entrypoint. Internally, this compiles the Neuron draft and target models and prepares them for use with `vLLM`'s continuous batching system (For more information, see Kwon, Woosuk, et al. [\"Efficient memory management for large language model serving with pagedattention.\"](https://arxiv.org/pdf/2309.06180) Proceedings of the 29th Symposium on Operating Systems Principles. 2023.). Neuron currently supports `vLLM` continuous batching with a block size equal to the model's maximum sequence length, so we set `block_size`, `max_model_len`, and `speculative_max_model_len` to the same value (1024 tokens in this tutorial). We configure speculative decoding to sample 4 draft tokens per iteration by setting `num_speculative_tokens=4`. The maximum number of sequences `vLLM` will process concurrently is also set to 4 with `max_num_seqs=4`.\n",
    "\n",
    "If the draft model Hugging Face `config.json` file contains `is_eagle=True`, EAGLE speculative decoding will be applied within the entrypoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e76e6a7-8236-4578-8cc5-51a91588af88",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "    model=target_model_path,\n",
    "    speculative_model=draft_model_path,\n",
    "    block_size=max_model_len,\n",
    "    device=\"neuron\",\n",
    "    dtype=\"bfloat16\",\n",
    "    max_model_len=max_model_len,\n",
    "    max_num_seqs=4,\n",
    "    num_speculative_tokens=4,\n",
    "    speculative_max_model_len=max_model_len,\n",
    "    swap_space=0,\n",
    "    tensor_parallel_size=32,\n",
    "    use_v2_block_manager=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49959784-e05d-41e1-b960-a2880d7b2bd6",
   "metadata": {},
   "source": [
    "### Generate Prompts\n",
    "\n",
    "After this step, the target and draft models are ready to be used for batched inference with `vLLM`. We now assemble a collection of prompts. The target is instruction-tuned and the draft is trained through EAGLE to match target feature and token output distributions, so we apply the Llama 3.1 prompt template to each prompt. We also initialize our vLLM `SamplingParameters`. For this exercise, we will use greedy sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4659a5-9f13-4f4f-a65e-4410b971692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather sample prompts for batched inference.\n",
    "prompts = [\n",
    "    \"Who are you?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"What is the future of AI?\",\n",
    "    \"What is Llama?\"\n",
    "]\n",
    "\n",
    "# Apply the Llama 3.1 prompt template to each prompt.\n",
    "# See https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/prompt_format.md\n",
    "llama_prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{0}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "prompts = [llama_prompt_template.format(prompt) for prompt in prompts]\n",
    "\n",
    "# Set sampling parameters.\n",
    "sampling_params = SamplingParams(temperature=0, top_p=1.0, top_k=1, max_tokens=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fc61fe-f89c-4815-9656-28bdd4a9f7e3",
   "metadata": {},
   "source": [
    "### Perform Batched Inference\n",
    "\n",
    "Finally, we use the `LLM` entrypoint to perform batched inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb100d5-f948-422d-b99e-eb1a9a090e0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform offline batched inference\n",
    "start = time.time()\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}\\nGenerated text: {generated_text!r}\")\n",
    "    print()\n",
    "print('-' * 40)\n",
    "print(f\"Inference Elapsed Time: {elapsed:.3f} seconds\")\n",
    "print('-' * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
