{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "bfbcb86d",
            "metadata": {},
            "source": [
                "# Run Hugging Face `facebook/opt-13b` autoregressive sampling on Inf2 & Trn1"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "72bf3d85",
            "metadata": {},
            "source": [
                "In this example we compile and deploy the Hugging Face [facebook/opt-13b](https://huggingface.co/facebook/opt-13b) model for tensor parallel inference on Neuron using the `transformers-neuronx` package.\n",
                "\n",
                "The example has the following main sections:\n",
                "1. Set up the Jupyter Notebook\n",
                "1. Install dependencies\n",
                "1. Download and construct the model\n",
                "1. Split the model `state_dict` into multiple files\n",
                "1. Perform autoregressive sampling using tensor parallelism\n",
                "\n",
                "This Jupyter Notebook should be run on an Inf2 instance (`inf2.48xlarge` or larger) or a Trn1 instance (`trn1.32xlarge`)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Verify that this Jupyter notebook is running the Python kernel environment that was set up according to the [PyTorch Installation Guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/torch-neuronx.html#setup-torch-neuronx). You can select the kernel from the 'Kernel -> Change Kernel' option on the top of this Jupyter notebook page."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "48b32cf9",
            "metadata": {},
            "source": [
                "## Set up the Jupyter Notebook"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2b5da6a7",
            "metadata": {},
            "source": [
                "The following steps set up Jupyter Notebook and launch this tutorial:\n",
                "1. Clone the [AWS Neuron Samples](https://github.com/aws-neuron/aws-neuron-samples) repo to your instance using\n",
                "```\n",
                "git clone https://github.com/aws-neuron/aws-neuron-samples.git\n",
                "```\n",
                "2. Navigate to the `transformers-neuronx` inference samples folder\n",
                "```\n",
                "cd aws-neuron-samples/torch-neuronx/transformers-neuronx/inference\n",
                "```\n",
                "3. Follow the instructions in [Jupyter Notebook QuickStart](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html) to run Jupyter Notebook on your instance.\n",
                "4. Locate this tutorial in your Jupyter Notebook session (`facebook-opt-13b-sampling.ipynb`) and launch it. Follow the rest of the instructions in this tutorial. "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e805bd56",
            "metadata": {},
            "source": [
                "## Install Dependencies"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0de11e14",
            "metadata": {},
            "source": [
                "This tutorial requires the following pip packages:\n",
                "\n",
                " - `torch-neuronx`\n",
                " - `neuronx-cc`\n",
                " - `transformers`\n",
                " - `transformers-neuronx`\n",
                "\n",
                "Most of these packages will be installed when configuring your environment using the [torch-neuronx inference setup guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/torch-neuronx.html#setup-torch-neuronx). The additional dependencies must be installed here:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c69d635e",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install transformers-neuronx transformers -U"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a032b339",
            "metadata": {},
            "source": [
                "## Download and construct the model"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b9a93feb",
            "metadata": {},
            "source": [
                "We download and construct the `facebook/opt-13b` model using the Hugging Face `from_pretrained` method."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4ab87fb7",
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers.models.opt import OPTForCausalLM\n",
                "\n",
                "hf_model = OPTForCausalLM.from_pretrained('facebook/opt-13b', low_cpu_mem_usage=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "37a37ba2",
            "metadata": {},
            "source": [
                "## Split the model state_dict into multiple files"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0c94e351",
            "metadata": {},
            "source": [
                "For the sake of reducing host memory usage, it is recommended to save the model `state_dict` as\n",
                "multiple files, as opposed to one monolithic file given by `torch.save`. This \"split-format\"\n",
                "`state_dict` can be created using the `save_pretrained_split` function. With this checkpoint format,\n",
                "the Neuron model loader can load parameters to the Neuron device high-bandwidth memory (HBM) directly\n",
                "by keeping at most one layer of model parameters in the CPU main memory.\n",
                "\n",
                "To reduce memory usage during compilation and deployment, we cast the attention and mlp to `float16` precision before saving them. We keep the layernorms in `float32`. To do this, we implement a callback function that casts each layer in the model. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a13d8770",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers_neuronx.module import save_pretrained_split\n",
                "\n",
                "def amp_callback(model, dtype):\n",
                "    # cast attention and mlp to low precision only; layernorms stay as f32\n",
                "    for block in model.model.decoder.layers:\n",
                "        block.self_attn.to(dtype)\n",
                "        block.fc1.to(dtype)\n",
                "        block.fc2.to(dtype)\n",
                "    model.lm_head.to(dtype)\n",
                "\n",
                "amp_callback(hf_model, torch.float16)\n",
                "save_pretrained_split(hf_model, './opt-13b-split')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6ac11413",
            "metadata": {},
            "source": [
                "## Perform autoregressive sampling using tensor parallelism"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d097b88a",
            "metadata": {},
            "source": [
                "Now we have all of the necessary files for running `facebook/opt-13b` autoregressive sampling. \n",
                "\n",
                "To get a large language model working on Inf2 & Trn1, tensor parallelism is used to split weights and data across multiple NeuronCores. Each NeuronCore has 16GB of memory. As a rule of thumb, the total space required per NeuronCore will be at least `2 * number of model parameters` for a `float16` casted model. In reality, the total space required is often greater due to the key value cache, which grows with sequence lenght. This memory usage determines the minimum viable instance size since the amount of memory that will be allocated on one NeuronCore is directly proportional to the parallelism degree (`tp_degree`), or rather the number of physical NeuronCores per instance. The parallelism degree must be chosen to ensure that the memory usage per NeuronCore will be less than the physical 16GB limit. While this determines the minimum instance sizing, further decreasing the memory usage per NeuronCore by using a larger instance and a higher `tp_degree` should result in a faster model\n",
                "\n",
                "We will use the Neuron `OPTForSampling` class to implement tensor parallelism. The default model config supports sampling up to sequence length 2048, and we set batch size to 2. Tensor-parallelism is enabled through the argument\n",
                "`tp_degree=2`. Internally, the Neuron tensor manipulator will shard and duplicate tensors to multiple\n",
                "NeuronCores (2 in this case) to enable tensor-parallel computations on multiple NeuronCores. The model computational graph is compiled by neuronx-cc for optimized inference on Neuron."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "65d1682d",
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "import torch\n",
                "from transformers import AutoTokenizer\n",
                "from transformers_neuronx.opt.model import OPTForSampling\n",
                "\n",
                "# load facebook/opt-13b to NeuronCores with 2-way tensor parallel\n",
                "# enable float16 casting\n",
                "neuron_model = OPTForSampling.from_pretrained('./opt-13b-split', batch_size=1, tp_degree=8, amp='f16')\n",
                "neuron_model.to_neuron()\n",
                "\n",
                "# construct a tokenizer and encode prompt text\n",
                "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-13b')\n",
                "batch_prompts = [\n",
                "    \"Hello, I'm a language model,\",\n",
                "    \"Welcome to Amazon Elastic Compute Cloud,\",\n",
                "]\n",
                "input_ids = torch.as_tensor([tokenizer.encode(text) for text in batch_prompts])\n",
                "\n",
                "with torch.inference_mode():\n",
                "    start = time.time()\n",
                "    generated_sequences = neuron_model.sample(input_ids, sequence_length=2048)\n",
                "    elapsed = time.time() - start\n",
                "\n",
                "generated_sequences = [tokenizer.decode(seq) for seq in generated_sequences]\n",
                "print(f'generated sequences {generated_sequences} in {elapsed} seconds')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ee60f67a",
            "metadata": {},
            "source": [
                "Larger batch sizes won't fit into an `inf2.8xlarge` or instance. These instances have 32 GB of HBM, and `facebook/opt-13b` has ~26 GB of model parameters in `float16`. With batch size 3, after storing model parameters and key-value caches, there will be less than 1 GB of HBM left, which is not enough for storing code and temporary data generated during the sampling computation. \n",
                "\n",
                "To use larger batch sizes, please consider using an `inf2.48xlarge` or  `trn1.32xlarge`. You can also try using a larger tensor parallelism degree, such as 8, on an `inf2.48xlarge` or a `trn1.32xlarge`. The `facebook/opt-13b` number of attention heads is 40, so the tensor parallelism degree must be a divisor of 40 and be supported on the Inf2 or Trn1 instance."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (Neuron PyTorch)",
            "language": "python",
            "name": "pytorch_venv"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
