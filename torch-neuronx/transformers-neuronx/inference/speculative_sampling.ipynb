{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run speculative sampling on Meta Llama models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In speculative sampling, we use use a smaller draft model to speculate future tokens. These are then sent to the larger target model, which accepts/rejects these tokens.  \n",
    "\n",
    "For a more detailed understanding, please refer to the original paper by DeepMind titled [\"Accelerating Large Language Model Decoding with Speculative Sampling\"](https://arxiv.org/abs/2302.01318)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we perform speculative sampling using the Hugging Face [\"meta-llama/Llama-2-70b\"](https://huggingface.co/meta-llama/Llama-2-70b) model and Hugging Face [\"meta-llama/Llama-2-7b\"](https://huggingface.co/meta-llama/Llama-2-7b).\n",
    "Here, the 70b model is considered the target model and the 7b model is considered the draft model.\n",
    "\n",
    "The example has the following main sections:\n",
    "\n",
    "1. Set up the Jupyter Notebook\n",
    "2. Install dependencies\n",
    "3. Download and construct the model\n",
    "5. Split the model `state_dict` into multiple files\n",
    "6. Perform speculative sampling\n",
    "\n",
    "This Jupyter Notebook should be run on a Trn1 instance (`trn1.32xlarge`). To run on an Inf2 instance (`inf2.48xlarge`) will require changing the `tp_degree` specified in compilation section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps set up Jupyter Notebook and launch this tutorial:\n",
    "\n",
    "1. Clone the [\"AWS Neuron Samples\"](https://github.com/aws-neuron/aws-neuron-samples) repo to your instance using\n",
    "```\n",
    "git clone https://github.com/aws-neuron/aws-neuron-samples.git\n",
    "```\n",
    "2. Navigate to the `transformers-neuronx` inference samples folder\n",
    "```\n",
    "    cd aws-neuron-samples/torch-neuronx/transformers-neuronx/inference\n",
    "```\n",
    "3. Follow the instructions in [\"Jupyter Notebook Quickstart\"](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html) to run Jupyter Notebook on your instance.\n",
    "\n",
    "4. Locate this tutorial in your Jupyter Notebook session (`speculative_sampling.ipynb`) and launch it. Follow the rest of the instructions in this tutorial.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial requires the following pip packages:\n",
    "\n",
    "- `torch-neuronx`\n",
    "- `neuronx-cc`\n",
    "- `sentencepiece`\n",
    "- `transformers`\n",
    "- `transformers-neuronx`\n",
    "\n",
    "Most of these packages will be installed when configuring your environment using the [\"torch-neuronx inference setup guide\"](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/torch-neuronx.html#setup-torch-neuronx). The additional dependencies must be installed here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers-neuronx sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use of the Llama 2 model is governed by the Meta license and must be downloaded and converted to the standard Hugging Face format prior to running this sample.\n",
    "\n",
    "Follow the steps described in [\"meta-llama/Llama-2-70b\"](https://huggingface.co/meta-llama/Llama-2-70b) and [\"meta-llama/Llama-2-7b\"](https://huggingface.co/meta-llama/Llama-2-7b) to get access to the Llama 2 models from Meta and download the weights and tokenizer.\n",
    "\n",
    "After gaining access to the model checkpoints, you should be able to use the already converted checkpoints. Otherwise, if you are converting your own model, feel free to use the [\"conversion script\"](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py). The script can be called with the following (example) command:\n",
    "\n",
    "```\n",
    "python src/transformers/models/llama/convert_llama_weights_to_hf.py \\\n",
    "    --input_dir /path/to/downloaded/llama/weights --model_size 70Bf --output_dir ./Llama-2-70b\n",
    " ```\n",
    "\n",
    "Note: For the purposes of this sample we assume you have saved the Llama-2-70b model and the Llama-2-7b model in separate directories called `Llama-2-70b`  and `Llama-2-7b` with the following formats:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Llama-2-70b/\n",
    "├── config.json\n",
    "├── generation_config.json\n",
    "├── pytorch_model-00001-of-00015.bin\n",
    "├── pytorch_model-00002-of-00015.bin\n",
    "├── pytorch_model-00003-of-00015.bin\n",
    "├── pytorch_model-00004-of-00015.bin\n",
    "├── pytorch_model-00005-of-00015.bin\n",
    "├── pytorch_model-00006-of-00015.bin\n",
    "├── pytorch_model-00007-of-00015.bin\n",
    "├── pytorch_model-00008-of-00015.bin\n",
    "├── pytorch_model-00009-of-00015.bin\n",
    "├── pytorch_model-00010-of-00015.bin\n",
    "├── pytorch_model-00011-of-00015.bin\n",
    "├── pytorch_model-00012-of-00015.bin\n",
    "├── pytorch_model-00013-of-00015.bin\n",
    "├── pytorch_model-00014-of-00015.bin\n",
    "├── pytorch_model-00015-of-00015.bin\n",
    "├── pytorch_model.bin.index.json\n",
    "├── special_tokens_map.json\n",
    "├── tokenizer.json\n",
    "├── tokenizer.model\n",
    "└── tokenizer_config.json"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Llama-2-7b/\n",
    "├── config.json\n",
    "├── generation_config.json\n",
    "├── pytorch_model-00001-of-00015.bin\n",
    "├── pytorch_model-00002-of-00015.bin\n",
    "├── pytorch_model-00003-of-00015.bin\n",
    "├── pytorch_model-00004-of-00015.bin\n",
    "├── pytorch_model-00005-of-00015.bin\n",
    "├── pytorch_model-00006-of-00015.bin\n",
    "├── pytorch_model-00007-of-00015.bin\n",
    "├── pytorch_model-00008-of-00015.bin\n",
    "├── pytorch_model-00009-of-00015.bin\n",
    "├── pytorch_model-00010-of-00015.bin\n",
    "├── pytorch_model-00011-of-00015.bin\n",
    "├── pytorch_model-00012-of-00015.bin\n",
    "├── pytorch_model-00013-of-00015.bin\n",
    "├── pytorch_model-00014-of-00015.bin\n",
    "├── pytorch_model-00015-of-00015.bin\n",
    "├── pytorch_model.bin.index.json\n",
    "├── special_tokens_map.json\n",
    "├── tokenizer.json\n",
    "├── tokenizer.model\n",
    "└── tokenizer_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download and construct the draft and target models using the Hugging Face `from_pretrained` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "draft_model = LlamaForCausalLM.from_pretrained('Llama-2-7b')\n",
    "target_model = LlamaForCausalLM.from_pretrained('Llama-2-70b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the model state_dict into multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of reducing host memory usage, it is recommended to save the model `state_dict` as multiple files, as opposed to one monolithic file given by `torch.save`. This \"split-format\" `state_dict` can be created using the `save_pretrained_split` function. With this checkpoint format, the Neuron model loader can load parameters to the Neuron device high-bandwidth memory (HBM) directly by keeping at most one layer of model parameters in the CPU main memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from transformers_neuronx.module import save_pretrained_split\n",
    "\n",
    "save_pretrained_split(draft_model, './Llama-2-7b-split')\n",
    "save_pretrained_split(target_model, './Llama-2-70b-split')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform speculative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0401a3e7",
   "metadata": {},
   "source": [
    "We now load and compile the draft model and the target model.\n",
    "We use the Neuron `LlamaForSampling` class to load both models. Without extra configuration, autoregressive sampling is used as default.\n",
    "\n",
    "Since we need to perform regular autoregressive sampling in the draft model, we load and compile it using the default options.\n",
    "For the target model, we need to explicitly enable speculative decoding by calling the function enable_speculative_decoder(k) and this will let the model compiled for computing a window of k tokens at a time.\n",
    "\n",
    "Note that when loading the models, we must use the same `tp_degree`. Attempting to use a different value for the draft/target model will result in a load failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb21762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers_neuronx.llama.model import LlamaForSampling\n",
    "\n",
    "print(\"\\nStarting to compile Draft Model....\")\n",
    "# Load draft model\n",
    "draft_neuron_model = LlamaForSampling.from_pretrained('./Llama-2-7b-split', n_positions=128, batch_size=1, tp_degree=32, amp='f32')\n",
    "# compile to neuron \n",
    "draft_neuron_model.to_neuron()\n",
    "print(\"\\nCompleted compilation of Draft Model\")\n",
    "\n",
    "print(\"\\nStarting to compile Target Model....\")\n",
    "# Load target model\n",
    "target_neuron_model = LlamaForSampling.from_pretrained('./Llama-2-70b-split', n_positions=128, batch_size=1, tp_degree=32, amp='f32')\n",
    "# Enable speculative decoder\n",
    "target_neuron_model.enable_speculative_decoder(7)\n",
    "# compile to neuron \n",
    "target_neuron_model.to_neuron()\n",
    "print(\"\\nCompleted compilation of Target Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ca0c3f",
   "metadata": {},
   "source": [
    "Next, we initialize the tokenizer and the text prompt. \n",
    "\n",
    "We then initialize the `SpeculativeGenerator` class and pass the draft model, target model and speculation length as arguments. We can use this to call the `sample()` function and get the final sampled tokens after using the tokenizer to decode them. \n",
    "\n",
    "Comparing the response generation time between speculative sampling and autoregressive sampling, we see that speculative sampling is faster than autoregressive sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14457e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers_neuronx.speculation import SpeculativeGenerator, DraftModelForSpeculation, DefaultTokenAcceptor\n",
    "import sentencepiece\n",
    "from transformers import LlamaTokenizer\n",
    "\n",
    "#Initialize tokenizer and text prompt\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"Llama-2-70b\")\n",
    "prompt = \"Hello, I'm a generative AI language model.\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# create SpeculativeGenerator\n",
    "spec_gen = SpeculativeGenerator(draft_neuron_model, target_neuron_model, 7)\n",
    "\n",
    "# call speculative sampling on given input\n",
    "start_spec_timer = time.time()\n",
    "\n",
    "print(\"Starting to call Speculative Sampling..\")\n",
    "response = spec_gen.sample(\n",
    "    input_ids=input_ids,\n",
    "    sequence_length=50,\n",
    ")\n",
    "end_spec_timer = time.time()\n",
    "\n",
    "generated_text = tokenizer.decode(response[0])\n",
    "print(f\"\\nDecoded tokens: {generated_text}\")\n",
    "\n",
    "print(f\"\\nSpeculative sampling response generation took {end_spec_timer - start_spec_timer} ms\")\n",
    "\n",
    "start_auto_r_timer = time.time()\n",
    "autor_response = target_neuron_model.sample(input_ids=input_ids, sequence_length=50)\n",
    "end_auto_r_timer = time.time()\n",
    "\n",
    "print(f\"\\nAutoregressive sampling response generation took {end_auto_r_timer - start_auto_r_timer} ms\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
