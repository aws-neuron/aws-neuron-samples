{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "911f6a08-e982-4516-8165-c61196c1c723",
   "metadata": {},
   "source": [
    "# Run Open Llama speculative sampling on Inf2 & Trn1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eded3d-de1f-453b-a676-7b2f8b92789a",
   "metadata": {},
   "source": [
    "In speculative sampling, we use use a smaller \"draft model\" to speculate future tokens. These are then sent to the larger \"target model\", which accepts/rejects these tokens.  \n",
    "\n",
    "For a more detailed understanding, please refer to the original paper by DeepMind titled [\"Accelerating Large Language Model Decoding with Speculative Sampling\"](https://arxiv.org/abs/2302.01318)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5a99ac-dfe0-47e2-9dd1-e2b3077c0e3b",
   "metadata": {},
   "source": [
    "In this example we perform speculative sampling using the Hugging Face \"openlm-research/open_llama_13b\" model and Hugging Face \"openlm-research/open_llama_3b\".\n",
    "Here, the 13b model is considered the target model and the 3b model is considered the draft model.\n",
    "\n",
    "The example has the following main sections:\n",
    "\n",
    "1. Set up the Jupyter Notebook\n",
    "2. Install dependencies\n",
    "3. Download and construct the model\n",
    "5. Split the model state_dict into multiple files\n",
    "6. Perform speculative sampling\n",
    "\n",
    "This Jupyter Notebook can be run on an Inf2 instance (inf2.48xlarge) or Trn1 instance (trn1.32xlarge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85b17c0-830a-4c01-b29f-f67f03c6c4d7",
   "metadata": {},
   "source": [
    "## Set up the Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed2e8f8-4230-464e-87cb-8ecf4a600c99",
   "metadata": {},
   "source": [
    "The following steps set up Jupyter Notebook and launch this tutorial:\n",
    "\n",
    "1. Clone the AWS Neuron Samples repo to your instance using\n",
    "\n",
    "    git clone https://github.com/aws-neuron/aws-neuron-samples.git\n",
    "\n",
    "2. Navigate to the transformers-neuronx inference samples folder\n",
    "\n",
    "    cd aws-neuron-samples/torch-neuronx/transformers-neuronx/inference\n",
    "\n",
    "3. Follow the instructions in Jupyter Notebook QuickStart to run Jupyter Notebook on your instance.\n",
    "\n",
    "4. Locate this tutorial in your Jupyter Notebook session (speculative_sampling.ipynb) and launch it. Follow the rest of the instructions in this tutorial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefbe1b3-601b-4b3c-9d1d-02dfad83c7cb",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b7ab91-16af-41f7-be56-c992768bf602",
   "metadata": {},
   "source": [
    "This tutorial requires the following pip packages:\n",
    "\n",
    "- torch-neuronx\n",
    "- neuronx-cc\n",
    "- sentencepiece\n",
    "- transformers\n",
    "- transformers-neuronx\n",
    "\n",
    "Most of these packages will be installed when configuring your environment using the torch-neuronx inference setup guide. The additional dependencies must be installed here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4805b0-d8cc-48b6-ab0d-3c1ca1e5c55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers-neuronx sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b9539b-b8bd-4312-8cf1-94f7ad684040",
   "metadata": {},
   "source": [
    "## Download and construct the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21025321-d9e4-4fa2-be4d-23f2281a4c73",
   "metadata": {},
   "source": [
    "We download and construct the draft and target models using the Hugging Face from_pretrained method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd811c6-db15-4c51-86b0-6072a76e8941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.auto import AutoModelForCausalLM\n",
    "\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(\"openlm-research/open_llama_3b\", low_cpu_mem_usage=True)\n",
    "target_model = AutoModelForCausalLM.from_pretrained(\"openlm-research/open_llama_13b\", low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7d8e2e-725c-4660-85d7-5f4236fd74d5",
   "metadata": {},
   "source": [
    "## Split the model state_dict into multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f656dfe1-d47f-452f-a0ed-b8deb5140229",
   "metadata": {},
   "source": [
    "For the sake of reducing host memory usage, it is recommended to save the model state_dict as multiple files, as opposed to one monolithic file given by torch.save. This \"split-format\" state_dict can be created using the save_pretrained_split function. With this checkpoint format, the Neuron model loader can load parameters to the Neuron device high-bandwidth memory (HBM) directly by keeping at most one layer of model parameters in the CPU main memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "892cf38f-00a8-4a62-8dcc-36b8d4885123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers_neuronx.module import save_pretrained_split\n",
    "\n",
    "save_pretrained_split(draft_model, './open-llama-3b-split')\n",
    "save_pretrained_split(target_model, './open-llama-13b-split')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfae6a46-6146-4fd8-8a9c-70071e15da77",
   "metadata": {},
   "source": [
    "## Perform speculative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fde547-0a50-4eb7-bd30-7f9100b021e9",
   "metadata": {},
   "source": [
    "We now load and compile the draft model and the target model.\n",
    "We use the Neuron `LlamaForSampling` class to load both models. Without extra configuration, autoregressive sampling is used as default.\n",
    "\n",
    "Since we need to perform regular autoregressive sampling in the draft model, we load and compile it using the default options.\n",
    "For the target model, we need to explicitly enable speculative decoding by calling the function enable_speculative_decoder(k) and this will let the model compiled for computing a window of k tokens at a time.\n",
    "\n",
    "Note that when loading the models, we must use the same `tp_degree`. Attempting to use a different value for the draft/target model will result in a load failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ba077-401e-4968-a30e-5f6d4557e344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers_neuronx.llama.model import LlamaForSampling\n",
    "\n",
    "print(\"\\nStarting to compile Draft Model....\")\n",
    "# Load draft model\n",
    "draft_neuron_model = LlamaForSampling.from_pretrained('./open-llama-3b-split', n_positions=256, batch_size=1, tp_degree=8, amp='f32')\n",
    "# compile to neuron \n",
    "draft_neuron_model.to_neuron()\n",
    "print(\"\\nCompleted compilation of Draft Model\")\n",
    "\n",
    "print(\"\\nStarting to compile Target Model....\")\n",
    "# Load target model\n",
    "target_neuron_model = LlamaForSampling.from_pretrained('./open-llama-13b-split', n_positions=256, batch_size=1, tp_degree=8, amp='f32')\n",
    "# Enable speculative decoder\n",
    "target_neuron_model.enable_speculative_decoder(4)\n",
    "# compile to neuron \n",
    "target_neuron_model.to_neuron()\n",
    "print(\"\\nCompleted compilation of Target Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c520b34-02e7-42f2-9a71-70806f26b13f",
   "metadata": {},
   "source": [
    "Next, we initialize the tokenizer and the text prompt. \n",
    "By default, we use the `DefaultTokenAcceptor` provided by Neuron. This follows the same acceptance logic as the [original DeepMind paper](https://arxiv.org/abs/2302.01318). \n",
    "If you choose to have a different acceptance logic, you can always create your own token acceptor class which needs to be a subclass of `TokenAcceptor` class. This needs to then be passed to the `SpeculativeGenerator` class. \n",
    "\n",
    "We then initialize the `SpeculativeGenerator` class and pass the draft model, target model and speculation length as arguments. We can use this to call the `sample()` function and get the final sampled tokens after using the tokenizer to decode them. \n",
    "\n",
    "Comparing the response generation time between speculative sampling and autoregressive sampling, we see that speculative sampling is faster than autoregressive sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d39e73-b6bc-4bf6-8831-5109d5e589e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers_neuronx.speculation import SpeculativeGenerator\n",
    "from transformers import LlamaTokenizer\n",
    "\n",
    "#Initialize tokenizer and text prompt\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"openlm-research/open_llama_3b\")\n",
    "prompt = \"Hello, I'm a generative AI language model.\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# create SpeculativeGenerator\n",
    "spec_gen = SpeculativeGenerator(draft_neuron_model, target_neuron_model, 4)\n",
    "\n",
    "# call speculative sampling on given input\n",
    "start_spec_timer = time.time()\n",
    "\n",
    "print(\"Starting to call Speculative Sampling..\")\n",
    "response = spec_gen.sample(\n",
    "    input_ids=input_ids,\n",
    "    sequence_length=50,\n",
    ")\n",
    "end_spec_timer = time.time()\n",
    "\n",
    "generated_text = tokenizer.decode(response[0])\n",
    "print(f\"\\nDecoded tokens: {generated_text}\")\n",
    "\n",
    "print(f\"\\nSpeculative sampling response generation took {end_spec_timer - start_spec_timer} ms\")\n",
    "\n",
    "start_auto_r_timer = time.time()\n",
    "autor_response = target_neuron_model.sample(input_ids=input_ids, sequence_length=50)\n",
    "end_auto_r_timer = time.time()\n",
    "\n",
    "print(f\"\\nAutoregressive sampling response generation took {end_auto_r_timer - start_auto_r_timer} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2b1945-f5ae-4f89-91b1-a7008cb75b86",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
