{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59913016-f89e-4a0e-9afe-b3a06e9112d5",
   "metadata": {},
   "source": [
    "# Run Hugging Face `Llama 3 8B` autoregressive sampling on Inf2 & Trn1 with 32k sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8454655-ec27-45e3-8da7-f82b744321ee",
   "metadata": {},
   "source": [
    "In this example we compile and deploy the Hugging Face [winglian/Llama-3-8b-64k-PoSE](https://huggingface.co/winglian/Llama-3-8b-64k-PoSE) model (based on position extension of LLaMA model) for tensor parallel inference on Neuron using the `transformers-neuronx` package. We use a sequence length of 32k.\n",
    "\n",
    "The example has the following main sections:\n",
    "1. Set up the Jupyter Notebook\n",
    "1. Install dependencies\n",
    "1. Download the model\n",
    "1. Perform autoregressive sampling using tensor parallelism\n",
    "\n",
    "This Jupyter Notebook can be run on an Inf2 instance (`inf2.48xlarge`) or Trn1 instance (`trn1.32xlarge`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2b7693-2950-41fc-a038-17cba44bf003",
   "metadata": {},
   "source": [
    "## Set up the Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47ef383-0dea-4423-8c38-29c73927fd78",
   "metadata": {},
   "source": [
    "The following steps set up Jupyter Notebook and launch this tutorial:\n",
    "1. Clone the [AWS Neuron Samples](https://github.com/aws-neuron/aws-neuron-samples) repo to your instance using\n",
    "```\n",
    "git clone https://github.com/aws-neuron/aws-neuron-samples.git\n",
    "```\n",
    "2. Navigate to the `transformers-neuronx` inference samples folder\n",
    "```\n",
    "cd aws-neuron-samples/torch-neuronx/transformers-neuronx/inference\n",
    "```\n",
    "3. Follow the instructions in [Jupyter Notebook QuickStart](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html) to run Jupyter Notebook on your instance.\n",
    "4. Locate this tutorial in your Jupyter Notebook session (`llama-3-8b-32k-sampling.ipynb`) and launch it. Follow the rest of the instructions in this tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a727963e-8178-4d2a-a5cd-a4f2bf00197e",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "This tutorial requires the following pip packages:\n",
    "\n",
    " - `torch-neuronx`\n",
    " - `neuronx-cc`\n",
    " - `sentencepiece`\n",
    " - `transformers`\n",
    " - `transformers-neuronx`\n",
    "\n",
    "\n",
    "Most of these packages will be installed when configuring your environment using the [torch-neuronx inference setup guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/torch-neuronx.html#setup-torch-neuronx). The additional dependencies must be installed here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4899b2-39b2-4309-b7df-48fe74b56eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers-neuronx sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14400e26-2058-44b0-b680-b1cee57203aa",
   "metadata": {},
   "source": [
    "## Download the model and perform autoregressive sampling using tensor parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e233a69-5658-4180-8f6c-91f377a01001",
   "metadata": {},
   "source": [
    "We download and construct the model checkpoint using the `from_pretrained` function simply using the Huggingface model name. This gives us all of the necessary files for running `winglian/Llama-3-8b-64k-PoSE` autoregressive sampling.\n",
    "\n",
    "The memory required to host any model can be computed with:\n",
    "```\n",
    "total memory = bytes per parameter * number of parameters\n",
    "```\n",
    "When using `float16` casted weights for a 8 billion parameter model, this works out to `2 * 8B` or ~16GB of weights. Each NeuronCore has 16GB of memory which means that a 16GB model would just fit on a single NeuronCore. In reality, the total space required is often greater than just the number of parameters due to caching attention layer projections (KV caching). This caching mechanism grows memory allocations linearly with sequence length and batch size.\n",
    "\n",
    "To get very large language models to fit on Inf2 & Trn1, tensor parallelism is used to split weights, data, and compute across multiple NeuronCores. The number of NeuronCores that the weights are split across can be controlled by setting the `tp_degree` parameter. This parallelism degree must be chosen to ensure that the memory usage per NeuronCore will be less than the physical 16GB limit. When configuring tensor parallelism, the memory per NeuronCore can be computed with:\n",
    "\n",
    "```\n",
    "memory per core = (bytes per parameter * number of parameters) / tp_degree\n",
    "```\n",
    "\n",
    "This can be used to compute the minimum instance sizing by ensuring that the value selected for `tp_degree` results in less than 16GB allocated per NeuronCore.\n",
    "\n",
    "Note that increasing the `tp_degree` beyond the minimum requirement almost always results in a faster model. Increasing the tensor parallelism degree improves memory bandwidth which improves model performance. To optimize performance it's recommended to use the highest tensor parallelism degree that is supported by the instance. In this sample we use tensor parallelism degree 32 to optimize performance on `trn1.32xlarge`, but this should be changed to 24 if you are using a `inf2.48xlarge`. \n",
    "\n",
    "We will use the Neuron `LlamaForSampling` class to implement tensor parallelism for the Llama based model. We supply the n_positions as 32768 and context length estimates to precompile various possible prompt lengths. Tensor parallelism is enabled through the argument `tp_degree=32`. We enable `float16` casting with the `amp='f16'` flag. The model computational graph is compiled by `neuronx-cc` for optimized inference on Neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea21739-a65e-4a5c-9a10-7f963a99a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the model\n",
    "\n",
    "from transformers_neuronx import LlamaForSampling\n",
    "\n",
    "\n",
    "# load winglian/Llama-3-8b-64k-PoSE to the NeuronCores with 32-way tensor parallelism and run compilation\n",
    "# we pass n_positions as 32768 so that the model supports 32k sequence length, and pass the context_length_estimate\n",
    "# list generated below.\n",
    "\n",
    "# we supply a list of context_length_estimate that allows us to get low context encoding latency even for \n",
    "# larger prompts than 16384 (largest context_length_estimate by default is 1/2 of n_positions input)\n",
    "context_length_estimate = [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 20480, 24576, 28672]\n",
    "\n",
    "neuron_model = LlamaForSampling.from_pretrained('winglian/Llama-3-8b-64k-PoSE', n_positions=32768, \\\n",
    "                                                context_length_estimate=context_length_estimate, \\\n",
    "                                                batch_size=1, tp_degree=32, amp='f16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc93079d-7d17-48fd-bf9d-7176bc061a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform autoregressive sampling\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import requests, re\n",
    "import os\n",
    "\n",
    "# Compiler flag -O1 is a workaround for “Too many instructions after unroll” in SDK 2.14\n",
    "# It can also help with faster compilation\n",
    "os.environ['NEURON_CC_FLAGS'] = '-O1'\n",
    "\n",
    "neuron_model.to_neuron()\n",
    "\n",
    "# construct a tokenizer and encode prompt text\n",
    "# For the prompt we take a recent publication (html format), strip html tags to convert to txt\n",
    "# and ask the model to summarize the paper. The input length is 26k+ tokens.\n",
    "tokenizer = AutoTokenizer.from_pretrained('winglian/Llama-3-8b-64k-PoSE')\n",
    "prompt = re.sub('<[^<]+?>', '', requests.get(\"https://arxiv.org/html/2402.19427v1\").text) # strip html tags\n",
    "prompt += \"\\n\\n========================THE END======================\\n\"\n",
    "prompt += \"A 10 point summary of the paper in simple words: \"\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\") \n",
    "num_input_tokens = len(input_ids[0]) # over 26k tokens\n",
    "print(f\"num_input_tokens: {num_input_tokens}\")\n",
    "\n",
    "# run inference with top-k sampling\n",
    "with torch.inference_mode():\n",
    "    start = time.time()\n",
    "    generated_sequences = neuron_model.sample(input_ids, sequence_length=32768, top_k=1)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "# display the new generated tokens\n",
    "generated_sequences = [tokenizer.decode(seq[num_input_tokens:]) for seq in generated_sequences]\n",
    "print(f'generated sequence {generated_sequences[0]} in {elapsed} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ac4991-7606-4c2f-90af-230998b0de20",
   "metadata": {},
   "source": [
    "## Save and load the compiled model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6a4ba9-40fd-4544-81ab-9fd249f22e4d",
   "metadata": {},
   "source": [
    "The ```save``` and ```load``` functions can be used to save and load compiled model artifacts respectively. Loading compiled model artifacts from a provided directory will avoid model recompilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07176c29-b30b-4d16-8291-3bd0142dc42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_model.save('./neuron_artifacts') # can be copied and used on a different neuron instance\n",
    "del neuron_model\n",
    "neuron_model = LlamaForSampling.from_pretrained('winglian/Llama-3-8b-64k-PoSE', n_positions=32768, \\\n",
    "                                                context_length_estimate=context_length_estimate, \\\n",
    "                                                batch_size=1, tp_degree=32, amp='f16')\n",
    "neuron_model.load('neuron_artifacts') # Load the compiled Neuron artifacts\n",
    "neuron_model.to_neuron() # will skip compile\n",
    "\n",
    "with torch.inference_mode():\n",
    "    start = time.time()\n",
    "    generated_sequences = neuron_model.sample(input_ids, sequence_length=32768, top_k=1)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "generated_sequences = [tokenizer.decode(seq[num_input_tokens:]) for seq in generated_sequences]\n",
    "print(f'generated sequence {generated_sequences[0]} in {elapsed} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-neuronx)",
   "language": "python",
   "name": "aws_neuron_venv_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
