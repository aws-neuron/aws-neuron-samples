{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59913016-f89e-4a0e-9afe-b3a06e9112d5",
   "metadata": {},
   "source": [
    "# Run Hugging Face `mistralai/Mixtral-8x7B-v0.1` autoregressive sampling on Inf2 & Trn1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8454655-ec27-45e3-8da7-f82b744321ee",
   "metadata": {},
   "source": [
    "In this example, we compile and deploy the Hugging Face [mistralai/Mixtral-8x7B-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1) model for tensor parallel inference on AWS Neuron devices using the `transformers-neuronx` package.\n",
    "\n",
    "The example has the following main sections:\n",
    "1. Set up the Jupyter Notebook\n",
    "1. Install dependencies\n",
    "1. Download and construct the model\n",
    "1. Perform autoregressive sampling\n",
    "\n",
    "This Jupyter Notebook can be run on an Inf2 instance (`inf2.48xlarge`) or Trn1 instance (`trn1.32xlarge`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2b7693-2950-41fc-a038-17cba44bf003",
   "metadata": {},
   "source": [
    "## Set up the Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47ef383-0dea-4423-8c38-29c73927fd78",
   "metadata": {},
   "source": [
    "The following steps set up Jupyter Notebook and launch this tutorial:\n",
    "1. Clone the [AWS Neuron Samples](https://github.com/aws-neuron/aws-neuron-samples) repo to your instance using\n",
    "```\n",
    "git clone https://github.com/aws-neuron/aws-neuron-samples.git\n",
    "```\n",
    "2. Navigate to the `transformers-neuronx` inference samples folder\n",
    "```\n",
    "cd aws-neuron-samples/torch-neuronx/transformers-neuronx/inference\n",
    "```\n",
    "3. Follow the instructions in [Jupyter Notebook QuickStart](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html) to run Jupyter Notebook on your instance.\n",
    "4. Locate this tutorial in your Jupyter Notebook session (`mixtral-8x7b-sampling.ipynb`) and launch it. Follow the rest instructions in this tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a727963e-8178-4d2a-a5cd-a4f2bf00197e",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "This tutorial requires the following pip packages:\n",
    "\n",
    " - `torch-neuronx`\n",
    " - `neuronx-cc`\n",
    " - `sentencepiece`\n",
    " - `transformers`\n",
    " - `transformers-neuronx`\n",
    "\n",
    "\n",
    "Most of these packages will be installed when configuring your environment using the [torch-neuronx inference setup guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/torch-neuronx.html#setup-torch-neuronx). The additional dependencies must be installed here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4899b2-39b2-4309-b7df-48fe74b56eb2",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "!pip install transformers-neuronx sentencepiece"
=======
    "!pip install transformers-neuronx"
>>>>>>> 727b528 (mixtral-8x7b inference sample)
=======
    "!pip install transformers-neuronx"
>>>>>>> 3907818 (mixtral-8x7b tnx inference sample)
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06fb496-f8c6-4222-b5ad-39b3e0bc0e22",
   "metadata": {},
   "source": [
    "## Download and construct the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2669028-8cf1-49f6-8e93-f6ceb39588fd",
   "metadata": {},
   "source": [
    "We download and construct the `mistralai/Mixtral-8x7B-v0.1` model using the Hugging Face `from_pretrained` method. We save the model checkpoint into a local directory, `./mistralai/Mixtral-8x7B-v0.1`. Note that starting from Neuron SDK 2.18, `transformers-neuronx` supports directly loading Hugging Face models in safetensor format and `save_pretrained_split` will be deprecated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba11439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MixtralForCausalLM\n",
    "\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "model = MixtralForCausalLM.from_pretrained('mistralai/Mixtral-8x7B-v0.1')\n",
=======
=======
>>>>>>> 3907818 (mixtral-8x7b tnx inference sample)
    "tokenizer = AutoTokenizer.from_pretrained('mistralai/Mixtral-8x7B-v0.1')\n",
    "model = MixtralForCausalLM.from_pretrained('mistralai/Mixtral-8x7B-v0.1')\n",
    "\n",
    "tokenizer.save_pretrained('./mistralai/Mixtral-8x7B-v0.1')\n",
<<<<<<< HEAD
>>>>>>> 727b528 (mixtral-8x7b inference sample)
=======
>>>>>>> 3907818 (mixtral-8x7b tnx inference sample)
    "model.save_pretrained('./mistralai/Mixtral-8x7B-v0.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1ededb-e0d6-4c1d-aac8-bc3d29bd6ebe",
   "metadata": {},
   "source": [
    "## Perform autoregressive sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a87b9f-2948-4db9-946f-b618533f03a7",
   "metadata": {},
   "source": [
    "Now we have all the necessary files for running `mistralai/Mixtral-8x7B-v0.1` autoregressive sampling. Due to the model size and mixture-of-expert (MoE) implementation in `transformers-neuronx`, the supported tensor parallelism (TP) degrees are {8, 16, 32}. We describe the detail consideration as follows.\n",
    "\n",
    "The memory required to host a model can be computed as:\n",
    "```\n",
    "total memory = bytes per parameter * number of parameters\n",
    "```\n",
    "The `mistralai/Mixtral-8x7B-v0.1` model consists of 46.7 billion parameters.  With `float16` casted weights, we need 93.4GB to store the model weights. In reality, the total space required is often greater than just the model parameters due to caching attention layer projections (KV caching). This caching mechanism grows memory allocations linearly with sequence length and batch size. The exact calculation can be found from the [AWS Neuron documentation page](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/transformers-neuronx/generative-llm-inference-with-neuron.html).\n",
    "\n",
    "To get very large language models to fit on Inf2 & Trn1, tensor parallelism is used to split weights, data, and compute across multiple NeuronCores, each equipped with 16GB high-bandwidth memory (HBM). For this model, we need at least 6 NeuronCores. \n",
    "\n",
    "The `mistralai/Mixtral-8x7B-v0.1` model adopts the MoE architecture with 8 experts in total. `transformers-neuronx` in Neuron SDK 2.18 employs expert parallelism for MoE architecture, splitting the 8 experts  across multiple NeuronCores. Note that increasing the TP degree beyond the minimum requirement almost always improves the model performance as more compute and memory bandwidth are available. To get better performance, it's recommended to use higher TP degree, for example, 32 for `trn1.32xlarge`. Note that we don't support TP degree of 24 on `inf2.48xlarge` for this model and the max TP degree that can be used on `inf2.48xlarge` is 16. If using TP degree 8 to run this model, users can use [int8 weight storage] (https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/transformers-neuronx/transformers-neuronx-developer-guide.html) to reduce the model memory footprint.\n",
    "\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "In the following, we use the `MixtralForSampling` class in `transformers-neuronx` to create the model and run autoregressive sampling. We enable tensor parallelism with the argument `tp_degree=16` and the use of data type `float16` with the argument `amp='f16'`. "
=======
    "In the following, we use the `MixtralForSampling` class in `transformers-neuronx` to create the model and run autoregressive sampling. We enable tensor parallelism with the argument `tp_degree=16` and the use of data type `float16` with the argument `amp='f16'`. We set the max sequence length with `n_positions=1024`. "
>>>>>>> 727b528 (mixtral-8x7b inference sample)
=======
    "In the following, we use the `MixtralForSampling` class in `transformers-neuronx` to create the model and run autoregressive sampling. We enable tensor parallelism with the argument `tp_degree=16` and the use of data type `float16` with the argument `amp='f16'`. We set the max sequence length with `n_positions=1024`. "
>>>>>>> 3907818 (mixtral-8x7b tnx inference sample)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff1b319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers_neuronx.mixtral.model import MixtralForSampling\n",
    "\n",
<<<<<<< HEAD
<<<<<<< HEAD
=======
    "# set the directory for storing compiled model files\n",
    "os.environ['NEURON_COMPILE_CACHE_URL'] = f'./neuron_cache'\n",
    "\n",
>>>>>>> 727b528 (mixtral-8x7b inference sample)
=======
    "# set the directory for storing compiled model files\n",
    "os.environ['NEURON_COMPILE_CACHE_URL'] = f'./neuron_cache'\n",
    "\n",
>>>>>>> 3907818 (mixtral-8x7b tnx inference sample)
    "# load mistralai/Mixtral-8x7B-v0.1 to the NeuronCores with 16-way tensor parallelism\n",
    "neuron_model = MixtralForSampling.from_pretrained(\n",
    "    './mistralai/Mixtral-8x7B-v0.1',\n",
    "    batch_size=1,\n",
    "    tp_degree=16,\n",
<<<<<<< HEAD
<<<<<<< HEAD
=======
    "    n_positions=1024,\n",
>>>>>>> 727b528 (mixtral-8x7b inference sample)
=======
    "    n_positions=1024,\n",
>>>>>>> 3907818 (mixtral-8x7b tnx inference sample)
    "    amp='f16')\n",
    "\n",
    "# compile model\n",
    "neuron_model.to_neuron()\n",
    "\n",
    "# construct a tokenizer and encode prompt text\n",
    "tokenizer = AutoTokenizer.from_pretrained('./mistralai/Mixtral-8x7B-v0.1')\n",
    "prompt = \"Hello, I'm a language model,\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# run inference with top-k sampling\n",
    "with torch.inference_mode():\n",
    "    start = time.time()\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "    generated_sequences = neuron_model.sample(input_ids, sequence_length=1024, top_k=1)\n",
=======
    "    generated_sequences = neuron_model.sample(input_ids, sequence_length=512, top_k=1) # sequence_length <= n_positions\n",
>>>>>>> 727b528 (mixtral-8x7b inference sample)
=======
    "    generated_sequences = neuron_model.sample(input_ids, sequence_length=512, top_k=1) # sequence_length <= n_positions\n",
>>>>>>> 3907818 (mixtral-8x7b tnx inference sample)
    "    elapsed = time.time() - start\n",
    "\n",
    "generated_sequences = [tokenizer.decode(seq) for seq in generated_sequences]\n",
    "print(f'generated sequences {generated_sequences} in {elapsed} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6a4ba9-40fd-4544-81ab-9fd249f22e4d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
