{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d416726",
   "metadata": {},
   "source": [
    "# MarianMT - Pytorch\n",
    "This notebook shows how to compile a pre-trained MarianMT model in PyTorch to AWS Inferentia (inf1 instances) using NeuronSDK. The original implementation is provided by HuggingFace.\n",
    "\n",
    "This is a simplified example based on the Neuron MarianMT tutorial. For more details regarding the code, reference the [tutorial](https://github.com/aws/aws-neuron-sdk/blob/master/src/examples/pytorch/transformers-marianmt.ipynb).\n",
    "\n",
    "**Reference:** https://huggingface.co/Helsinki-NLP/opus-mt-en-de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea195b",
   "metadata": {},
   "source": [
    "## 1) Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc531ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Neuron PyTorch\n",
    "%pip install -U pip\n",
    "%pip install -U torch-neuron \"transformers==4.26.1\" neuron-cc[tensorflow] sentencepiece\n",
    "# use --force-reinstall if you're facing some issues while loading the modules\n",
    "# now restart the kernel again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c268ce",
   "metadata": {},
   "source": [
    "## 2) Load a pre-trained model to CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d45837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "model_name='Helsinki-NLP/opus-mt-en-de'   # English -> German model\n",
    "num_texts = 1                             # Number of input texts to decode\n",
    "num_beams = 4                             # Number of beams per input text\n",
    "max_encoder_length = 32                   # Maximum input token length\n",
    "max_decoder_length = 32                   # Maximum output token length\n",
    "\n",
    "model_cpu = MarianMTModel.from_pretrained(model_name)\n",
    "model_cpu.config.max_length = max_decoder_length\n",
    "model_cpu.eval()\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d22e9",
   "metadata": {},
   "source": [
    "## 3) Test inference on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f01c3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def infer(model, tokenizer, text):\n",
    "\n",
    "    # Truncate and pad the max length to ensure that the token size is compatible with fixed-sized encoder (Not necessary for pure CPU execution)\n",
    "    batch = tokenizer(text, max_length=max_decoder_length, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "    output = model.generate(**batch, max_length=max_decoder_length, num_beams=num_beams, num_return_sequences=num_beams)\n",
    "    results = [tokenizer.decode(t, skip_special_tokens=True) for t in output]\n",
    "\n",
    "    print('Texts:')\n",
    "    for i, summary in enumerate(results):\n",
    "        print(i + 1, summary)\n",
    "\n",
    "sample_text = \"I am a small frog.\"\n",
    "infer(model_cpu, tokenizer, sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691fbb73",
   "metadata": {},
   "source": [
    "## 4) Create padded encoder & decoder wrappers\n",
    "\n",
    "The following wrappers allow the model to be traced with padded inputs and produce outputs that can be used by the huggingface generate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb55867d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class PaddedEncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.encoder = model.model.encoder\n",
    "        self.main_input_name = 'input_ids'\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.encoder(input_ids, attention_mask=attention_mask, return_dict=False)\n",
    "\n",
    "\n",
    "class PaddedDecoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.weight = model.model.shared.weight.clone().detach()\n",
    "        self.bias = model.final_logits_bias.clone().detach()\n",
    "        self.decoder = model.model.decoder\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, encoder_outputs, index):\n",
    "\n",
    "        # Invoke the decoder\n",
    "        hidden, = self.decoder(\n",
    "            input_ids=input_ids,\n",
    "            encoder_hidden_states=encoder_outputs,\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            return_dict=False,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "        _, n_length, _ = hidden.shape\n",
    "\n",
    "        # Create selection mask\n",
    "        mask = torch.arange(n_length, dtype=torch.float32) == index\n",
    "        mask = mask.view(1, -1, 1)\n",
    "\n",
    "        # Broadcast mask\n",
    "        masked = torch.multiply(hidden, mask)\n",
    "\n",
    "        # Reduce along 1st dimension\n",
    "        hidden = torch.sum(masked, 1, keepdims=True)\n",
    "\n",
    "        # Compute final linear layer for token probabilities\n",
    "        logits = F.linear(\n",
    "            hidden,\n",
    "            self.weight,\n",
    "            bias=self.bias\n",
    "        )\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674b5048",
   "metadata": {},
   "source": [
    "## 5) Create a padded GenerationMixin subclass\n",
    "\n",
    "This generator object allows us to reuse the various sampling methods provided by huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e08c7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from transformers import GenerationMixin, AutoConfig\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput, BaseModelOutput\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "\n",
    "\n",
    "class PaddedGenerator(PreTrainedModel, GenerationMixin):\n",
    "\n",
    "    @classmethod\n",
    "    def from_model(cls, model):\n",
    "        generator = cls(model.config)\n",
    "        generator.encoder = PaddedEncoder(model)\n",
    "        generator.decoder = PaddedDecoder(model)\n",
    "        return generator\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "            self,\n",
    "            input_ids,\n",
    "            encoder_outputs=None,\n",
    "            attention_mask=None,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        # Pad the inputs for Neuron\n",
    "        current_length = input_ids.shape[1]\n",
    "        pad_size = self.config.max_length - current_length\n",
    "        return dict(\n",
    "            input_ids=F.pad(input_ids, (0, pad_size)),\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_outputs=encoder_outputs.last_hidden_state,\n",
    "            current_length=torch.tensor(current_length - 1),\n",
    "        )\n",
    "\n",
    "    def get_encoder(self):\n",
    "        def encode(input_ids, attention_mask, **kwargs):\n",
    "            output, = self.encoder(input_ids, attention_mask)\n",
    "            return BaseModelOutput(\n",
    "                last_hidden_state=output,\n",
    "            )\n",
    "        return encode\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, encoder_outputs, current_length, **kwargs):\n",
    "        logits = self.decoder(input_ids, attention_mask, encoder_outputs, current_length)\n",
    "        return Seq2SeqLMOutput(logits=logits)\n",
    "\n",
    "    @property\n",
    "    def device(self):  # Attribute required by beam search\n",
    "        return torch.device('cpu')\n",
    "\n",
    "    def save_pretrained(self, directory):\n",
    "        if os.path.isfile(directory):\n",
    "            print(f\"Provided path ({directory}) should be a directory, not a file\")\n",
    "            return\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        torch.jit.save(self.encoder, os.path.join(directory, 'encoder.pt'))\n",
    "        torch.jit.save(self.decoder, os.path.join(directory, 'decoder.pt'))\n",
    "        self.config.save_pretrained(directory)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, directory):\n",
    "        config = AutoConfig.from_pretrained(directory)\n",
    "        obj = cls(config)\n",
    "        obj.encoder = torch.jit.load(os.path.join(directory, 'encoder.pt'))\n",
    "        obj.decoder = torch.jit.load(os.path.join(directory, 'decoder.pt'))\n",
    "        setattr(obj.encoder, 'main_input_name', 'input_ids')  # Attribute required by beam search\n",
    "        return obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538449f3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6) Test padded inference on CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf33479b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "padded_model_cpu = PaddedGenerator.from_model(model_cpu)\n",
    "infer(padded_model_cpu, tokenizer, sample_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5ce070",
   "metadata": {},
   "source": [
    "## 7) Trace the encoder & decoder for inference on Neuron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d4e44",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_neuron\n",
    "\n",
    "\n",
    "def trace(model, num_texts, num_beams, max_decoder_length, max_encoder_length):\n",
    "    \"\"\"\n",
    "    Traces the encoder and decoder modules for use on Neuron.\n",
    "\n",
    "    This function fixes the network to the given sizes. Once the model has been\n",
    "    compiled to a given size, the inputs to these networks must always be of\n",
    "    fixed size.\n",
    "\n",
    "    Args:\n",
    "        model (PaddedGenerator): The padded generator to compile for Neuron\n",
    "        num_texts (int): The number of input texts to translate at once\n",
    "        num_beams (int): The number of beams to compute per text\n",
    "        max_decoder_length (int): The maximum number of tokens to be generated\n",
    "        max_encoder_length (int): The maximum number of input tokens that will be encoded\n",
    "    \"\"\"\n",
    "\n",
    "    # Trace the encoder\n",
    "    inputs = (\n",
    "        torch.ones((num_texts, max_encoder_length), dtype=torch.long),\n",
    "        torch.ones((num_texts, max_encoder_length), dtype=torch.long),\n",
    "    )\n",
    "    encoder = torch_neuron.trace(model.encoder, inputs)\n",
    "\n",
    "    # Trace the decoder (with expanded inputs)\n",
    "    batch_size = num_texts * num_beams\n",
    "    inputs = (\n",
    "        torch.ones((batch_size, max_decoder_length), dtype=torch.long),\n",
    "        torch.ones((batch_size, max_encoder_length), dtype=torch.long),\n",
    "        torch.ones((batch_size, max_encoder_length, model.config.d_model), dtype=torch.float),\n",
    "        torch.tensor(0),\n",
    "    )\n",
    "    decoder = torch_neuron.trace(model.decoder, inputs)\n",
    "\n",
    "    traced = PaddedGenerator(model.config)\n",
    "    traced.encoder = encoder\n",
    "    traced.decoder = decoder\n",
    "    setattr(encoder, 'main_input_name', 'input_ids')  # Attribute required by beam search\n",
    "    return traced\n",
    "\n",
    "\n",
    "padded_model_neuron = trace(padded_model_cpu, num_texts, num_beams, max_decoder_length, max_encoder_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7789d6",
   "metadata": {},
   "source": [
    "## 8) Test padded inference on Neuron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab7016d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "infer(padded_model_neuron, tokenizer, sample_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
