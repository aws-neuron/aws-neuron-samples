{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a4aa9b8",
   "metadata": {},
   "source": [
    "# Fairseq for AWS Inferentia\n",
    "\n",
    "**Separate encoder/decoder approach**\n",
    "\n",
    "This notebook demonstrates how to compile the Fairseq encoder and decoder for Inferentia, and then swap the compiled models back into the original Fairseq model object.\n",
    "\n",
    "This approach is more flexible than the alternative nn.Sequential \"stacked encoder/decoder\" approach, as variable sequence length can be specified at inference time. However, a possible drawback is that separate inference requests are required for each autogressive decoder call (proportional to sequence length) which could introduce latency for longer sequences.\n",
    "\n",
    "**Reference:** https://github.com/facebookresearch/fairseq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea195b",
   "metadata": {},
   "source": [
    "## 1) Install dependencies\n",
    "**Tested with:** Python 3.6.x\n",
    "\n",
    "Fairseq also requires GCC to compile some C++ files. If you're using Ubuntu, install build-essential python3-setuptools and python3-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e41c976-116a-4600-bd50-59f681a5c1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pip repository  to point to the Neuron repository\n",
    "%pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com\n",
    "# now restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163d24c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Install Neuron PyTorch\n",
    "%pip install -U --force-reinstall torch==1.10.1 torch-neuron==1.10.1.* neuron-cc[tensorflow] \"protobuf<4\" \\\n",
    "    torchvision==0.11.2 tensorflow==1.15.0 setuptools==59.5.0 \\\n",
    "    requests tensorboardX --extra-index-url=https://download.pytorch.org/whl/torch_stable.html\n",
    "# use --force-reinstall if you're facing some issues while loading the modules\n",
    "# now restart the kernel again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922331c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir('fairseq'):\n",
    "    !git clone https://github.com/pytorch/fairseq && \\\n",
    "    cd fairseq && git checkout acd9a53607d1e5c64604e88fc9601d0ee56fd6f1 && \\\n",
    "    pip3 install --editable ./ && \\\n",
    "    pip3 --no-cache-dir install sacremoses fastBPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e94754",
   "metadata": {},
   "source": [
    "## 2) Initialize libraries and prepare input samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c17e50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import types\n",
    "import torch\n",
    "import torch.neuron\n",
    "import torch.nn.functional as F\n",
    "assert(torch.__version__.startswith(\"1.10.1\"))\n",
    "\n",
    "max_length=32 # you can increase this, but it can impact on performance\n",
    "sentences = [\n",
    "    \"i've seen things, you people wouldn't believe, hmmm.\",\n",
    "    \"attack ships on fire off the shoulder of Orion.\",\n",
    "    \"I've watched c Beams glitter in the dark near the Tannhauser Gate.\",\n",
    "    \"All those moments, will be lost in time like tears in rain.\",\n",
    "    \"time to die\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c268ce",
   "metadata": {},
   "source": [
    "## 3) Load a pre-trained model and check if it is .jit traceable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ecbec8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de.single_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54be66c2",
   "metadata": {},
   "source": [
    "### 3.1) Adjust the encoder to make it traceable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8023c931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e(self, src_tokens, src_lengths=None, **kwargs):    \n",
    "    if torch.jit.is_tracing():\n",
    "        print('tracing encoder...')\n",
    "        values = list(self.encoder.forward_(src_tokens, src_lengths).values())\n",
    "        return values[0],tuple(values[1]),values[2],tuple(values[3])\n",
    "    elif hasattr(self.encoder, 'forward_neuron'):        \n",
    "        delta = torch.as_tensor(self.encoder.max_decoder_length - src_tokens.shape[1])\n",
    "        pad_size = (0, delta)\n",
    "        src_tokens = F.pad(src_tokens, pad_size, \"constant\", 1) # 1 is the pad_token_id\n",
    "        strc_lengths = torch.ones([max_length], dtype=torch.int64)\n",
    "        out = self.encoder.forward_neuron(src_tokens, src_lengths)        \n",
    "        # we'll not unpad to make it already prepared for the decoder\n",
    "        return {\n",
    "            'encoder_out': out[0], 'encoder_padding_mask':out[1],\n",
    "            'encoder_embedding':out[2], 'encoder_states':out[3],\n",
    "            'fc_results':None, 'src_tokens':src_tokens,'src_lengths': [src_lengths]\n",
    "        }\n",
    "    else:\n",
    "        return self.encoder.forward_(src_tokens, src_lengths)\n",
    "if not hasattr(model.models[0].encoder, 'forward_'):\n",
    "    model.models[0].encoder.forward_ = model.models[0].encoder.forward\n",
    "model.models[0].encoder.max_decoder_length = max_length\n",
    "model.models[0].encoder.forward = types.MethodType(e, model.models[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280a0106",
   "metadata": {},
   "source": [
    "### 3.2) Adjust the decoder to make it traceable\n",
    "The decoder is more complex because it is invoked many times during prediction with different input shapes. We need to pad the input shapes before compiling the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55887a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(self, logits, index):\n",
    "    _, n_length, _ = logits.shape\n",
    "\n",
    "    # Create selection mask\n",
    "    mask = torch.arange(n_length, dtype=torch.int32) == index\n",
    "    mask = mask.view(1, -1, 1)\n",
    "\n",
    "    # Broadcast mask\n",
    "    masked = torch.multiply(logits, mask.to(torch.float32))\n",
    "\n",
    "    # Reduce along 1st dimension    \n",
    "    return torch.unsqueeze(torch.sum(masked, 1), 1)\n",
    "\n",
    "def pad(self, tensor, pad_val=(0,0), value=1):\n",
    "    return F.pad(tensor, pad_val, \"constant\", value)\n",
    "\n",
    "def d(self, prev_output_tokens, encoder_out, pad_size=torch.as_tensor(0), **kwargs):\n",
    "    if torch.jit.is_tracing():\n",
    "        print('tracing decoder...')\n",
    "        kwargs['features_only'] = True # do not apply output_projection\n",
    "        encoder_out = {'encoder_out': encoder_out[0], 'encoder_padding_mask': encoder_out[1] }        \n",
    "\n",
    "        out,extra = self.forward_(prev_output_tokens, encoder_out, **kwargs)        \n",
    "        index = torch.as_tensor(out.shape[1] - 1) - pad_size        \n",
    "        out = self.output_projection( self.reduce(out, index) )        \n",
    "        return out,tuple(extra['attn']),tuple(extra['inner_states'])\n",
    "    elif hasattr(self, 'forward_neuron'):        \n",
    "        pad_size = torch.as_tensor(self.max_decoder_length - prev_output_tokens.shape[1])\n",
    "        prev_output_tokens = self.pad(prev_output_tokens, (0,pad_size))\n",
    "        encoder_out_new = encoder_out['encoder_out']\n",
    "        encoder_padding_mask_new = encoder_out['encoder_padding_mask']\n",
    "        \n",
    "        out,attn,inner_states = self.forward_neuron(\n",
    "            prev_output_tokens, [encoder_out_new, encoder_padding_mask_new], pad_size )\n",
    "\n",
    "        return out, {'attn': attn, 'inner_states': inner_states}\n",
    "    else:\n",
    "        print('checking trace...')        \n",
    "        encoder_out = {'encoder_out': encoder_out[0], 'encoder_padding_mask': encoder_out[1] }\n",
    "        return self.forward_(prev_output_tokens, encoder_out, **kwargs)\n",
    "        \n",
    "if not hasattr(model.models[0].decoder, 'forward_'):\n",
    "    model.models[0].decoder.forward_ = model.models[0].decoder.forward\n",
    "model.models[0].decoder.max_decoder_length = max_length\n",
    "model.models[0].decoder.forward = types.MethodType(d, model.models[0].decoder)\n",
    "\n",
    "model.models[0].decoder.reduce = types.MethodType(reduce, model.models[0].decoder)\n",
    "model.models[0].decoder.pad = types.MethodType(pad, model.models[0].decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7493597",
   "metadata": {},
   "source": [
    "### 3.3) Check if both encoder and decoder are traceable now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360efae2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if hasattr(model.models[0].encoder, 'forward_neuron'): del model.models[0].encoder.forward_neuron\n",
    "if hasattr(model.models[0].decoder, 'forward_neuron'): del model.models[0].decoder.forward_neuron\n",
    "\n",
    "try:\n",
    "    inp_enc = (torch.ones([1,max_length], dtype=torch.int64), torch.ones([max_length], dtype=torch.int64))\n",
    "    y = model.models[0].encoder(*inp_enc) # warmup\n",
    "    traced_encoder = torch.jit.trace(model.models[0].encoder, inp_enc)\n",
    "    print(\"Cool! Model is jit traceable\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(f\"Ops. Something went wrong. Model is not traceable {e}\")\n",
    "## ok the model is .jit traceable. now let's compile it with NeuronSDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db3cdc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prev_output_tokens = torch.zeros([5, max_length], dtype=torch.int64)\n",
    "encoder_out = [torch.rand([max_length, 5, 1024], dtype=torch.float32)]\n",
    "encoder_padding_mask = [torch.zeros([5, max_length], dtype=torch.bool)]\n",
    "delta=torch.as_tensor(0)\n",
    "\n",
    "if hasattr(model.models[0].decoder, 'forward_neuron'): del model.models[0].decoder.forward_neuron\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        inp_dec = (prev_output_tokens, [encoder_out, encoder_padding_mask], delta)\n",
    "        y = model.models[0].decoder(*inp_dec) # warmup\n",
    "        traced_decoder = torch.jit.trace(model.models[0].decoder, inp_dec)    \n",
    "        y = traced_decoder(*inp_dec) \n",
    "    print(\"Cool! Model is jit traceable\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(f\"Ops. Something went wrong. Model is not traceable {e}\")\n",
    "## ok the model is .jit traceable. now let's compile it with NeuronSDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c0f50",
   "metadata": {},
   "source": [
    "### 3.4) Quick test to verify the traced modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f79d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.models[0].encoder.forward_neuron = traced_encoder\n",
    "model.models[0].decoder.forward_neuron = traced_decoder\n",
    "model.translate(sentences[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46d0b80",
   "metadata": {},
   "source": [
    "## 4) Analyze & compile the model for Inferentia with NeuronSDK\n",
    "\n",
    "Neuron Check Model tool provides user with basic information about the compiled and uncompiled model’s operations without the use of TensorBoard-Neuron.  \n",
    "https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-tools/tutorial-neuron-check-model.html\n",
    "\n",
    "\n",
    "The PyTorch-Neuron trace Python API provides a method to generate PyTorch models for execution on Inferentia, which can be serialized as TorchScript. It is analogous to torch.jit.trace() function in PyTorch.   \n",
    "https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/api-compilation-python-api.html?highlight=trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e8c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.neuron\n",
    "print(torch.neuron.analyze_model(traced_encoder, example_inputs=inp_enc))\n",
    "print(torch.neuron.analyze_model(traced_decoder, example_inputs=inp_dec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf29808a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.neuron\n",
    "\n",
    "#https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-cc/command-line-reference.html#cmdoption-neuron-cc-arg-0\n",
    "\n",
    "ops = torch.neuron.get_supported_operations() + ['aten::embedding']\n",
    "if not os.path.isfile(\"fairseq_encoder_neuron.pt\"):\n",
    "    model_neuron_encoder = torch.neuron.trace(traced_encoder, example_inputs=inp_enc, op_whitelist=ops)\n",
    "    ## Export to saved model\n",
    "    model_neuron_encoder.save(\"fairseq_encoder_neuron.pt\")\n",
    "\n",
    "if not os.path.isfile(\"fairseq_decoder_neuron.pt\"):\n",
    "    model_neuron_decoder = torch.neuron.trace(traced_decoder, example_inputs=inp_dec, op_whitelist=ops)\n",
    "    ## Export to saved model\n",
    "    model_neuron_decoder.save(\"fairseq_decoder_neuron.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1db4378",
   "metadata": {},
   "source": [
    "### 4.1) Verify the optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6d7b17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.models[0].encoder.forward_neuron = torch.load('fairseq_encoder_neuron.pt')\n",
    "model.models[0].decoder.forward_neuron = torch.load('fairseq_decoder_neuron.pt')\n",
    "model.translate(sentences[0:1]) # warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a32d3a0",
   "metadata": {},
   "source": [
    "## 5) A simple test to check the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "234f5c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"i've seen things, you people wouldn't believe, hmmm.\",\n",
       "  'ich habe Dinge gesehen, ihr Leute würdet es nicht glauben, hmmm.'),\n",
       " ('attack ships on fire off the shoulder of Orion.',\n",
       "  'Angriffsschiffe, die von der Schulter des Orion aus beschossen werden.'),\n",
       " (\"I've watched c Beams glitter in the dark near the Tannhauser Gate.\",\n",
       "  'Ich habe c Beams im Dunkeln in der Nähe des Tannhäuser Tores glitzern sehen.'),\n",
       " ('All those moments, will be lost in time like tears in rain.',\n",
       "  'All diese Momente gehen in der Zeit verloren wie Tränen im Regen.'),\n",
       " ('time to die', 'Zeit zu sterben')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## a good next step is to enable dynamic_batch_size to allow predicing\n",
    "## multiple sentences at the same time. Also, you can compile decoders\n",
    "## with different input shapes\n",
    "[(s,model.translate(s)) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ac84db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_aws_neuron_pytorch_p36)",
   "language": "python",
   "name": "conda_aws_neuron_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
