{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "2a4aa9b8",
            "metadata": {},
            "source": [
                "# Fairseq for AWS Inferentia\n",
                "\n",
                "**Separate encoder/decoder approach**\n",
                "\n",
                "This notebook demonstrates how to compile the Fairseq encoder and decoder for Inferentia, and then swap the compiled models back into the original Fairseq model object.\n",
                "\n",
                "This approach is more flexible than the alternative nn.Sequential \"stacked encoder/decoder\" approach, as variable sequence length can be specified at inference time. However, a possible drawback is that separate inference requests are required for each autogressive decoder call (proportional to sequence length) which could introduce latency for longer sequences.\n",
                "\n",
                "**Reference:** https://github.com/facebookresearch/fairseq"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "66ea195b",
            "metadata": {},
            "source": [
                "## 1) Install dependencies\n",
                "**Tested with:** Python 3.8.x\n",
                "\n",
                "Fairseq also requires GCC to compile some C++ files. If you're using Ubuntu, install build-essential python3-setuptools and python3-dev"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Verify that this Jupyter notebook is running the Python kernel environment that was set up according to the [Inf1 PyTorch Installation Guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/torch-neuron.html#setup-torch-neuron). You can select the kernel from the 'Kernel -> Change Kernel' option on the top of this Jupyter notebook page."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5f918544",
            "metadata": {},
            "outputs": [],
            "source": [
                "#Install Neuron PyTorch\n",
                "%pip install -U --force-reinstall torch==1.11.0 torch-neuron==1.11.0.* neuron-cc[tensorflow] \\\n",
                "    requests tensorboardX --extra-index-url=https://download.pytorch.org/whl/torch_stable.html\n",
                "# use --force-reinstall if you're facing some issues while loading the modules\n",
                "# now restart the kernel again"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "163d24c2",
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "if not os.path.isdir('fairseq-local'):\n",
                "    !git clone https://github.com/pytorch/fairseq fairseq-local && \\\n",
                "    cd fairseq-local && git checkout acd9a53607d1e5c64604e88fc9601d0ee56fd6f1 && \\\n",
                "    pip3 install --editable ./ && \\\n",
                "    pip3 --no-cache-dir install sacremoses torch==1.11.0 torchaudio==0.11.0 \"numpy==1.22.1\" scikit-learn fastBPE"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "819a915e",
            "metadata": {},
            "source": [
                "**Remember to restart kernel before continuing!**"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "98e94754",
            "metadata": {},
            "source": [
                "## 2) Initialize libraries and prepare input samples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "76c17e50",
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import types\n",
                "import torch\n",
                "import torch.neuron\n",
                "import torch.nn.functional as F\n",
                "print(torch.__version__)\n",
                "assert(torch.__version__.startswith(\"1.11.0\"))\n",
                "\n",
                "max_length=32 # you can increase this, but it can impact on performance\n",
                "sentences = [\n",
                "    \"i've seen things, you people wouldn't believe, hmmm.\",\n",
                "    \"attack ships on fire off the shoulder of Orion.\",\n",
                "    \"I've watched c Beams glitter in the dark near the Tannhauser Gate.\",\n",
                "    \"All those moments, will be lost in time like tears in rain.\",\n",
                "    \"time to die\"\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "78c268ce",
            "metadata": {},
            "source": [
                "## 3) Load a pre-trained model and check if it is .jit traceable"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9ed3f022",
            "metadata": {},
            "outputs": [],
            "source": [
                "model = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de.single_model')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a5650969",
            "metadata": {},
            "outputs": [],
            "source": [
                "# do this if you hit `No module named 'skearn'` above\n",
                "# !rm -rf ~/.cache/torch/hub/pytorch_fairseq_main/fairseq\n",
                "# model = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de.single_model')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "280eb68b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# do this if you hit `Primary config directory not found.` above\n",
                "# !rm -rf ~/.cache/torch/hub/\n",
                "# model = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de.single_model')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "54be66c2",
            "metadata": {},
            "source": [
                "### 3.1) Adjust the encoder to make it traceable"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8023c931",
            "metadata": {},
            "outputs": [],
            "source": [
                "def e(self, src_tokens, src_lengths=None, **kwargs):    \n",
                "    if torch.jit.is_tracing():\n",
                "        print('tracing encoder...')\n",
                "        values = list(self.encoder.forward_(src_tokens, src_lengths).values())\n",
                "        return values[0],tuple(values[1]),values[2],tuple(values[3])\n",
                "    elif hasattr(self.encoder, 'forward_neuron'):        \n",
                "        delta = torch.as_tensor(self.encoder.max_decoder_length - src_tokens.shape[1])\n",
                "        pad_size = (0, delta)\n",
                "        src_tokens = F.pad(src_tokens, pad_size, \"constant\", 1) # 1 is the pad_token_id\n",
                "        strc_lengths = torch.ones([max_length], dtype=torch.int64)\n",
                "        out = self.encoder.forward_neuron(src_tokens, src_lengths)        \n",
                "        # we'll not unpad to make it already prepared for the decoder\n",
                "        return {\n",
                "            'encoder_out': out[0], 'encoder_padding_mask':out[1],\n",
                "            'encoder_embedding':out[2], 'encoder_states':out[3],\n",
                "            'fc_results':None, 'src_tokens':src_tokens,'src_lengths': [src_lengths]\n",
                "        }\n",
                "    else:\n",
                "        return self.encoder.forward_(src_tokens, src_lengths)\n",
                "if not hasattr(model.models[0].encoder, 'forward_'):\n",
                "    model.models[0].encoder.forward_ = model.models[0].encoder.forward\n",
                "model.models[0].encoder.max_decoder_length = max_length\n",
                "model.models[0].encoder.forward = types.MethodType(e, model.models[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "280a0106",
            "metadata": {},
            "source": [
                "### 3.2) Adjust the decoder to make it traceable\n",
                "The decoder is more complex because it is invoked many times during prediction with different input shapes. We need to pad the input shapes before compiling the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f55887a8",
            "metadata": {},
            "outputs": [],
            "source": [
                "def reduce(self, logits, index):\n",
                "    _, n_length, _ = logits.shape\n",
                "\n",
                "    # Create selection mask\n",
                "    mask = torch.arange(n_length, dtype=torch.int32) == index\n",
                "    mask = mask.view(1, -1, 1)\n",
                "\n",
                "    # Broadcast mask\n",
                "    masked = torch.multiply(logits, mask.to(torch.float32))\n",
                "\n",
                "    # Reduce along 1st dimension    \n",
                "    return torch.unsqueeze(torch.sum(masked, 1), 1)\n",
                "\n",
                "def pad(self, tensor, pad_val=(0,0), value=1):\n",
                "    return F.pad(tensor, pad_val, \"constant\", value)\n",
                "\n",
                "def d(self, prev_output_tokens, encoder_out, pad_size=torch.as_tensor(0), **kwargs):\n",
                "    if torch.jit.is_tracing():\n",
                "        print('tracing decoder...')\n",
                "        kwargs['features_only'] = True # do not apply output_projection\n",
                "        encoder_out = {'encoder_out': encoder_out[0], 'encoder_padding_mask': encoder_out[1] }        \n",
                "\n",
                "        out,extra = self.forward_(prev_output_tokens, encoder_out, **kwargs)        \n",
                "        index = torch.as_tensor(out.shape[1] - 1) - pad_size        \n",
                "        out = self.output_projection( self.reduce(out, index) )        \n",
                "        return out,tuple(extra['attn']),tuple(extra['inner_states'])\n",
                "    elif hasattr(self, 'forward_neuron'):        \n",
                "        pad_size = torch.as_tensor(self.max_decoder_length - prev_output_tokens.shape[1])\n",
                "        prev_output_tokens = self.pad(prev_output_tokens, (0,pad_size))\n",
                "        encoder_out_new = encoder_out['encoder_out']\n",
                "        encoder_padding_mask_new = encoder_out['encoder_padding_mask']\n",
                "        \n",
                "        out,attn,inner_states = self.forward_neuron(\n",
                "            prev_output_tokens, [encoder_out_new, encoder_padding_mask_new], pad_size )\n",
                "\n",
                "        return out, {'attn': attn, 'inner_states': inner_states}\n",
                "    else:\n",
                "        print('checking trace...')        \n",
                "        encoder_out = {'encoder_out': encoder_out[0], 'encoder_padding_mask': encoder_out[1] }\n",
                "        return self.forward_(prev_output_tokens, encoder_out, **kwargs)\n",
                "        \n",
                "if not hasattr(model.models[0].decoder, 'forward_'):\n",
                "    model.models[0].decoder.forward_ = model.models[0].decoder.forward\n",
                "model.models[0].decoder.max_decoder_length = max_length\n",
                "model.models[0].decoder.forward = types.MethodType(d, model.models[0].decoder)\n",
                "\n",
                "model.models[0].decoder.reduce = types.MethodType(reduce, model.models[0].decoder)\n",
                "model.models[0].decoder.pad = types.MethodType(pad, model.models[0].decoder)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a7493597",
            "metadata": {},
            "source": [
                "### 3.3) Check if both encoder and decoder are traceable now"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "360efae2",
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "if hasattr(model.models[0].encoder, 'forward_neuron'): del model.models[0].encoder.forward_neuron\n",
                "if hasattr(model.models[0].decoder, 'forward_neuron'): del model.models[0].decoder.forward_neuron\n",
                "\n",
                "try:\n",
                "    inp_enc = (torch.ones([1,max_length], dtype=torch.int64), torch.ones([max_length], dtype=torch.int64))\n",
                "    y = model.models[0].encoder(*inp_enc) # warmup\n",
                "    traced_encoder = torch.jit.trace(model.models[0].encoder, inp_enc)\n",
                "    print(\"Cool! Model is jit traceable\")\n",
                "except Exception as e:\n",
                "    print(e)\n",
                "    print(f\"Ops. Something went wrong. Model is not traceable {e}\")\n",
                "## ok the model is .jit traceable. now let's compile it with NeuronSDK"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7db3cdc5",
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "prev_output_tokens = torch.zeros([5, max_length], dtype=torch.int64)\n",
                "encoder_out = [torch.rand([max_length, 5, 1024], dtype=torch.float32)]\n",
                "encoder_padding_mask = [torch.zeros([5, max_length], dtype=torch.bool)]\n",
                "delta=torch.as_tensor(0)\n",
                "\n",
                "if hasattr(model.models[0].decoder, 'forward_neuron'): del model.models[0].decoder.forward_neuron\n",
                "\n",
                "try:\n",
                "    with torch.no_grad():\n",
                "        inp_dec = (prev_output_tokens, [encoder_out, encoder_padding_mask], delta)\n",
                "        y = model.models[0].decoder(*inp_dec) # warmup\n",
                "        traced_decoder = torch.jit.trace(model.models[0].decoder, inp_dec)    \n",
                "        y = traced_decoder(*inp_dec) \n",
                "    print(\"Cool! Model is jit traceable\")\n",
                "except Exception as e:\n",
                "    print(e)\n",
                "    print(f\"Ops. Something went wrong. Model is not traceable {e}\")\n",
                "## ok the model is .jit traceable. now let's compile it with NeuronSDK"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "525c0f50",
            "metadata": {},
            "source": [
                "### 3.4) Quick test to verify the traced modules"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b4f79d66",
            "metadata": {},
            "outputs": [],
            "source": [
                "model.models[0].encoder.forward_neuron = traced_encoder\n",
                "model.models[0].decoder.forward_neuron = traced_decoder\n",
                "model.translate(sentences[0:1])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b46d0b80",
            "metadata": {},
            "source": [
                "## 4) Analyze & compile the model for Inferentia with NeuronSDK\n",
                "\n",
                "Neuron Check Model tool provides user with basic information about the compiled and uncompiled model\u2019s operations without the use of TensorBoard-Neuron.  \n",
                "https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-tools/tutorial-neuron-check-model.html\n",
                "\n",
                "\n",
                "The PyTorch-Neuron trace Python API provides a method to generate PyTorch models for execution on Inferentia, which can be serialized as TorchScript. It is analogous to torch.jit.trace() function in PyTorch.   \n",
                "https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/api-compilation-python-api.html?highlight=trace"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0ab71fb7",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.neuron\n",
                "print(torch.neuron.analyze_model(traced_encoder, example_inputs=inp_enc))\n",
                "print(torch.neuron.analyze_model(traced_decoder, example_inputs=inp_dec))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cf29808a",
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "import torch.neuron\n",
                "\n",
                "#https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-cc/command-line-reference.html#cmdoption-neuron-cc-arg-0\n",
                "\n",
                "ops = torch.neuron.get_supported_operations() + ['aten::embedding']\n",
                "if not os.path.isfile(\"fairseq_encoder_neuron.pt\"):\n",
                "    model_neuron_encoder = torch.neuron.trace(traced_encoder, example_inputs=inp_enc, op_whitelist=ops)\n",
                "    ## Export to saved model\n",
                "    model_neuron_encoder.save(\"fairseq_encoder_neuron.pt\")\n",
                "\n",
                "if not os.path.isfile(\"fairseq_decoder_neuron.pt\"):\n",
                "    model_neuron_decoder = torch.neuron.trace(traced_decoder, example_inputs=inp_dec, op_whitelist=ops)\n",
                "    ## Export to saved model\n",
                "    model_neuron_decoder.save(\"fairseq_decoder_neuron.pt\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b1db4378",
            "metadata": {},
            "source": [
                "### 4.1) Verify the optimized model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6c6d7b17",
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "# run this under python3.8 kernel can leard to kernel deadeal\n",
                "model.models[0].encoder.forward_neuron = torch.load('fairseq_encoder_neuron.pt')\n",
                "model.models[0].decoder.forward_neuron = torch.load('fairseq_decoder_neuron.pt')\n",
                "model.translate(sentences[0:1]) # warmup"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8a32d3a0",
            "metadata": {},
            "source": [
                "## 5) A simple test to check the predictions"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6d82e583",
            "metadata": {},
            "source": [
                "If the kernel die after running the cell below, please try copy all codes into a python script and run it a `python fairseq_script.py`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "234f5c82",
            "metadata": {},
            "outputs": [],
            "source": [
                "## a good next step is to enable dynamic_batch_size to allow predicing\n",
                "## multiple sentences at the same time. Also, you can compile decoders\n",
                "## with different input shapes\n",
                "[(s,model.translate(s)) for s in sentences]"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.16"
        },
        "vscode": {
            "interpreter": {
                "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}